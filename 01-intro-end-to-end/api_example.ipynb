{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iPJnDH0HhXB"
      },
      "source": [
        "<img src=\"https://imagedelivery.net/Dr98IMl5gQ9tPkFM5JRcng/3e5f6fbd-9bc6-4aa1-368e-e8bb1d6ca100/Ultra\" alt=\"Image description\" width=\"160\" />\n",
        "\n",
        "<br/>\n",
        "\n",
        "# Introduction to Contextual AI Platform using the API\n",
        "\n",
        "Contextual AI lets you create and use generative AI agents. This notebook introduces an end-to-end example workflow for creating a Retrieval-Augmented Generation (RAG) agent for a financial use case. The agent will answer questions based on the documents provided, but avoid any forward looking statements, e.g., Tell me about sales in 2028. This notebook uses the API, there is another notebook using the python client.\n",
        "\n",
        "This notebook covers the following steps:\n",
        "- Creating a Datastore\n",
        "- Ingesting Documents\n",
        "- Creating an RAG Agent\n",
        "- Querying an RAG Agent\n",
        "- Evaluating an RAG Agent\n",
        "- Improving the RAG Agent (Updating prompt and tuning)\n",
        "\n",
        "With the exception of the tuning model, the rest of the notebook can be run in under 15 minutes. \n",
        "The full documentation is available at [docs.contextual.ai](https://docs.contextual.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw_6FO_hh3aM"
      },
      "source": [
        "## Prerequisites:\n",
        "\n",
        "- API key, please contact Contextual AI's sales team to get your API key.\n",
        "\n",
        "- Data files, this demo also uses 3 files, an ingested document, evaluation dataset, and a training dataset. These are toy datasets to illustrate the functionality and are built towards a use case for training a RAG agent to avoid forward looking statements.\n",
        "\n",
        "      Ingestion: `Apple.pdf`\n",
        "\n",
        "      Evaluation: `eval_short.csv`\n",
        "\n",
        "      Training: `fin_train.jsonl`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czJ6eN8OHxhj"
      },
      "source": [
        "## Setup of Libraries, API, Dataset\n",
        "\n",
        "To begin, you will need an API key to securely access the API. To generate an API key, your admin can follow the process below:\n",
        "\n",
        "1.   Log into your tenant at app.contextual.ai\n",
        "2.   Click on \"API Keys\"\n",
        "3.   Click on \"Create API Key\"\n",
        "4.   Please keep your key in a secure place, and do not share it with anyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4wJG66VTIQvO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set up your API key here - i have it in my environment variables\n",
        "API_KEY = os.environ[\"CONTEXTUAL_API_KEY\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1AeByb1uHm9P"
      },
      "outputs": [],
      "source": [
        "REQUEST_URL = 'https://api.contextual.ai/v1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FV52HrlPLWIF"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict\n",
        "from IPython.display import display, JSON\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IhviooO2LauS"
      },
      "outputs": [],
      "source": [
        "# Helper function to define headers for API calls\n",
        "def get_headers(content_type: str = \"application/json\") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Generate headers for API requests\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"Content-Type\": content_type,\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcqh_-j1MzCn"
      },
      "source": [
        "## Step 1: Create your Datastore\n",
        "\n",
        "\n",
        "You will need to first create a datastore for your agent using the  /datastores endpoint. A datastore is secure storage for data. Each agent will have it's own datastore for storing data securely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tb-cMb-gMUD2"
      },
      "outputs": [],
      "source": [
        "def create_datastore(name: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Create a new datastore using the Contextual AI API.\n",
        "\n",
        "    Args:\n",
        "        name: Name of the datastore\n",
        "\n",
        "    Returns:\n",
        "        Dict: The JSON response from the API\n",
        "    \"\"\"\n",
        "    url = f\"{REQUEST_URL}/datastores\"\n",
        "\n",
        "    payload = {\"name\": name}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=get_headers(), json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error creating datastore: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlulbIvjdbZA",
        "outputId": "7029ff7d-193a-4cf1-98e8-0e29451ec446"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    result = create_datastore(name=\"Demo_fin_rag\") #TODO: Set a name for your datastore\n",
        "    datastore_id = result['id']\n",
        "    print(f\"Datastore ID: {datastore_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to create datastore: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IkAep8Vf29_"
      },
      "source": [
        "## Step 2: Ingest Documents into your Datastore\n",
        "\n",
        "You can now ingest documents into your Agent's datastore using the /datastores endpoint. Documents must be a PDF or HTML file.\n",
        "\n",
        "\n",
        "I am using a example PDF. You can also use your own documents here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8OBeqkJOqEck"
      },
      "outputs": [],
      "source": [
        "def ingest_file_to_datastore(datastore_id: str, file_path: str):\n",
        "   \"\"\"\n",
        "   Upload a local file to Contextual AI datastore\n",
        "\n",
        "   Args:\n",
        "       datastore_id: ID of the target datastore\n",
        "       file_path: Path to local file to upload\n",
        "   \"\"\"\n",
        "   try:\n",
        "       url = f\"{REQUEST_URL}/datastores/{datastore_id}/documents\"\n",
        "\n",
        "       with open(file_path, 'rb') as f:\n",
        "           response = requests.post(\n",
        "               url,\n",
        "               headers={\n",
        "                   'accept': 'application/json',\n",
        "                   'authorization': f\"Bearer {API_KEY}\"\n",
        "               },\n",
        "               files={'file': f}\n",
        "           )\n",
        "           response.raise_for_status()\n",
        "           print(f\"Successfully uploaded {file_path} to datastore {datastore_id}\")\n",
        "           return response.json()\n",
        "\n",
        "   except requests.exceptions.RequestException as e:\n",
        "       print(f\"Upload failed: {str(e)}\")\n",
        "       raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOBqQWf5qV9J",
        "outputId": "ff6938ea-455e-48a8-adf6-52ad52bca30c"
      },
      "outputs": [],
      "source": [
        "result = ingest_file_to_datastore(datastore_id, 'data/Apple.pdf')\n",
        "document_id = result['id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZdyD5cK8lc"
      },
      "source": [
        "Once ingested, you can view the list of documents, see their metadata, and also delete documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "j78jlN1_lIbM"
      },
      "outputs": [],
      "source": [
        "def get_document_metadata(datastore_id, document_id):\n",
        "    \"\"\"\n",
        "    Fetch metadata for a specific document from a datastore.\n",
        "\n",
        "    Parameters:\n",
        "        datastore_id (str): The unique ID of the datastore.\n",
        "        document_id (str): The unique ID of the document.\n",
        "        api_key (str): The API key for authentication.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the document metadata if the request is successful.\n",
        "        None: If the request fails, return None.\n",
        "\n",
        "    Raises:\n",
        "        Exception: For HTTP errors or unexpected responses.\n",
        "\n",
        "    Usage:\n",
        "        metadata = get_document_metadata(\"datastore123\", \"document456\")\n",
        "        if metadata:\n",
        "            print(\"Metadata retrieved:\", metadata)\n",
        "        else:\n",
        "            print(\"Failed to fetch metadata.\")\n",
        "    \"\"\"\n",
        "    url = f\"https://api.contextual.ai/v1/datastores/{datastore_id}/documents/{document_id}/metadata\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        'authorization': f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Assuming the response is in JSON format\n",
        "    else:\n",
        "        print(f\"Error: Failed to fetch metadata (HTTP {response.status_code}).\")\n",
        "        print(\"Response:\", response.text)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nKQERKxKwMx",
        "outputId": "9bae7dae-c3bd-4bb8-e1f3-fd1524a0f37f"
      },
      "outputs": [],
      "source": [
        "metadata = get_document_metadata(datastore_id, document_id)\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcTyR5QeHw7z"
      },
      "source": [
        "## Step 3: Create your Agent\n",
        "\n",
        "Next let's create the Agent and modify it to our needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "B1dnTQZDKLP3"
      },
      "outputs": [],
      "source": [
        "def create_agent(\n",
        "    name: str,\n",
        "    description: str,\n",
        "    system_prompt: Optional[str] = None,\n",
        "    datastore_ids: Optional[List[str]] = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Create a new agent in Contextual AI\n",
        "\n",
        "    Args:\n",
        "        name: Name of the agent\n",
        "        description: Description of the agent\n",
        "        system_guidelines: Guidelines for the system\n",
        "        datastore_ids: Optional list of datastore IDs\n",
        "\n",
        "    Returns:\n",
        "        JSON response from the API\n",
        "    \"\"\"\n",
        "    url = f\"{REQUEST_URL}/agents\"\n",
        "\n",
        "    payload = {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"system_prompt\": system_prompt,\n",
        "        \"datastore_ids\": datastore_ids or []\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        url,\n",
        "        headers=get_headers(),\n",
        "        json=payload\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr4jmjF_LtTp"
      },
      "source": [
        "Some additional parameters include setting a system prompt or using a previously tuned model.\n",
        "\n",
        "`system_prompt` is used for the instructions that your RAG system references when generating responses. Note that we do not guarantee that the system will follow these instructions exactly.\n",
        "\n",
        "`llm_model_id` is the optional model ID of a tuned model to use for generation. Contextual AI will use a default model if none is specified.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "r5jsWXpBPLtx"
      },
      "outputs": [],
      "source": [
        "# Sample prompt\n",
        "system_prompt = '''\n",
        "You are an AI assistant specialized in financial analysis and reporting. Your responses should be precise, accurate, and sourced exclusively from official financial documentation provided to you. Please follow these guidelines:\n",
        "\n",
        "Data Analysis & Response Quality:\n",
        "* Only use information explicitly stated in provided documentation (e.g., earnings releases, financial statements, investor presentations)\n",
        "* Present comparative analyses using structured formats with tables and bullet points where appropriate\n",
        "* Include specific period-over-period comparisons (quarter-over-quarter, year-over-year) when relevant\n",
        "* Maintain consistency in numerical presentations (e.g., consistent units, decimal places)\n",
        "* Flag any one-time items or special charges that impact comparability\n",
        "\n",
        "Technical Accuracy:\n",
        "* Use industry-standard financial terminology\n",
        "* Define specialized acronyms on first use\n",
        "* Never interchange distinct financial terms (e.g., revenue, profit, income, cash flow)\n",
        "* Always include units with numerical values\n",
        "* Pay attention to fiscal vs. calendar year distinctions\n",
        "* Present monetary values with appropriate scale (millions/billions)\n",
        "\n",
        "Response Format:\n",
        "* Begin with a high-level summary of key findings when analyzing data\n",
        "* Structure detailed analyses in clear, hierarchical formats\n",
        "* Use markdown for lists, tables, and emphasized text\n",
        "* Maintain a professional, analytical tone\n",
        "* Present quantitative data in consistent formats (e.g., basis points for ratios)\n",
        "\n",
        "Critical Guidelines:\n",
        "* Do not make forward-looking projections unless directly quoted from source materials\n",
        "* Avoid opinions, speculation, or assumptions\n",
        "* If information is unavailable or irrelevant, clearly state this without additional commentary\n",
        "* Answer questions directly, then stop\n",
        "* Do not reference source document names or file types in responses\n",
        "* Focus only on information that directly answers the query\n",
        "\n",
        "For any analysis, provide comprehensive insights using all relevant available information while maintaining strict adherence to these guidelines and focusing on delivering clear, actionable information.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymCcfHR1Lra6",
        "outputId": "310369a2-90a6-4b40-e3f1-fcc90f117d2f"
      },
      "outputs": [],
      "source": [
        "# Now let's try creating an agent\n",
        "try:\n",
        "    app_response = create_agent(\n",
        "        name=\"Demo-Finance Forward Looking\",\n",
        "        description=\"Research Agent using only Historical Information\",\n",
        "        system_prompt=system_prompt,\n",
        "        datastore_ids=[datastore_id]\n",
        "    )\n",
        "    # Store agent ID for later use\n",
        "    agent_id = app_response['id']\n",
        "    print(f\"Agent ID created: {agent_id}\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error creating agent: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFq4oMe9gMuz"
      },
      "source": [
        "## Step 4: Query your Agent\n",
        "\n",
        "Let's query our agent to see if its working. Let's pass in some objects and get a response.\n",
        "\n",
        "The required information is the agent_id and messages.  \n",
        "\n",
        "Optional information includes parameters for streaming, conversation_id, and model_id if using a different fine tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "b-6NVbr5gPBx"
      },
      "outputs": [],
      "source": [
        "def query_agent(\n",
        "   agent_id: str,\n",
        "   messages: List[Dict[str, str]],\n",
        "   model_id: Optional[str] = None,\n",
        "   conversation_id: Optional[str] = None,\n",
        "   stream: bool = False\n",
        "):\n",
        "   \"\"\"\n",
        "   Query a Contextual AI agent\n",
        "\n",
        "   Args:\n",
        "       agent_id: Agent ID of the agent to query\n",
        "       messages: List of message dictionaries with 'content' and 'role' keys\n",
        "       model_id: Optional model ID for specific fine-tuned model\n",
        "       conversation_id: Optional conversation ID for message history\n",
        "       stream: Whether to stream the response\n",
        "   \"\"\"\n",
        "   try:\n",
        "       url = f\"{REQUEST_URL}/agents/{agent_id}/query\"\n",
        "\n",
        "       payload = {\n",
        "           \"messages\": messages,\n",
        "           \"stream\": stream\n",
        "       }\n",
        "\n",
        "       if model_id:\n",
        "           payload[\"model_id\"] = model_id\n",
        "       if conversation_id:\n",
        "           payload[\"conversation_id\"] = conversation_id\n",
        "\n",
        "       response = requests.post(\n",
        "           url,\n",
        "           headers=get_headers(),\n",
        "           json=payload\n",
        "       )\n",
        "       response.raise_for_status()\n",
        "       return response.json()\n",
        "\n",
        "   except requests.exceptions.RequestException as e:\n",
        "       print(f\"Query failed: {str(e)}\")\n",
        "       raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOW94UdMySbp"
      },
      "source": [
        "**Note:** It may take a few minutes for the document to be ingested and processed. The Assistant will give a detailed answer once the documents are ingested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE4d616-rseT",
        "outputId": "dc58d319-0ef6-4d2a-ea30-5d2d8e109db3"
      },
      "outputs": [],
      "source": [
        "result = query_agent(\n",
        "    agent_id=agent_id,\n",
        "    messages=[{\n",
        "        # Input your question here\n",
        "        \"content\": \"what is Apple revenue in 2022\",\n",
        "        \"role\": \"user\",\n",
        "    }]\n",
        ")\n",
        "\n",
        "print(result[\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ozRLyVGS9xb",
        "outputId": "3b0547c9-940c-425b-fd19-5fae2fff72c3"
      },
      "outputs": [],
      "source": [
        "result['retrieval_contents']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5annKXLeNCGT"
      },
      "source": [
        "## Step 5: Evaluate your Agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLVlvOTfNctA"
      },
      "source": [
        "Evaluation endpoints allow you to evaluate your Agent using a set of prompts (questions) and reference (gold) answers. We support two metrics: equivalence and groundedness.\n",
        "\n",
        "Equivalance evaluates if the Agent response is equivalent to the ground truth (model-driven binary classification).  \n",
        "Groundedness decomposes the Agent response into claims and then evaluates if the claims are grounded by the retrieved documents.\n",
        "\n",
        "For those using unit tests, we also offer our `lmunit` endpoint, get more details [here](https://contextual.ai/blog/lmunit/) \n",
        "\n",
        "Lets start with an evaluation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "qGGzUpKaFTOC",
        "outputId": "2a15a0b3-61bb-408c-cca5-7ced897537d7"
      },
      "outputs": [],
      "source": [
        "eval = pd.read_csv('data/eval_short.csv')\n",
        "eval.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start an evaluation job that measures equivalence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "Upy0gy6VC_ls"
      },
      "outputs": [],
      "source": [
        "def evaluate_with_csv(agent_id: str, csv_path: str, metrics: str = \"equivalence\"):\n",
        "    \"\"\"\n",
        "    Evaluate an agent using a CSV file.\n",
        "\n",
        "    Args:\n",
        "        agent_id: ID of the agent to evaluate.\n",
        "        csv_path: Path to the evaluation CSV file.\n",
        "        metrics: Metrics to evaluate (e.g., \"equivalence\", \"groundedness\").\n",
        "\n",
        "    Returns:\n",
        "        dict: API response containing evaluation results or status.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.contextual.ai/v1/agents/{agent_id}/evaluate\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "    files = {\n",
        "        \"evalset_file\": (csv_path, open(csv_path, \"rb\"), \"text/csv\")\n",
        "    }\n",
        "    payload = {\n",
        "        \"metrics\": metrics\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, data=payload, files=files, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during evaluation: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        # Ensure file handle is closed properly\n",
        "        files[\"evalset_file\"][1].close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "csv_path = \"data/eval_short.csv\"\n",
        "eval_result = evaluate_with_csv(agent_id, csv_path, metrics=\"equivalence\")\n",
        "print(eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation jobs can take time, especially longer ones. Here is how you can check on their status. This dataset usually takes a few minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXkFf02FFjiJ",
        "outputId": "64892529-7577-4131-ab47-6d10c4cedb97"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_metadata(agent_id: str, job_id: str):\n",
        "    \"\"\"\n",
        "    Retrieve evaluation metadata for a specific evaluation job.\n",
        "\n",
        "    Args:\n",
        "        agent_id: ID of the agent.\n",
        "        job_id: ID of the evaluation job.\n",
        "\n",
        "    Returns:\n",
        "        dict: API response containing the evaluation metadata.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.contextual.ai/v1/agents/{agent_id}/evaluate/jobs/{job_id}/metadata\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an error for HTTP codes >= 400\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching evaluation metadata: {e}\")\n",
        "        raise\n",
        "\n",
        "metadata = get_evaluation_metadata(agent_id, eval_result['id'])\n",
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View our evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_evaluation_dataset(agent_id: str, dataset_name: str, batch_size: int = 64):\n",
        "    \"\"\"\n",
        "    Retrieve evaluation dataset.\n",
        "\n",
        "    Args:\n",
        "        agent_id: ID of the agent.\n",
        "        dataset_name: Name of the dataset.\n",
        "        batch_size: Size of the batch (default: 64).\n",
        "\n",
        "    Returns:\n",
        "        bytes: API response containing the evaluation dataset in octet-stream format.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.contextual.ai/v1/agents/{agent_id}/datasets/evaluate/{dataset_name}\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/octet-stream\",\n",
        "        \"authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "    \n",
        "    params = {\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        response.raise_for_status()  # Raise an error for HTTP codes >= 400\n",
        "        return response.content  # Using .content since we expect octet-stream\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching evaluation dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "eval_dataset = get_evaluation_dataset(agent_id=agent_id, dataset_name=metadata['dataset_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's write this out into pandas and csv to make it a bit friendlier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "\n",
        "def parse_jsonl_to_df(content):\n",
        "    \"\"\"\n",
        "    Parse JSONL bytes content into a pandas DataFrame, with flattened results.\n",
        "    \n",
        "    Args:\n",
        "        content (bytes): JSONL content in bytes format\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Parsed DataFrame\n",
        "    \"\"\"\n",
        "    # Decode bytes to string and split into lines\n",
        "    lines = content.decode('utf-8').strip().split('\\n')\n",
        "    \n",
        "    # Parse each line and flatten the results\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        entry = json.loads(line)\n",
        "        \n",
        "        # Parse the results string and flatten\n",
        "        if 'results' in entry:\n",
        "            results = ast.literal_eval(entry['results'])\n",
        "            # Remove the original results field\n",
        "            del entry['results']\n",
        "            # Add flattened results\n",
        "            if isinstance(results, dict):\n",
        "                for key, value in results.items():\n",
        "                    if isinstance(value, dict):\n",
        "                        for subkey, subvalue in value.items():\n",
        "                            entry[f'{key}_{subkey}'] = subvalue\n",
        "                    else:\n",
        "                        entry[key] = value\n",
        "        \n",
        "        data.append(entry)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Example usage:\n",
        "df = parse_jsonl_to_df(eval_dataset)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('eval_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692gSc7ZNyve"
      },
      "source": [
        "## Step 6: Tune your Agent\n",
        "\n",
        "Contexual AI allows you to tune your entire agent end to end for improvemed performance. To run a tune job, you need to specificy a training file and an optional test file. (If no test file is provided, the tuning job will hold out a portion of the training file as the test set.)\n",
        "\n",
        "A tuning job requires fine tuning models and the expectation should be it will take a couple of hours to run.\n",
        "\n",
        "After the tune job completes, the metadata associated with the tune job will include evaluation results and a model ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmRrBTy0OWzc"
      },
      "source": [
        "### 6.1 Format for the training file:\n",
        "\n",
        "The file should be in JSON array format, where each element of the array is a JSON object represents a single training example. The four required fields are guideline, prompt, response, and knowledge.\n",
        "\n",
        "- knowledge field should be an array of strings, each string representing a piece of knowledge that the model should use to generate the response.\n",
        "\n",
        "- reference: The gold-standard answer to the prompt.\n",
        "\n",
        "- guideline field should be guidelines for the expected response.\n",
        "\n",
        "- prompt field should be a question or statement that the model should respond to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf1DeSP-VXR4",
        "outputId": "21421ffe-50d0-44be-b0b0-6f45b420260d"
      },
      "outputs": [],
      "source": [
        "!head data/fin_train.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1tcNqTv0gaQ"
      },
      "source": [
        "### 6.2 Starting a Tuning Model Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Q74AUc0Ii2"
      },
      "outputs": [],
      "source": [
        "def tune_agent(agent_id: str, training_file: str):\n",
        "   \"\"\"\n",
        "   Tune agent with training data file\n",
        "\n",
        "   Args:\n",
        "       agent_id: ID of agent to tune\n",
        "       training_file: Path to JSON training data file with format:\n",
        "           [{\"guideline\": str, \"prompt\": str, \"response\": str, \"knowledge\": List[str]}]\n",
        "\n",
        "   Returns:\n",
        "       dict: API response\n",
        "   \"\"\"\n",
        "   url = f\"{REQUEST_URL}/agents/{agent_id}/tune\"\n",
        "   headers = {\n",
        "       \"accept\": \"application/json\",\n",
        "       \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "   }\n",
        "\n",
        "   with open(training_file, 'rb') as f:\n",
        "       files = {'training_file': f}\n",
        "       response = requests.post(url, headers=headers, files=files)\n",
        "       return response.json()\n",
        "\n",
        "job_id = tune_agent(agent_id, \"data/fin_train.jsonl\")\n",
        "print (job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGao-yw7yLVY"
      },
      "source": [
        " ### 6.3 Checking the Status.\n",
        "\n",
        " You can check the status of the job using the API. For detailed information, refer to the API documentation. When the tuning job is complete, the status will turn to completed. The response payload will also contain evaluation_results ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBy8pONdyOnu",
        "outputId": "254f5737-e292-46a6-838a-c5f19dc624b5"
      },
      "outputs": [],
      "source": [
        "def get_tune_job_metadata(agent_id: str, job_id: str):\n",
        "   \"\"\"Get metadata for a specific tuning job\"\"\"\n",
        "   url = f\"{REQUEST_URL}/agents/{agent_id}/tune/jobs/{job_id}/metadata\"\n",
        "   headers = {\n",
        "       \"accept\": \"application/json\",\n",
        "       \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "   }\n",
        "   response = requests.get(url, headers=headers)\n",
        "   return response.json()\n",
        "\n",
        "\n",
        "result = get_tune_job_metadata(agent_id, job_id['id'])\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg4YXZdYyR9X"
      },
      "source": [
        "When the tuning job is complete, the metadata would look like the following:\n",
        "```\n",
        "{'job_status': 'completed',\n",
        " 'evaluation_results': {'grounded_generation_train_test.json_equivalence': 1.0,\n",
        "  'grounded_generation_train_test.json_helpfulness': 0.814156498263641,\n",
        "  'grounded_generation_train_test.json_groundedness': 0.7781168677598632},\n",
        " 'model_id': 'registry/model-ada3c484-3ce0f31f-llm-fd6c2'}\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_77uMH5ybBh"
      },
      "source": [
        "### 6.4 Updating the agent\n",
        "Once the tuned job is complete, you can deploy the tuned model via editing the agent through API. Note that currently a single fine-tuned model deployment is allowed per tenant. Please see the API doc for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiuYZrBsybjo",
        "outputId": "d749ac21-d2ce-475f-966b-291c2e49c160"
      },
      "outputs": [],
      "source": [
        "def update_agent(agent_id: str, llm_model_id: str):\n",
        "   \"\"\"Update agent's LLM model\"\"\"\n",
        "   url = f\"{REQUEST_URL}/agents/{agent_id}\"\n",
        "   headers = {\n",
        "       \"accept\": \"application/json\",\n",
        "       \"content-type\": \"application/json\",\n",
        "       \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "   }\n",
        "   data = {\"llm_model_id\": llm_model_id}\n",
        "   response = requests.put(url, headers=headers, json=data)\n",
        "   return response.json()\n",
        "\n",
        "update_agent(agent_id, result['model_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKAmujbKgUBj"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "In this Notebook, we've created a RAG agent in the finance domain, evaluating the agent, and tuned it for better performance. You can learn more at [docs.contextual.ai](https://docs.contextual.ai/). Finally, reach out to your account team if you have further questions or issues."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
