{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cba3f87-ee36-496f-9921-d5b086a30a8f",
   "metadata": {},
   "source": [
    "# Tune & Evaluation Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57126865-7da0-4157-9d0a-63e67ef0bf2c",
   "metadata": {},
   "source": [
    "In the Beginner’s Guide, we went through the process of creating an API Key, creating a Datastore and ingesting documents, creating an Agent, and querying the Agent. This guide covers the next steps of tuning and evaluating your Agent. Make sure you’ve gone through all the steps in the Beginner’s Guide first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e46973-58d7-4e0d-b9e6-7fd97c85acd5",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b49df2-78a4-4dff-afed-889adb963d16",
   "metadata": {},
   "source": [
    "We've created a powerful set of APIs that enable you to specialize Agents to your data. Tuning often leads to significant improvements in performance for your specific use cases.\n",
    "\n",
    "### 1. Create a tune job\n",
    "\n",
    "To create a tune job, you need a training file and can optionally provide a test file. If no test file is provided, the API will automatically perform a train-test split on the training file.\n",
    "\n",
    "The API expects the data to be in JSON format with four required fields: `guideline,prompt,reference,knowledge`. See the [API docs](https://docs.contextual.ai/reference/create_tune_job_agents__agent_id__tune_post) for an explanation of each of these fields. Here is a [dummy example of what a tune set should look like](https://drive.google.com/drive/folders/1exULG56OXIquVI7N7NRSD4TKyPWATgXR?usp=drive_link):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aafe45ef-3080-43b7-a97b-ca07bc7d4d90",
   "metadata": {},
   "source": [
    "[\n",
    "  {\n",
    "    \"guideline\": \"The answer should be accurate.\",\n",
    "    \"prompt\": \"What was last quarter's revenue?\",\n",
    "    \"reference\": \"According to recent reports, the Q3 revenue was $1.2 million, a 0.1 million increase from Q2.\",\n",
    "    \"knowledge\": [\n",
    "        \"Quarterly report: Q3 revenue was $1.2 million.\",\n",
    "        \"Quarterly report: Q2 revenue was $1.1 million.\",\n",
    "        ...\n",
    "    ],\n",
    "  },\n",
    "  ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792ece2-f08d-44b3-9cbc-b025dd3d4a63",
   "metadata": {},
   "source": [
    "Use the following command to create a tune job. You will need to pass in the `agent_id` and `file_path` for your training file. If you do not provide a `model_id`, we will automatically use the Agent’s default model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785da18-1628-4301-8d6b-50597e19da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTUAL_API_KEY=\"key-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc41a98-425a-41c0-bf1b-d2bda5b994d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid API Key.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from contextual import ContextualAI\n",
    "\n",
    "# create a client\n",
    "client = ContextualAI(\n",
    "    api_key=CONTEXTUAL_API_KEY,\n",
    ")\n",
    "\n",
    "# test the API Key\n",
    "try:\n",
    "    response = create_agent_output = client.agents.list()\n",
    "    print(\"Valid API Key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Invalid API Key: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9978c4-51b3-4e63-a174-45542c18b13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"1e494c21-63f9-48b3-8584-2ca7f4e25c52\",\"datastore_ids\":[\"416ab3d1-f96f-4023-ab02-9fb18823941e\"]}\n"
     ]
    }
   ],
   "source": [
    "# Create an agent with name 'My First Agent'\n",
    "try:\n",
    "    create_agent_output = client.agents.create(\n",
    "        name=\"My First Agent\"\n",
    "    )\n",
    "    print(create_agent_output.model_dump_json())\n",
    "    agent_id = create_agent_output.id\n",
    "except Exception as e:\n",
    "    print(f\"Encountered error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36fc6ff-75bc-43a5-80e0-7b59fc9efcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '54adc134-50ca-4569-a4a4-8e7470f7361a'}\n"
     ]
    }
   ],
   "source": [
    "# create a dataset file\n",
    "with open(\"Dummy_TuneSet.json\", 'rb') as training_file:\n",
    "    try:\n",
    "        response = client.agents.tune.create(\n",
    "            agent_id=agent_id,\n",
    "            training_file=training_file,\n",
    "        )\n",
    "        job_id=response.id\n",
    "        print(response.to_dict())\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7da6f4-a839-43ab-9aad-51db81c4ef69",
   "metadata": {},
   "source": [
    "When the command runs you’ll be returned a `job_id` for the tune job. Keep in mind that tuning will take some time to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca80018-3dcd-4e4b-8544-a0178b8ba9e9",
   "metadata": {},
   "source": [
    "### 2. Check the status of the tune job\n",
    "\n",
    "You can check the status of the tune job by passing in the `agent_id` and `job_id`. When the job is complete, the status will change from processing to completed. The response payload will also contain the tuned `model_id` and the `evaluation_results` of the tuned model. The following code waits for the job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d968-13c3-4402-ab3e-490ef57f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    status = \"pending\"\n",
    "    while status in ['pending', 'processing']:\n",
    "        response = client.agents.tune.jobs.metadata(\n",
    "            agent_id=agent_id,\n",
    "            job_id=job_id,\n",
    "        )\n",
    "        status = response.job_status\n",
    "        print(f\"Status {status}\", end=\"\\r\")\n",
    "        time.sleep(2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974ac4a-be35-42d2-b621-ad318aa8bcd6",
   "metadata": {},
   "source": [
    "### 3. Deploy the tuned model\n",
    "\n",
    "Before you can use the tuned model, you need to deploy it to your Agent. You can do so by editing the configuration of your Agent and passing in the tuned `model_id`. Currently, we only allow a single fine-tuned model to be deployed per tenant. Please see the [API docs](https://docs.contextual.ai/reference/edit_agent_agents__agent_id__put) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55ac9022-450b-43e1-8a58-4632780c1a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: None\n"
     ]
    }
   ],
   "source": [
    "# get the model_id we just trained\n",
    "try:\n",
    "    response = client.agents.tune.jobs.metadata(\n",
    "        agent_id=agent_id,\n",
    "        job_id=job_id,\n",
    "    )\n",
    "    model_id = response.model_id\n",
    "    print(f\"model_id: {model_id}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "536e25ed-ad03-4014-bf14-495f27b3c4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentsResource.update() missing 1 required positional argument: 'agent_id'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = client.agents.update(\n",
    "        llm_model_id=model_id,\n",
    "    )\n",
    "    print(response.to_dict())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fd252-04ed-43d4-9d79-efbc5eac7af6",
   "metadata": {},
   "source": [
    "The deployment might take a moment to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdba6e1-fbe8-432b-93f0-2f855f9f6b0c",
   "metadata": {},
   "source": [
    "### 4. Query your tuned model!\n",
    "After you have deployed the tuned model, you can now query it with the usual command. Make sure you pass your new tuned model_id in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81169a0b-4191-45d2-9235-85936bb23bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = client.agents.query.create(\n",
    "        agent_id=agent_id,\n",
    "        llm_model_id=model_id,\n",
    "        messages=[{\n",
    "            # Input your question here\n",
    "            \"content\": \"What is the revenue of Apple?\",\n",
    "            \"role\": \"user\",\n",
    "        }]\n",
    "    )\n",
    "    print(query.message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Encountered error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f1a42-2988-41ac-a048-df6b1d062ce6",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f0cc8-c5c8-45c8-93c9-48ab502091e8",
   "metadata": {},
   "source": [
    "Evaluation endpoints allow you to evaluate your Agent using a set of prompts (questions) and reference (gold) answers. We support two metrics: equivalence and groundedness.\n",
    "\n",
    "* The first metric (”equivalence”) evaluates if the Agent response is equivalent to the ground truth (model-driven binary classification).\n",
    "* The second metric (”groundedness”) decomposes the Agent response into claims and then evaluates if the claims are grounded by the retrieved documents.\n",
    "### 1. Create an evaluation job.\n",
    "You will need to provide the evaluation data. You can provide the evaluation data in two ways: (i) by uploading an `evalset_file` as a CSV or (ii) creating an eval `Dataset` through the Dataset API. We will be focusing on (i), but you can read about (ii) in our API Docs.\n",
    "\n",
    "The API expects the data to be in CSV format with two required columns: `prompt`,`reference`. `prompt` is the question, while `reference` is the correct ground truth answer. See the [API docs](https://docs.contextual.ai/reference/create_evaluation_agents__agent_id__evaluate_post) for an explanation of each of these fields. Here is a [dummy example of what an eval set should look like](https://drive.google.com/drive/folders/1exULG56OXIquVI7N7NRSD4TKyPWATgXR?usp=drive_link):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d44e609-6844-40d2-82bf-7816df95ed45",
   "metadata": {},
   "source": [
    "{\n",
    " \"prompt\": \"What was the sales of Apple at the end of Q3 2022?\",\n",
    " \"reference\": \"Apple's sales was 100 million in the quarter ending Aug 31, 2022.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cb91d-caac-491c-be8a-08e697096fac",
   "metadata": {},
   "source": [
    "Use the following command to create your evaluation job. You will need to pass in your `agent_id` and `file_path` to your evaluation set. In the example below, we are evaluating on both equivalence and groundedness, but you can choose to evaluate only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "779bb243-2190-496a-a852-e24e6b1e9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# curl --request 'POST' \\\n",
    "#   --url 'https://api.contextual.ai/v1/agents/{agent_id}/evaluate' \\\n",
    "#   --header 'accept: application/json' \\\n",
    "#   --header 'Content-Type: multipart/form-data' \\\n",
    "#   --header 'Authorization: Bearer $API_KEY' \\\n",
    "#   --form 'metrics[]=equivalence' \\\n",
    "#   --form 'metrics[]=groundedness' \\\n",
    "#   --form 'evalset_file=@{$file_path};type=text/csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210eb55f-a81c-42bb-8701-fa71c854d1b6",
   "metadata": {},
   "source": [
    "### 2. Check the status of your evaluation job.\n",
    "You can use the following command to check the status of your evaluation job, where you’ll need to pass in your `agent_id` and evaluation `job_id`. If the evaluation job has completed, you will see your evaluation `metrics` , `job_metadata`, and the `dataset_name` where your eval metrics and row-by-row results are stored (you will need to use the `/datasets` API to view this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7342bf4b-22b8-4538-a7e2-1aa549bd3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl --request 'GET' \\\n",
    "#   --url 'https://api.contextual.ai/v1/agents/{agent_id}/evaluate/jobs/{job_id}/metadata' \\\n",
    "#   --header 'accept: application/json' \\\n",
    "#   --header 'Authorization: Bearer $API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44721d1c-06f7-4c9e-aacb-000eabe9bcbc",
   "metadata": {},
   "source": [
    "### 3. View your evaluation results.\n",
    "In Step 2, you should be able to get a dataset_name when your evaluation job has completed. You can then view your raw evaluation results (equivalence and/or groundedness scores for each question-response pair) with the `/datasets` endpoint. You can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86dbb13f-f0fe-4441-b4d4-28fb03c4e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl --request GET \\\n",
    "#      --url https://api.contextual.ai/v1/agents/{agent_id}/datasets/evaluate/{dataset_name} \\\n",
    "#      --header 'accept: application/octet-stream' \\\n",
    "#      --header 'authorization: Bearer $API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4650321-d2cc-48da-a43a-f85353d9c4b1",
   "metadata": {},
   "source": [
    "## LMUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd46568-367a-4bc3-8164-da368d572c31",
   "metadata": {},
   "source": [
    "In the previous section, we talked about how you can evaluate your Agent with a curated evalset. The Contextual Platform also provides another method of evaluation via natural language unit tests using the /lmunit endpoint. To understand the use cases of /lmunit, please read our blogpost.\n",
    "\n",
    "Follow these steps to use the `/lmunit` endpoint.\n",
    "\n",
    "1. Come up with criteria for what constitutes a good response in the context of your agent. The criteria can be about the content, form, or style of a response. Is there a style or tone you want the agent to maintain? Do good answers exhibit a specific reasoning pattern or cover specific content?\n",
    "2. Translate one of these criteria into a specific, clear, and testable statement or question. For example:\n",
    "    * Does the response maintain a professional style?\n",
    "    * Does the response impartially cover different opinions or perspectives that exist on a question?\n",
    "    * Does the response mention US Federal Reserve policy if the question is about the rate of inflation?\n",
    "3. Use the /lmunit endpoint to evaluate a given query-response pair. Scores are reported on a 1-5 scale, with higher scores indicating better satisfaction of the test criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae62d7e-5fc5-4cfc-8f53-c0512e4272b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl --request POST \\\n",
    "#      --url https://api.contextual.ai/v1/lmunit \\\n",
    "#      --header 'accept: application/json' \\\n",
    "#      --header 'authorization: Bearer $API_KEY ' \\\n",
    "#      --header 'content-type: application/json' \\\n",
    "#      --data '"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bc034ae-b126-419c-880c-733289942ff3",
   "metadata": {},
   "source": [
    "{\n",
    "  \"query\": \"I remember learning about an ex prisoner who was brought to America to help\n",
    "  train the soldiers. But the details escape me. Can anyone provide details to who he was?\",\n",
    "  \n",
    "  \"response\": \"Those clues are kind of vague, but one possible candidate might be\n",
    "  Casimir Pulaski. He was an effective cavalry officer who was embroiled in the chaos of\n",
    "  Poland in the later 18th c. and fought on a losing side, but while he was tried and condemned\n",
    "  and his possessions confiscated, he’d fled to France by then. So, “ex prisoner” is not quite\n",
    "  correct. But he did indeed help train American cavalry—and irritated quite a few who served with\n",
    "  him with his imperious manner. If you heard about him in the US, it might be because there are a\n",
    "  lot of towns named after him, and he became quite a popular hero to later Polish-Americans.\n",
    "  Pienkos, A. (1976). A Bicentennial Look at Casimir Pulaski: Polish, American and Ethnic Folk Hero.\n",
    "  Polish American Studies, 33(1), 5–17. http://www.jstor.org/stable/20147942\",\n",
    "  \n",
    "  \"unit_test\": \"Is the response helpful and aligned with the spirit of what the prompt\n",
    "  was asking for?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bf315-1f7b-4a16-8f3a-9e500d21930b",
   "metadata": {},
   "source": [
    "If your request is successful, you'll receive a score. LMUnit enables targeted evaluation of criteria that you care about, enabling you to evolve and optimize your Agents.\n",
    "\n",
    "🎉 That was a quick spin-through our tune and eval endpoints! To learn more about our APIs and their capabilities, visit [docs.contextual.ai](https://docs.contextual.ai). We look forward to seeing what you build with our platform 🏗️.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
