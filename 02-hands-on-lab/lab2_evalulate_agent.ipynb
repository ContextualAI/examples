{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iPJnDH0HhXB"
   },
   "source": [
    "<img src=\"https://imagedelivery.net/Dr98IMl5gQ9tPkFM5JRcng/3e5f6fbd-9bc6-4aa1-368e-e8bb1d6ca100/Ultra\" alt=\"Image description\" width=\"160\" />\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Introduction to Contextual AI Platform using the Python Client\n",
    "\n",
    "Contextual AI lets you create and use generative AI agents. This notebook introduces an end-to-end example workflow for creating a Retrieval-Augmented Generation (RAG) agent for a financial use case. The agent will answer questions based on the documents provided, but avoid any forward looking statements, e.g., Tell me about sales in 2028. This notebook uses the python client.\n",
    "\n",
    "This notebook covers the following steps:\n",
    "- Creating a Datastore\n",
    "- Ingesting Documents\n",
    "- Creating an RAG Agent\n",
    "- Querying an RAG Agent\n",
    "- Evaluating an RAG Agent\n",
    "- Improving the RAG Agent (Updating prompt and tuning)\n",
    "\n",
    "With the exception of the tuning model, the rest of the notebook can be run in under 15 minutes. \n",
    "The full documentation is available at [docs.contextual.ai](https://docs.contextual.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw_6FO_hh3aM"
   },
   "source": [
    "## Prerequisites:\n",
    "\n",
    "- API key, please contact Contextual AI's sales team to get your API key.\n",
    "\n",
    "- Data files, this demo also uses 3 files, an ingested document, evaluation dataset, and a training dataset. These are toy datasets to illustrate the functionality of the platform.\n",
    "\n",
    "      Ingestion: `Apple.pdf`\n",
    "\n",
    "      Evaluation: `eval_short.csv`\n",
    "\n",
    "      Training: `fin_train.jsonl`\n",
    "\n",
    "- To use the python client `pip install --pre contextual-client`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czJ6eN8OHxhj"
   },
   "source": [
    "## Setup of Libraries, API, Dataset\n",
    "\n",
    "To begin, you will need an API key to securely access the API. To generate an API key, your admin can follow the process below:\n",
    "\n",
    "1.   Log into your tenant at app.contextual.ai\n",
    "2.   Click on \"API Keys\"\n",
    "3.   Click on \"Create API Key\"\n",
    "4.   Please keep your key in a secure place, and do not share it with anyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "from contextual import ContextualAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"key-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4wJG66VTIQvO"
   },
   "outputs": [],
   "source": [
    "client = ContextualAI(\n",
    "    api_key=API_KEY,  # This is the default and can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id=\"5ee7e13c-871f-406e-85e9-1cbc49679ec9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOW94UdMySbp"
   },
   "source": [
    "**Note:** It may take a few minutes for the document to be ingested and processed. The Assistant will give a detailed answer once the documents are ingested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is lots more information you can access about the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ozRLyVGS9xb",
    "outputId": "3b0547c9-940c-425b-fd19-5fae2fff72c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval contents: [RetrievalContent(content_id='d998e506-0c1b-a6d9-3b6c-89070559e526', doc_id='851f87a9-4353-43eb-86c1-cec7483f1a6c', doc_name='Apple', format='pdf', type='file', content=None, extras=None, number=1, page=9, url=None), RetrievalContent(content_id='72d90c9b-d83f-5cb3-f05c-dc9e6f45297f', doc_id='851f87a9-4353-43eb-86c1-cec7483f1a6c', doc_name='Apple', format='pdf', type='file', content=None, extras=None, number=2, page=3, url=None), RetrievalContent(content_id='9ccfd991-9f25-5db4-ec83-b3e1e5af801e', doc_id='851f87a9-4353-43eb-86c1-cec7483f1a6c', doc_name='Apple', format='pdf', type='file', content=None, extras=None, number=3, page=13, url=None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRetrieval contents:\", query_result.retrieval_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5annKXLeNCGT"
   },
   "source": [
    "## Step 5: Evaluate your Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLVlvOTfNctA"
   },
   "source": [
    "Evaluation endpoints allow you to evaluate your Agent using a set of prompts (questions) and reference (gold) answers. We support two metrics: equivalence and groundedness.\n",
    "\n",
    "Equivalance evaluates if the Agent response is equivalent to the ground truth (model-driven binary classification).  \n",
    "Groundedness decomposes the Agent response into claims and then evaluates if the claims are grounded by the retrieved documents.\n",
    "\n",
    "For those using unit tests, we also offer our `lmunit` endpoint, get more details [here](https://contextual.ai/blog/lmunit/) \n",
    "\n",
    "Let's start with an evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "qGGzUpKaFTOC",
    "outputId": "2a15a0b3-61bb-408c-cca5-7ced897537d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was Apple's total net sales for 2022?</td>\n",
       "      <td>Apple's total net sales for the three months e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was Apple's Services revenue in Q1 2023 a...</td>\n",
       "      <td>Apple's Services revenue was $20,766 million i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was Apple's iPhone revenue in Q1 2026?</td>\n",
       "      <td>I am an AI assistant created by Contextual AI....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much did Apple spend on research and devel...</td>\n",
       "      <td>Apple spent $7,709 million on research and dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the next amazing product from Apple?</td>\n",
       "      <td>I am an AI assistant created by Contextual AI....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0         What was Apple's total net sales for 2022?   \n",
       "1  What was Apple's Services revenue in Q1 2023 a...   \n",
       "2        What was Apple's iPhone revenue in Q1 2026?   \n",
       "3  How much did Apple spend on research and devel...   \n",
       "4       What is the next amazing product from Apple?   \n",
       "\n",
       "                                           reference  \n",
       "0  Apple's total net sales for the three months e...  \n",
       "1  Apple's Services revenue was $20,766 million i...  \n",
       "2  I am an AI assistant created by Contextual AI....  \n",
       "3  Apple spent $7,709 million on research and dev...  \n",
       "4  I am an AI assistant created by Contextual AI....  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = pd.read_csv('data/eval_short.csv')\n",
    "eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an evaluation job that measures both equivalence and groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/eval_short.csv', 'rb') as f:\n",
    "    eval_result = client.agents.evaluate.create(\n",
    "        agent_id=agent_id,\n",
    "        metrics=[\"equivalence\", \"groundedness\"],\n",
    "        evalset_file=f\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation jobs can take time, especially longer ones. Here is how you can check on their status. This dataset usually takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationJobMetadata(dataset_name='eval_short_20071207-0404-4abd-9e21-3f96b4757432_results', job_metadata=JobMetadata(num_failed_predictions=0, num_predictions=12, num_successful_predictions=12), metrics={'equivalence_score': {'score': 0.9166666666666666}, 'groundedness_score': {'score': 0.5}}, status='completed')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_status = client.agents.evaluate.jobs.metadata(agent_id=agent_id, job_id=eval_result.id)\n",
    "eval_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the evaluation job is completed you can look at the final results across your evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def process_binary_evaluation(binary_response):\n",
    "    \"\"\"\n",
    "    Process BinaryAPIResponse into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        binary_response: BinaryAPIResponse from evaluate.retrieve\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed evaluation data\n",
    "    \"\"\"\n",
    "    # Read the binary content\n",
    "    content = binary_response.read()\n",
    "    \n",
    "    # Now decode the content\n",
    "    lines = content.decode('utf-8').strip().split('\\n')\n",
    "    \n",
    "    # Parse each line and flatten the results\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Parse the results string if it exists\n",
    "            if 'results' in entry:\n",
    "                results = ast.literal_eval(entry['results'])\n",
    "                del entry['results']\n",
    "                if isinstance(results, dict):\n",
    "                    for key, value in results.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            for subkey, subvalue in value.items():\n",
    "                                entry[f'{key}_{subkey}'] = subvalue\n",
    "                        else:\n",
    "                            entry[key] = value\n",
    "            \n",
    "            data.append(entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>reference</th>\n",
       "      <th>response</th>\n",
       "      <th>guideline</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>status</th>\n",
       "      <th>equivalence_score_score</th>\n",
       "      <th>equivalence_score_metadata</th>\n",
       "      <th>factuality_v4.5_score_score</th>\n",
       "      <th>factuality_v4.5_score_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was Apple's total net sales for 2022?</td>\n",
       "      <td>Apple's total net sales for the three months e...</td>\n",
       "      <td>I am an AI assistant created by Contextual AI....</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The generated response does not provide any in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'description': 'There are claims but no knowl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was Apple's Services revenue in Q1 2023 a...</td>\n",
       "      <td>Apple's Services revenue was $20,766 million i...</td>\n",
       "      <td>**Summary:**Apple's Services revenue in Q1 202...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Section: FORM 10-Q\\nSub-Section: Apple Inc.\\...</td>\n",
       "      <td>completed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The generated response provides the same core ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'claim_scores': [{'score': 'Unsupported', 'cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was Apple's iPhone revenue in Q1 2026?</td>\n",
       "      <td>I am an AI assistant created by Contextual AI....</td>\n",
       "      <td>**iPhone Revenue in Q1 2026:**According to App...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Section: FORM 10-Q\\nSub-Section: Apple Inc.\\...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The generated response provides specific and a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'claim_scores': [{'score': 'Unsupported', 'cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much did Apple spend on research and devel...</td>\n",
       "      <td>Apple spent $7,709 million on research and dev...</td>\n",
       "      <td>**Research and Development Expenditure in 2022...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Section: FORM 10-Q\\nSub-Section: Operating E...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The generated response does not provide the fu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'claim_scores': [{'score': 'Supported', 'clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the next amazing product from Apple?</td>\n",
       "      <td>I am an AI assistant created by Contextual AI....</td>\n",
       "      <td>I'm happy to help! However, I must clarify tha...</td>\n",
       "      <td></td>\n",
       "      <td>[\"Section: FORM 10-Q\\nSub-Section: Quarterly H...</td>\n",
       "      <td>completed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Both responses convey the same core informatio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'claim_scores': [{'score': 'Unsupported', 'cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0         What was Apple's total net sales for 2022?   \n",
       "1  What was Apple's Services revenue in Q1 2023 a...   \n",
       "2        What was Apple's iPhone revenue in Q1 2026?   \n",
       "3  How much did Apple spend on research and devel...   \n",
       "4       What is the next amazing product from Apple?   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Apple's total net sales for the three months e...   \n",
       "1  Apple's Services revenue was $20,766 million i...   \n",
       "2  I am an AI assistant created by Contextual AI....   \n",
       "3  Apple spent $7,709 million on research and dev...   \n",
       "4  I am an AI assistant created by Contextual AI....   \n",
       "\n",
       "                                            response guideline  \\\n",
       "0  I am an AI assistant created by Contextual AI....             \n",
       "1  **Summary:**Apple's Services revenue in Q1 202...             \n",
       "2  **iPhone Revenue in Q1 2026:**According to App...             \n",
       "3  **Research and Development Expenditure in 2022...             \n",
       "4  I'm happy to help! However, I must clarify tha...             \n",
       "\n",
       "                                           knowledge     status  \\\n",
       "0                                                 []  completed   \n",
       "1  [\"Section: FORM 10-Q\\nSub-Section: Apple Inc.\\...  completed   \n",
       "2  [\"Section: FORM 10-Q\\nSub-Section: Apple Inc.\\...  completed   \n",
       "3  [\"Section: FORM 10-Q\\nSub-Section: Operating E...  completed   \n",
       "4  [\"Section: FORM 10-Q\\nSub-Section: Quarterly H...  completed   \n",
       "\n",
       "   equivalence_score_score                         equivalence_score_metadata  \\\n",
       "0                      0.0  The generated response does not provide any in...   \n",
       "1                      1.0  The generated response provides the same core ...   \n",
       "2                      0.0  The generated response provides specific and a...   \n",
       "3                      0.0  The generated response does not provide the fu...   \n",
       "4                      1.0  Both responses convey the same core informatio...   \n",
       "\n",
       "   factuality_v4.5_score_score  \\\n",
       "0                          0.0   \n",
       "1                          0.0   \n",
       "2                          0.0   \n",
       "3                          1.0   \n",
       "4                          0.0   \n",
       "\n",
       "                      factuality_v4.5_score_metadata  \n",
       "0  {'description': 'There are claims but no knowl...  \n",
       "1  {'claim_scores': [{'score': 'Unsupported', 'cl...  \n",
       "2  {'claim_scores': [{'score': 'Unsupported', 'cl...  \n",
       "3  {'claim_scores': [{'score': 'Supported', 'clai...  \n",
       "4  {'claim_scores': [{'score': 'Unsupported', 'cl...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results = client.agents.datasets.evaluate.retrieve(dataset_name=eval_status.dataset_name, agent_id = agent_id)\n",
    "df = process_binary_evaluation(eval_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am going to save the results of the evaluation to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('eval_results_python.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "692gSc7ZNyve"
   },
   "source": [
    "## Step 6: Improving your Agent\n",
    "\n",
    "Contexual AI gives you multiple methods for improving your overall agent performance. Two methods available via the API are modifying the system prompt or tuning the models. It's recommended you start with modifying the system prompt before using tuning. Please reach out to the account team for more best practices around tuning.\n",
    "\n",
    "Let's go through both options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Revising the system prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial testing, you may want to revise the system prompt. Here I have an updated prompt with additional information in the critical guidelines section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt2 = '''\n",
    "You are an AI assistant specialized in financial analysis and reporting. Your responses should be precise, accurate, and sourced exclusively from official financial documentation provided to you. Please follow these guidelines:\n",
    "\n",
    "Data Analysis & Response Quality:\n",
    "* Only use information explicitly stated in provided documentation (e.g., earnings releases, financial statements, investor presentations)\n",
    "* Present comparative analyses using structured formats with tables and bullet points where appropriate\n",
    "* Include specific period-over-period comparisons (quarter-over-quarter, year-over-year) when relevant\n",
    "* Maintain consistency in numerical presentations (e.g., consistent units, decimal places)\n",
    "* Flag any one-time items or special charges that impact comparability\n",
    "\n",
    "Technical Accuracy:\n",
    "* Use industry-standard financial terminology\n",
    "* Define specialized acronyms on first use\n",
    "* Never interchange distinct financial terms (e.g., revenue, profit, income, cash flow)\n",
    "* Always include units with numerical values\n",
    "* Pay attention to fiscal vs. calendar year distinctions\n",
    "* Present monetary values with appropriate scale (millions/billions)\n",
    "\n",
    "Response Format:\n",
    "* Begin with a high-level summary of key findings when analyzing data\n",
    "* Structure detailed analyses in clear, hierarchical formats\n",
    "* Use markdown for lists, tables, and emphasized text\n",
    "* Maintain a professional, analytical tone\n",
    "* Present quantitative data in consistent formats (e.g., basis points for ratios)\n",
    "\n",
    "Critical Guidelines:\n",
    "* Do not make forward-looking projections unless directly quoted from source materials\n",
    "* Do not answer any questions for 2024 or later\n",
    "* Avoid opinions, speculation, or assumptions\n",
    "* If information is unavailable or irrelevant, clearly state this without additional commentary\n",
    "* Answer questions directly, then stop\n",
    "* Do not reference source document names or file types in responses\n",
    "* Focus only on information that directly answers the query\n",
    "\n",
    "For any analysis, provide comprehensive insights using all relevant available information while maintaining strict adherence to these guidelines and focusing on delivering clear, actionable information.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now update the agent. And verify that changes by checking the agent metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an AI assistant specialized in financial analysis and reporting. Your responses should be precise, accurate, and sourced exclusively from official financial documentation provided to you. Please follow these guidelines:\n",
      "\n",
      "Data Analysis & Response Quality:\n",
      "* Only use information explicitly stated in provided documentation (e.g., earnings releases, financial statements, investor presentations)\n",
      "* Present comparative analyses using structured formats with tables and bullet points where appropriate\n",
      "* Include specific period-over-period comparisons (quarter-over-quarter, year-over-year) when relevant\n",
      "* Maintain consistency in numerical presentations (e.g., consistent units, decimal places)\n",
      "* Flag any one-time items or special charges that impact comparability\n",
      "\n",
      "Technical Accuracy:\n",
      "* Use industry-standard financial terminology\n",
      "* Define specialized acronyms on first use\n",
      "* Never interchange distinct financial terms (e.g., revenue, profit, income, cash flow)\n",
      "* Always include units with numerical values\n",
      "* Pay attention to fiscal vs. calendar year distinctions\n",
      "* Present monetary values with appropriate scale (millions/billions)\n",
      "\n",
      "Response Format:\n",
      "* Begin with a high-level summary of key findings when analyzing data\n",
      "* Structure detailed analyses in clear, hierarchical formats\n",
      "* Use markdown for lists, tables, and emphasized text\n",
      "* Maintain a professional, analytical tone\n",
      "* Present quantitative data in consistent formats (e.g., basis points for ratios)\n",
      "\n",
      "Critical Guidelines:\n",
      "* Do not make forward-looking projections unless directly quoted from source materials\n",
      "* Do not answer any questions for 2024 or later\n",
      "* Avoid opinions, speculation, or assumptions\n",
      "* If information is unavailable or irrelevant, clearly state this without additional commentary\n",
      "* Answer questions directly, then stop\n",
      "* Do not reference source document names or file types in responses\n",
      "* Focus only on information that directly answers the query\n",
      "\n",
      "For any analysis, provide comprehensive insights using all relevant available information while maintaining strict adherence to these guidelines and focusing on delivering clear, actionable information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client.agents.update(agent_id=agent_id, system_prompt=system_prompt2)\n",
    "\n",
    "agent_config = client.agents.metadata(agent_id=agent_id)\n",
    "print (agent_config.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have updated the agent, go try running another evaluation job. You will see the performance has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmRrBTy0OWzc"
   },
   "source": [
    "### 6.2 Tuning the Contextual System\n",
    "\n",
    "To run a tune job, you need to specificy a training file and an optional test file. (If no test file is provided, the tuning job will hold out a portion of the training file as the test set.)\n",
    "\n",
    "A tuning job requires fine tuning models and the expectation should be it will take a couple of hours to run.\n",
    "\n",
    "After the tune job completes, the metadata associated with the tune job will include evaluation results and a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Tuning dataset:  \n",
    "\n",
    "The file should be in JSON array format, where each element of the array is a JSON object represents a single training example. The four required fields are guideline, prompt, response, and knowledge.\n",
    "\n",
    "- knowledge field should be an array of strings, each string representing a piece of knowledge that the model should use to generate the response.\n",
    "\n",
    "- reference: The gold-standard answer to the prompt.\n",
    "\n",
    "- guideline field should be guidelines for the expected response.\n",
    "\n",
    "- prompt field should be a question or statement that the model should respond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rf1DeSP-VXR4",
    "outputId": "21421ffe-50d0-44be-b0b0-6f45b420260d"
   },
   "outputs": [],
   "source": [
    "!head data/fin_train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1tcNqTv0gaQ"
   },
   "source": [
    "#### 6.2.2 Starting a tuning model job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fin_train.jsonl', 'rb') as f:\n",
    "    tune_job = client.agents.tune.create(\n",
    "    agent_id=agent_id,\n",
    "    training_file=f\n",
    ")\n",
    "    \n",
    "tune_job_id = tune_job.id\n",
    "print(f\"Tune job created: {tune_job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHsd1Vzx0LDN",
    "outputId": "902694ff-0b03-4122-c434-7235ee8d66c7"
   },
   "outputs": [],
   "source": [
    "print (agent_id)\n",
    "print (tune_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGao-yw7yLVY"
   },
   "source": [
    "#### 6.2.3 Checking the status.\n",
    "\n",
    " You can check the status of the job using the API. For detailed information, refer to the API documentation\". When the tuning job is complete, the status will turn to completed. The response payload will also contain evaluation_results, such as scores for equivalence, helpfulness, and groundedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBy8pONdyOnu",
    "outputId": "254f5737-e292-46a6-838a-c5f19dc624b5"
   },
   "outputs": [],
   "source": [
    "tune_metadata = client.agents.tune.jobs.metadata(\n",
    "    agent_id=agent_id,\n",
    "    job_id=tune_job_id\n",
    ")\n",
    "print(\"Tuning job metadata:\", tune_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg4YXZdYyR9X"
   },
   "source": [
    "When the tuning job is complete, the metadata would look like the following:\n",
    "```\n",
    "{'job_status': 'completed',\n",
    " 'evaluation_results': {'grounded_generation_train_test.json_equivalence': 1.0,\n",
    "  'grounded_generation_train_test.json_helpfulness': 0.814156498263641,\n",
    "  'grounded_generation_train_test.json_groundedness': 0.7781168677598632},\n",
    " 'model_id': 'registry/model-ada3c484-3ce0f31f-llm-fd6c2'}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_77uMH5ybBh"
   },
   "source": [
    "#### 6.2.4 Updating the agent\n",
    "Once the tuned job is complete, you can deploy the tuned model via editing the agent through API. Note that currently a single fine-tuned model deployment is allowed per tenant. Please see the API doc for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.agents.update(agent_id=agent_id, llm_model_id=tune_metadata.model_id)\n",
    "\n",
    "print(\"Agent updated with tuned model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKAmujbKgUBj"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "In this Notebook, we've created a RAG agent in the finance domain, evaluating the agent, and tuned it for better performance. You can learn more at [docs.contextual.ai](https://docs.contextual.ai/). Finally, reach out to your account team if you have further questions or issues."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
