{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iPJnDH0HhXB"
   },
   "source": [
    "<img src=\"https://imagedelivery.net/Dr98IMl5gQ9tPkFM5JRcng/3e5f6fbd-9bc6-4aa1-368e-e8bb1d6ca100/Ultra\" alt=\"Image description\" width=\"160\" />\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Natural Language Unit Testing for LLM Response Quality: A Financial Services Case Study\n",
    "\n",
    "\n",
    "Evaluating LLM outputs is critical, but underexplored for generative AI uses cases. Natural language unit tests provide a systematic approach for evaluating LLM response quality.\n",
    "This notebook demonstrates how to create and apply unit tests using [LMUnit](https://contextual.ai/blog/lmunit/), a specialized model developed by Contextual AI that achieves state-of-the-art performance.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Why Natural Language Unit Testing?\n",
    "\n",
    "Traditional LLM evaluation methods often face several challenges:\n",
    "- Human evaluations are inconsistent and costly, while metrics like ROUGE fail to capture nuanced quality measures.\n",
    "- General-purpose LLMs may not provide fine-grained feedback\n",
    "- Simple yes/no evaluations miss important nuances\n",
    "\n",
    "Natural language unit tests address these challenges by:\n",
    "- Breaking down evaluation into specific, testable criteria\n",
    "- Providing granular feedback on different quality aspects\n",
    "- Enabling systematic improvement of LLM outputs\n",
    "- Supporting domain-specific quality requirements\n",
    "\n",
    "For example, financial compliance often requires precise regulatory phrasing, which is hard to assess with a generic style evaluation.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "This notebook walks through the process of:\n",
    "1. Setting up the LMUnit environment\n",
    "2. Loading evaluation data\n",
    "3. Designing effective unit tests\n",
    "4. Running evaluations with LMUnit\n",
    "5. Analyzing and visualizing results\n",
    "6. Interpreting scores and improving responses\n",
    "\n",
    "### What This Notebook Covers\n",
    "This notebook focuses on response quality evaluation - how well information is presented and communicated. While we use financial services as our example domain, the principles can be adapted to any field.\n",
    "\n",
    "The notebook does not cover factual accuracy or retriever results. (Stay tuned for that!)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/03-lmunit/LMUnit.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29JM_ro71PR2"
   },
   "source": [
    "## 1: Setup development environment\n",
    "\n",
    "Before we begin evaluating LLM responses, we need to set up our environment.  \n",
    "\n",
    "LMUnit is provided through the Contextual AI python client, which we'll install first. If you prefer, you can use LMUnit directly from the [API](https://docs.contextual.ai/reference/lmunit_lmunit_post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOYFDTFHVeA6",
    "outputId": "4b87322e-1fa5-4c23-f0e6-6e003c9a34d4"
   },
   "outputs": [],
   "source": [
    "!pip install contextual-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LJ8OLSN1PR3"
   },
   "source": [
    "We'll need several Python packages for data handling and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDAopcn7tgAN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from contextual import ContextualAI\n",
    "\n",
    "# polar plots\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "\n",
    "#clustering analysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecg0h-uY1PR4"
   },
   "source": [
    "To use LMUnit, you'll need an API key from Contextual AI. You can [request a key here](https://contextual.ai/request-lmunit-api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wJG66VTIQvO"
   },
   "outputs": [],
   "source": [
    "client = ContextualAI(api_key=\"ADD YOUR KEY HERE\")\n",
    "# Consider using environment variables for production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcqh_-j1MzCn"
   },
   "source": [
    "## 2: Load evaluation dataset\n",
    "\n",
    "LMUnit evaluates query-response pairs, which means we need:\n",
    "- The original query/prompt - `prompt`\n",
    "- The LLM's response - `response`\n",
    "\n",
    "For this notebook, I am going to use synthetic financial data. The dataset contains 10 financial questions and responses, designed to showcase different aspects of response quality evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGgBRkd-1PR4",
    "outputId": "97674993-4115-4c61-9d7b-5e13b1aab0cf"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/financial_qa_pairs.csv' if os.path.exists('data/financial_qa_pairs.csv')\n",
    "    else \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/03-lmunit/data/financial_qa_pairs.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBNaPz90tgAQ"
   },
   "source": [
    "## 3: Identify unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRMLBEb3tgAQ"
   },
   "source": [
    "Unit tests can more insightful than simply asking a LLM to answer if a response is high quality. When writing unit tests, strive to:\n",
    "- Specific and focused on one aspect\n",
    "- Clear and unambiguous\n",
    "- Measurable and consistent\n",
    "- Relevant to the domain\n",
    "- Framed positively\n",
    "\n",
    "You should develop unit tests that match your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcRiFJcN2qZP"
   },
   "source": [
    "For this example, we will use global unit tests that we will run across all our responses. The following were six critical dimensions for financial services communications we chose as unit tests:\n",
    "\n",
    "1. **Context**\n",
    "   - Question: \"Are relevant market conditions or external factors acknowledged?\"\n",
    "   - Why: Ensures responses consider the broader financial environment\n",
    "\n",
    "2. **Clarity**\n",
    "   - Question: \"Is complex financial information presented in an accessible way?\"\n",
    "   - Why: Tests whether technical concepts are explained effectively\n",
    "\n",
    "3. **Precision**\n",
    "   - Question: \"Is terminology used accurately and consistently?\"\n",
    "   - Why: Validates proper use of financial terms\n",
    "\n",
    "4. **Compliance**\n",
    "   - Question: \"Does the response adhere to relevant financial regulations and disclosure requirements?\"\n",
    "   - Why: Ensures regulatory alignment\n",
    "\n",
    "5. **Actionable**\n",
    "   - Question: \"Does the response provide clear next steps or implications?\"\n",
    "   - Why: Tests practical utility of responses\n",
    "\n",
    "6. **Risks**\n",
    "   - Question: \"Are potential risks clearly identified and explained?\"\n",
    "   - Why: Verifies appropriate risk disclosure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bj4XFTtMtgAQ"
   },
   "outputs": [],
   "source": [
    "unit_tests = [\n",
    "    \"Are relevant market conditions or external factors acknowledged?\",\n",
    "    \"Is complex financial information presented in an accessible way?\",\n",
    "    \"Is terminology used accurately and consistently?\",\n",
    "    \"Does the response adhere to relevant financial regulations and disclosure requirements?\",\n",
    "    \"Does the response provide clear next steps or implications?\",\n",
    "    \"Are potential risks clearly identified and explained?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpS99F8itgAQ"
   },
   "source": [
    "## 4: Evaluate unit tests Using LMUnit\n",
    "\n",
    "LMUnit is specifically trained for evaluating natural language unit tests and provides:\n",
    "- Scores on a continuous 1-5 scale\n",
    "- Consistent evaluation across different criteria\n",
    "- Better performance than general-purpose LLMs like GPT-4\n",
    "- Efficient batch processing capabilities\n",
    "- Ability to add rubrics to evaluation (for example to create a binary evaluation score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmewmxpm1PR5"
   },
   "source": [
    "Let's start with a simple example to understand how LMUnit works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpGrBXsxCFXI",
    "outputId": "b0d81ece-089f-4699-b9df-eefe9452e739"
   },
   "outputs": [],
   "source": [
    "\n",
    "response = client.lmunit.create(\n",
    "                    query=\"What material is used in N95 masks?\",\n",
    "                    response=\"N95 masks are made primarily of polypropylene. This synthetic material is created through a melt-blowing process that creates multiple layers of microfibers. The material was chosen because it can be electrostatically charged to attract particles. Particles are the constituents of the universe\",\n",
    "                    unit_test=\"Does the response avoid unnecessary information?\"\n",
    "                )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li9lTq6ltgAQ"
   },
   "source": [
    "For our use case, we need to apply each global unit test to the query/response pairs we identified in the evaluation data.\n",
    "Here is helper function for testing batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reYAm40itgAQ"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_unit_tests_with_progress(\n",
    "    df: pd.DataFrame,\n",
    "    unit_tests: List[str],\n",
    "    batch_size: int = 10\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run unit tests with progress tracking and error handling.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with prompt-response pairs\n",
    "        unit_tests: List of unit test strings\n",
    "        batch_size: Number of tests to run in parallel\n",
    "\n",
    "    Returns:\n",
    "        List of test results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Process in batches with progress bar\n",
    "    for idx in tqdm(range(0, len(df)), desc=\"Processing responses\"):\n",
    "        row = df.iloc[idx]\n",
    "        row_results = []\n",
    "\n",
    "        for test in unit_tests:\n",
    "            try:\n",
    "                result = client.lmunit.create(\n",
    "                    query=row['prompt'],\n",
    "                    response=row['response'],\n",
    "                    unit_test=test\n",
    "                )\n",
    "                row_results.append({\n",
    "                    'test': test,\n",
    "                    'score': result.score,\n",
    "                    'metadata': result.metadata if hasattr(result, 'metadata') else None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error with prompt {idx}, test '{test}': {e}\")\n",
    "                row_results.append({\n",
    "                    'test': test,\n",
    "                    'score': None,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "\n",
    "        results.append({\n",
    "            'prompt': row['prompt'],\n",
    "            'response': row['response'],\n",
    "            'test_results': row_results\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNLXYv60tgAQ",
    "outputId": "458b839f-4977-47a8-dbc0-784042fcef98"
   },
   "outputs": [],
   "source": [
    "results = run_unit_tests_with_progress(df, unit_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUnwC6CBtgAQ"
   },
   "source": [
    "That is it!  Now we can examine the results. Looking at the results, you see every unit test is scored on a continuous scale of 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahfFlDO7tgAQ",
    "outputId": "f47b8212-26e2-4497-e75d-c1165746a21f"
   },
   "outputs": [],
   "source": [
    "for result in results[:2]:  # Slice to get the first two entries\n",
    "    print(f\"\\nPrompt: {result['prompt']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(\"Test Results:\")\n",
    "    for test_result in result['test_results']:\n",
    "        print(f\"- {test_result['test']}: {test_result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XngWEvvItgAQ"
   },
   "source": [
    "Save out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ9iGzNA1PR5"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([(r['prompt'], r['response'], t['test'], t['score']) for r in results for t in r['test_results']], columns=['prompt', 'response', 'test', 'score']).to_csv(f\"unit_test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA5j9k3v1PR6"
   },
   "source": [
    "## 5: Visualize individual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYhpaOZW1PR6"
   },
   "source": [
    "To under the results of the unit tests, visualizations can be helpful. Here we create a visualization of individual response radar plots showing performance across all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftSFloOB1PR6"
   },
   "outputs": [],
   "source": [
    "def map_test_to_category(test_question: str) -> str:\n",
    "    \"\"\"Map the full test question to its category.\"\"\"\n",
    "    category_mapping = {\n",
    "        'Are relevant market conditions or external factors': 'CONTEXT',\n",
    "        'Is complex financial information presented': 'CLARITY',\n",
    "        'Is terminology used accurately': 'PRECISION',\n",
    "        'Does the response adhere to relevant financial regulations': 'COMPLIANCE',\n",
    "        'Does the response provide clear next steps': 'ACTIONABLE',\n",
    "        'Are potential risks clearly identified': 'RISK'\n",
    "    }\n",
    "\n",
    "    for key, value in category_mapping.items():\n",
    "        if key.lower() in test_question.lower():\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def create_unit_test_plots(results: List[Dict],\n",
    "                          test_indices: Optional[Union[int, List[int]]] = None,\n",
    "                          figsize: tuple = (10, 10)):\n",
    "    \"\"\"\n",
    "    Create polar plot(s) for unit test results. Can plot either a single test,\n",
    "    specific multiple tests, or all tests in a row.\n",
    "\n",
    "    Args:\n",
    "        results: List of dictionaries containing test results\n",
    "        test_indices: Optional; Either:\n",
    "            - None (plots all results)\n",
    "            - int (plots single result)\n",
    "            - List[int] (plots multiple specific results)\n",
    "        figsize: Tuple specifying the figure size (width, height)\n",
    "    \"\"\"\n",
    "    # Handle different input cases for test_indices\n",
    "    if test_indices is None:\n",
    "        indices_to_plot = list(range(len(results)))\n",
    "    elif isinstance(test_indices, int):\n",
    "        if test_indices >= len(results):\n",
    "            raise IndexError(f\"test_index {test_indices} is out of range. Only {len(results)} results available.\")\n",
    "        indices_to_plot = [test_indices]\n",
    "    else:\n",
    "        if not test_indices:\n",
    "            raise ValueError(\"test_indices list cannot be empty\")\n",
    "        if max(test_indices) >= len(results):\n",
    "            raise IndexError(f\"test_index {max(test_indices)} is out of range. Only {len(results)} results available.\")\n",
    "        indices_to_plot = test_indices\n",
    "\n",
    "    # Categories in desired order\n",
    "    categories = ['CONTEXT', 'CLARITY', 'PRECISION',\n",
    "                 'COMPLIANCE', 'ACTIONABLE', 'RISK']\n",
    "\n",
    "    # Set up the angles for the polar plot\n",
    "    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # Close the plot\n",
    "\n",
    "    # Calculate figure size based on number of plots\n",
    "    num_plots = len(indices_to_plot)\n",
    "    fig_width = figsize[0] * num_plots\n",
    "    fig = plt.figure(figsize=(fig_width, figsize[1]))\n",
    "\n",
    "    # Create a subplot for each result\n",
    "    for plot_idx, result_idx in enumerate(indices_to_plot):\n",
    "        result = results[result_idx]\n",
    "\n",
    "        # Create subplot\n",
    "        ax = plt.subplot(1, num_plots, plot_idx + 1, projection='polar')\n",
    "\n",
    "        # Get scores for this result\n",
    "        scores = []\n",
    "        for category in categories:\n",
    "            score = None\n",
    "            for test_result in result['test_results']:\n",
    "                mapped_category = map_test_to_category(test_result['test'])\n",
    "                if mapped_category == category:\n",
    "                    score = test_result['score']\n",
    "                    break\n",
    "            scores.append(score if score is not None else 0)\n",
    "\n",
    "        # Close the scores array\n",
    "        scores = np.concatenate((scores, [scores[0]]))\n",
    "\n",
    "        # Plot the scores\n",
    "        ax.plot(angles, scores, 'o-', linewidth=2)\n",
    "        ax.fill(angles, scores, alpha=0.25)\n",
    "\n",
    "        # Set the labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories)\n",
    "\n",
    "        # Set the scale\n",
    "        ax.set_ylim(0, 5)\n",
    "\n",
    "        # Add grid\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Add score values as annotations\n",
    "        for angle, score, category in zip(angles[:-1], scores[:-1], categories):\n",
    "            ax.text(angle, score + 0.2, f'{score:.2f}',\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "        # Add title for each subplot\n",
    "        prompt = result['prompt']\n",
    "        ax.set_title(f\"Test {result_idx}\\n{prompt}\", pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmuYOeb21PR6"
   },
   "source": [
    "Radar plots are a great way to visualize the different dimensions that the unit tests provide. Try changing the index to view other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZwLvsN11PR6",
    "outputId": "5f153881-9565-4a75-dd32-160579fb10c3"
   },
   "outputs": [],
   "source": [
    "# Plot the second test result\n",
    "fig = create_unit_test_plots(results, test_indices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4sUUpXO9SH9"
   },
   "source": [
    "You want to compare multiple plots?  Try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9Qa-vW71PR6",
    "outputId": "b11b3864-31a9-4615-d705-562da48268c1"
   },
   "outputs": [],
   "source": [
    "fig = create_unit_test_plots(results, test_indices=[0, 1, 2,])\n",
    "fig = create_unit_test_plots(results, test_indices=[3, 4, 5,])\n",
    "fig = create_unit_test_plots(results, test_indices=[6, 7, 8,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMvM3rPE1PR6"
   },
   "source": [
    "## 6: Visualize group results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OHJeJa71PR6"
   },
   "source": [
    "For analyzing larger sets of results, it's useful to use clustering methods. Let's walk through using clustering to help analyze a dataset of 40 unit test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AMKJ9Ds9Ca7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'data/synthetic_financial_responses.csv' if os.path.exists('data/synthetic_financial_responses.csv')\n",
    "    else \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/03-lmUnit/data/synthetic_financial_responses.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKEuDUij1PR6"
   },
   "source": [
    "Let's start by using Kmeans and clustering this into four groups. For your analysis, you may need to use fewer or more clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQBgkRWO1PR6"
   },
   "outputs": [],
   "source": [
    "def cluster_responses(df: pd.DataFrame, n_clusters: int = 4) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Perform clustering on response evaluation data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing evaluation scores\n",
    "        n_clusters: Number of clusters to identify\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame with cluster assignments, DataFrame of cluster centers)\n",
    "    \"\"\"\n",
    "    categories = ['CONTEXT', 'CLARITY', 'PRECISION',\n",
    "                 'COMPLIANCE', 'ACTIONABLE', 'RISK']\n",
    "    # Prepare and perform clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Calculate cluster centers\n",
    "    cluster_centers = pd.DataFrame(\n",
    "        scaler.inverse_transform(kmeans.cluster_centers_),\n",
    "        columns=categories\n",
    "    )\n",
    "    return df_clustered, cluster_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRlo3-LW1PR6"
   },
   "source": [
    "Let's look at how each of our samples are now clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stDUh3C01PR6",
    "outputId": "c218a2a6-d80b-4ab7-d5d8-fc13fd8ae073"
   },
   "outputs": [],
   "source": [
    "df_clustered, centers = cluster_responses(df)\n",
    "df_clustered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmkaVZXA1PR6"
   },
   "source": [
    "We can visualize these cluster, both in terms of how the clusters centers vary as well as how the individuals points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_UJhaSU1PR6",
    "outputId": "5135010e-f10e-482c-c166-79de9ab5e644"
   },
   "outputs": [],
   "source": [
    "def visualize_clusters(df: pd.DataFrame, cluster_centers: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create visualizations for cluster analysis.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        cluster_centers: DataFrame of cluster centers\n",
    "    \"\"\"\n",
    "    # 1. Heatmap of cluster centers\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_centers, annot=True, cmap='RdYlBu_r', fmt='.2f')\n",
    "    plt.title('Response Pattern Cluster Centers')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Scatter plot of key dimensions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(df['CONTEXT'], df['ACTIONABLE'],\n",
    "                         c=df['cluster'], cmap='viridis')\n",
    "    plt.xlabel('CONTEXT Score')\n",
    "    plt.ylabel('ACTIONABLE Score')\n",
    "    plt.title('Cluster Distribution (Context vs Actionable)')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_clusters(df_clustered, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYFMagSn1PR7"
   },
   "source": [
    "I have a little bit of code here that helps to analyze the clusters by the categories we used for unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fxwu6E4T1PR-",
    "outputId": "19eee696-2e74-4db4-8cd9-eb2bcbe9dd4e"
   },
   "outputs": [],
   "source": [
    "def explain_clusters(df: pd.DataFrame, cluster_centers: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Provide detailed explanation of cluster characteristics.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        cluster_centers: DataFrame of cluster centers\n",
    "    \"\"\"\n",
    "    expected_categories = ['CONTEXT', 'CLARITY', 'PRECISION',\n",
    "                         'COMPLIANCE', 'ACTIONABLE', 'RISK']\n",
    "\n",
    "    print(\"\\nCluster Analysis:\")\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    # Print cluster centers\n",
    "    print(\"\\nCluster Centers:\")\n",
    "    print(cluster_centers.round(2))\n",
    "\n",
    "    # Print cluster sizes\n",
    "    print(\"\\nCluster Sizes:\")\n",
    "    print(df['cluster'].value_counts().sort_index())\n",
    "\n",
    "    # Analyze each cluster\n",
    "    print(\"\\nCluster Characteristics:\")\n",
    "    for i in range(len(cluster_centers)):\n",
    "        cluster_df = df[df['cluster'] == i]\n",
    "        print(f\"\\nCluster {i}:\")\n",
    "\n",
    "        # Calculate average scores\n",
    "        avg_scores = cluster_df[expected_categories].mean()\n",
    "        sorted_scores = avg_scores.sort_values(ascending=False)\n",
    "\n",
    "        # Get top and bottom categories\n",
    "        top_cats = list(sorted_scores.head(2).items())\n",
    "        bottom_cats = list(sorted_scores.tail(2).items())\n",
    "\n",
    "        # Print characteristics\n",
    "        print(f\"Size: {len(cluster_df)} responses\")\n",
    "        print(f\"Strongest areas: {top_cats[0][0]} ({top_cats[0][1]:.2f}), \"\n",
    "              f\"{top_cats[1][0]} ({top_cats[1][1]:.2f})\")\n",
    "        print(f\"Weakest areas: {bottom_cats[0][0]} ({bottom_cats[0][1]:.2f}), \"\n",
    "              f\"{bottom_cats[1][0]} ({bottom_cats[1][1]:.2f})\")\n",
    "\n",
    "\n",
    "explain_clusters(df_clustered, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TDtTQvk9uLN"
   },
   "source": [
    "#### Interpret the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_zT8ceZ1PR-"
   },
   "source": [
    "Now that we have a better understanding of the response clusters, we can identify the patterns and characteristics of each cluster.\n",
    "\n",
    "Cluster 0: Compliance Blind Spot\n",
    "Clear communication but missing regulatory elements\n",
    "High CLARITY/PRECISION, Low COMPLIANCE/RISK\n",
    "\n",
    "Cluster 1: Clarity Gap\n",
    "High context awareness but poor explanation clarity\n",
    "High CONTEXT/RISK, Low CLARITY/PRECISION\n",
    "\n",
    "Cluster 2: Theory-Practice Gap\n",
    "Strong theoretical understanding but impractical\n",
    "High PRECISION/CLARITY, Low ACTIONABLE\n",
    "\n",
    "Cluster 3: Surface Analysis\n",
    "Basic understanding without depth\n",
    "Medium CLARITY but Low CONTEXT/RISK\n",
    "\n",
    "Once you have an understanding of what kinds of errors are happening, time to dig in and address those errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgKf24OntgAQ"
   },
   "source": [
    "## Best Practices for Using LMUnit\n",
    "\n",
    "1. Unit Test Design:\n",
    "   - Keep tests focused and specific\n",
    "   - Avoid compound criteria\n",
    "   - Use clear, unambiguous language\n",
    "   - Assess a desirable quality, such as “Is the response coherent?” rather than “Is the response incoherent?”\n",
    "\n",
    "2. Evaluation Strategy:\n",
    "   - Start with global tests\n",
    "   - Add query-level tests as needed\n",
    "   - Monitor patterns across responses\n",
    "\n",
    "3. Score Interpretation:\n",
    "   - 5: Excellent - Fully satisfies criteria\n",
    "   - 4: Good - Minor issues\n",
    "   - 3: Acceptable - Some issues\n",
    "   - 2: Poor - Significant issues\n",
    "   - 1: Unacceptable - Fails criteria\n",
    "   - Remember you can bring a custom rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKAmujbKgUBj"
   },
   "source": [
    "## Next Steps\n",
    "- Customize unit tests for your use case\n",
    "- Integrate with your evaluation pipeline\n",
    "- Monitor and adjust based on results\n",
    "\n",
    "For more information, visit:\n",
    "- LMUnit documentation: [https://contextual.ai/blog/lmunit/](https://contextual.ai/blog/lmunit/)\n",
    "- Get Started notebooks: [https://github.com/ContextualAI/examples/tree/main/01-getting-started](https://github.com/ContextualAI/examples/tree/main/01-getting-started)\n",
    "- Contextual Documentation: [https://docs.contextual.ai/](https://docs.contextual.ai)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
