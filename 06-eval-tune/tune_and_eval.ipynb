{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cba3f87-ee36-496f-9921-d5b086a30a8f",
   "metadata": {},
   "source": [
    "# Tune & Evaluation Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57126865-7da0-4157-9d0a-63e67ef0bf2c",
   "metadata": {},
   "source": [
    "In the Beginner‚Äôs Guide, we went through the process of creating an API Key, creating a Datastore and ingesting documents, creating an Agent, and querying the Agent. This guide covers the next steps of tuning and evaluating your Agent. Make sure you‚Äôve gone through all the steps in the Beginner‚Äôs Guide first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e46973-58d7-4e0d-b9e6-7fd97c85acd5",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b49df2-78a4-4dff-afed-889adb963d16",
   "metadata": {},
   "source": [
    "We've created a powerful set of APIs that enable you to specialize Agents to your data. Tuning often leads to significant improvements in performance for your specific use cases.\n",
    "\n",
    "### 1. Create a tune job\n",
    "\n",
    "To create a tune job, you need a training file and can optionally provide a test file. If no test file is provided, the API will automatically perform a train-test split on the training file.\n",
    "\n",
    "The API expects the data to be in JSON format with four required fields: `guideline,prompt,reference,knowledge`. See the [API docs](https://docs.contextual.ai/reference/create_tune_job_agents__agent_id__tune_post) for an explanation of each of these fields. Here is a [dummy example of what a tune set should look like](https://drive.google.com/drive/folders/1exULG56OXIquVI7N7NRSD4TKyPWATgXR?usp=drive_link):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aafe45ef-3080-43b7-a97b-ca07bc7d4d90",
   "metadata": {},
   "source": [
    "[\n",
    "  {\n",
    "    \"guideline\": \"The answer should be accurate.\",\n",
    "    \"prompt\": \"What was last quarter's revenue?\",\n",
    "    \"reference\": \"According to recent reports, the Q3 revenue was $1.2 million, a 0.1 million increase from Q2.\",\n",
    "    \"knowledge\": [\n",
    "        \"Quarterly report: Q3 revenue was $1.2 million.\",\n",
    "        \"Quarterly report: Q2 revenue was $1.1 million.\",\n",
    "        ...\n",
    "    ],\n",
    "  },\n",
    "  ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792ece2-f08d-44b3-9cbc-b025dd3d4a63",
   "metadata": {},
   "source": [
    "Use the following command to create a tune job. You will need to pass in the `agent_id` and `file_path` for your training file. If you do not provide a `model_id`, we will automatically use the Agent‚Äôs default model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66713340",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%pip install contextual-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7be29",
   "metadata": {},
   "source": [
    "Insert your API key here üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1785da18-1628-4301-8d6b-50597e19da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTUAL_API_KEY=\"key-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267a756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from contextual import ContextualAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc41a98-425a-41c0-bf1b-d2bda5b994d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a client\n",
    "client = ContextualAI(\n",
    "    api_key=CONTEXTUAL_API_KEY,\n",
    ")\n",
    "\n",
    "# test the API Key\n",
    "try:\n",
    "    response = create_agent_output = client.agents.list()\n",
    "    print(\"Valid API Key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Invalid API Key: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9978c4-51b3-4e63-a174-45542c18b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with name 'My First Agent'\n",
    "try:\n",
    "    create_agent_output = client.agents.create(\n",
    "        name=\"My First Agent\"\n",
    "    )\n",
    "    print(create_agent_output.model_dump_json())\n",
    "    agent_id = create_agent_output.id\n",
    "except Exception as e:\n",
    "    print(f\"Encountered error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/Dummy_TuneSet.csv'):\n",
    "    print(f\"Fetching data/Dummy_TuneSet.csv\")\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/02-tune-eval/data/Dummy_TuneSet.csv\")\n",
    "    with open('data/Dummy_TuneSet.csv', 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fc6ff-75bc-43a5-80e0-7b59fc9efcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset file\n",
    "with open(\"data/Dummy_TuneSet.json\", 'rb') as training_file:\n",
    "    try:\n",
    "        response = client.agents.tune.create(\n",
    "            agent_id=agent_id,\n",
    "            training_file=training_file,\n",
    "        )\n",
    "        job_id=response.id\n",
    "        print(response.to_dict())\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7da6f4-a839-43ab-9aad-51db81c4ef69",
   "metadata": {},
   "source": [
    "When the command runs you‚Äôll be returned a `job_id` for the tune job. Keep in mind that tuning will take several hours to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca80018-3dcd-4e4b-8544-a0178b8ba9e9",
   "metadata": {},
   "source": [
    "### 2. Check the status of the tune job\n",
    "\n",
    "You can check the status of the tune job by passing in the `agent_id` and `job_id`. When the job is complete, the status will change from processing to completed. The response payload will also contain the tuned `model_id` and the `evaluation_results` of the tuned model. The following code waits for the job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d968-13c3-4402-ab3e-490ef57f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.agents.tune.jobs.metadata(\n",
    "    agent_id=agent_id,\n",
    "    job_id=job_id,\n",
    ")\n",
    "response.job_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974ac4a-be35-42d2-b621-ad318aa8bcd6",
   "metadata": {},
   "source": [
    "### 3. Deploy the tuned model\n",
    "\n",
    "Before you can use the tuned model, you need to deploy it to your Agent. You can do so by editing the configuration of your Agent and passing in the tuned `model_id`. Currently, we only allow a single fine-tuned model to be deployed per tenant. Please see the [API docs](https://docs.contextual.ai/reference/edit_agent_agents__agent_id__put) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac9022-450b-43e1-8a58-4632780c1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model_id we just trained\n",
    "try:\n",
    "    response = client.agents.tune.jobs.metadata(\n",
    "        agent_id=agent_id,\n",
    "        job_id=job_id,\n",
    "    )\n",
    "    model_id = response.model_id\n",
    "    print(f\"model_id: {model_id}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e25ed-ad03-4014-bf14-495f27b3c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.agents.update(\n",
    "        llm_model_id=model_id,\n",
    "    )\n",
    "    print(response.to_dict())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fd252-04ed-43d4-9d79-efbc5eac7af6",
   "metadata": {},
   "source": [
    "The deployment might take a moment to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdba6e1-fbe8-432b-93f0-2f855f9f6b0c",
   "metadata": {},
   "source": [
    "### 4. Query your tuned model!\n",
    "After you have deployed the tuned model, you can now query it with the usual command. Make sure you pass your new tuned model_id in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81169a0b-4191-45d2-9235-85936bb23bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = client.agents.query.create(\n",
    "        agent_id=agent_id,\n",
    "        llm_model_id=model_id,\n",
    "        messages=[{\n",
    "            # Input your question here\n",
    "            \"content\": \"What is the revenue of Apple?\",\n",
    "            \"role\": \"user\",\n",
    "        }]\n",
    "    )\n",
    "    print(query.message.content)\n",
    "except Exception as e:\n",
    "    print(f\"Encountered error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f1a42-2988-41ac-a048-df6b1d062ce6",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f0cc8-c5c8-45c8-93c9-48ab502091e8",
   "metadata": {},
   "source": [
    "Evaluation endpoints allow you to evaluate your Agent using a set of prompts (questions) and reference (gold) answers. We support two metrics: equivalence and groundedness.\n",
    "\n",
    "* The first metric (‚Äùequivalence‚Äù) evaluates if the Agent response is equivalent to the ground truth (model-driven binary classification).\n",
    "* The second metric (‚Äùgroundedness‚Äù) decomposes the Agent response into claims and then evaluates if the claims are grounded by the retrieved documents.\n",
    "### 1. Create an evaluation job.\n",
    "You will need to provide the evaluation data. You can provide the evaluation data in two ways: (i) by uploading an `evalset_file` as a CSV or (ii) creating an eval `Dataset` through the Dataset API. We will be focusing on (i), but you can read about (ii) in our API Docs.\n",
    "\n",
    "The API expects the data to be in CSV format with two required columns: `prompt`,`reference`. `prompt` is the question, while `reference` is the correct ground truth answer. See the [API docs](https://docs.contextual.ai/reference/create_evaluation_agents__agent_id__evaluate_post) for an explanation of each of these fields. Here is a [dummy example of what an eval set should look like](https://drive.google.com/drive/folders/1exULG56OXIquVI7N7NRSD4TKyPWATgXR?usp=drive_link):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1560c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/Dummy_EvalSet.csv'):\n",
    "    print(f\"Fetching data/Dummy_EvalSet.csv\")\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/02-tune-eval/data/Dummy_EvalSet.csv\")\n",
    "    with open('data/Dummy_EvalSet.csv', 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d44e609-6844-40d2-82bf-7816df95ed45",
   "metadata": {},
   "source": [
    "{\n",
    " \"prompt\": \"What was the sales of Apple at the end of Q3 2022?\",\n",
    " \"reference\": \"Apple's sales was 100 million in the quarter ending Aug 31, 2022.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cb91d-caac-491c-be8a-08e697096fac",
   "metadata": {},
   "source": [
    "Use the following command to create your evaluation job. You will need to pass in your `agent_id` and `file_path` to your evaluation set. In the example below, we are evaluating on both equivalence and groundedness, but you can choose to evaluate only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "779bb243-2190-496a-a852-e24e6b1e9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Dummy_EvalSet.csv', 'rb') as f:\n",
    "    eval_result = client.agents.evaluate.create(\n",
    "        agent_id=agent_id,\n",
    "        metrics=[\"equivalence\", \"groundedness\"],\n",
    "        evalset_file=f\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210eb55f-a81c-42bb-8701-fa71c854d1b6",
   "metadata": {},
   "source": [
    "### 2. Check the status of your evaluation job.\n",
    "You can use the following command to check the status of your evaluation job, where you‚Äôll need to pass in your `agent_id` and evaluation `job_id`. If the evaluation job has completed, you will see your evaluation `metrics` , `job_metadata`, and the `dataset_name` where your eval metrics and row-by-row results are stored (you will need to use the `/datasets` API to view this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7342bf4b-22b8-4538-a7e2-1aa549bd3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_status = client.agents.evaluate.jobs.metadata(agent_id=agent_id, job_id=eval_result.id)\n",
    "from tqdm import tqdm\n",
    "\n",
    "progress = tqdm(total=eval_status.job_metadata.num_predictions)\n",
    "progress.update(eval_status.job_metadata.num_processed_predictions)\n",
    "progress.set_description(\"Evaluation Progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44721d1c-06f7-4c9e-aacb-000eabe9bcbc",
   "metadata": {},
   "source": [
    "### 3. View your evaluation results.\n",
    "In Step 2, you should be able to get a dataset_name when your evaluation job has completed. You can then view your raw evaluation results (equivalence and/or groundedness scores for each question-response pair) with the `/datasets` endpoint. You can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86dbb13f-f0fe-4441-b4d4-28fb03c4e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = client.agents.datasets.evaluate.retrieve(dataset_name=eval_status.dataset_name, agent_id=agent_id)\n",
    "print(eval_results)\n",
    "\n",
    "eval_objects = [json.loads(line) for line in eval_results.splitlines()]\n",
    "\n",
    "df = pd.DataFrame(eval_objects)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708bf315-1f7b-4a16-8f3a-9e500d21930b",
   "metadata": {},
   "source": [
    "\n",
    "üéâ That was a quick spin-through our tune and eval endpoints! To learn more about our APIs and their capabilities, visit [docs.contextual.ai](https://docs.contextual.ai). We look forward to seeing what you build with our platform üèóÔ∏è.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
