{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iPJnDH0HhXB"
      },
      "source": [
        "<img src=\"https://imagedelivery.net/Dr98IMl5gQ9tPkFM5JRcng/3e5f6fbd-9bc6-4aa1-368e-e8bb1d6ca100/Ultra\" alt=\"Image description\" width=\"160\" />\n",
        "\n",
        "<br/>\n",
        "\n",
        "# Specialization Deep Dive\n",
        "\n",
        "This notebook provides a deep dive on specializing or improving your Contextual AI agents. It focuses on showing you the specific settings, but to dive deeper into the usefulness of the settings, please consult full documentation available at [docs.contextual.ai](https://docs.contextual.ai/)\n",
        "\n",
        "This notebook covers the following steps:\n",
        "- Queries / Retrievals\n",
        "- Evaluation Job\n",
        "- Modifying System Prompt\n",
        "- Retrieval Settings\n",
        "- Filter Model / Prompt\n",
        "- Generation Settings\n",
        "- Tuning the Agent\n",
        "\n",
        "For getting usage data and agent feedback, check out example using the metrics API in the [quick start notebook](https://github.com/ContextualAI/examples/tree/main/01-getting-started/quick-start.ipynb).\n",
        "\n",
        "\n",
        "The notebook requires you to first build an agent going through the [getting started](https://github.com/ContextualAI/examples/tree/main/01-getting-started) or the [hands on lab](https://github.com/ContextualAI/examples/tree/main/02-hands-on-lab).\n",
        "\n",
        "To run this notebook interactively, you can open it in Google Colab.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/06-improve-agent-performance/improvement-overview.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "cim9P99PQP1w",
        "outputId": "d87bf3f4-b624-4e95-fb74-dc71a1ef5b9d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict\n",
        "from IPython.display import display, JSON\n",
        "import pandas as pd\n",
        "from contextual import ContextualAI\n",
        "import ast\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modify this with your API key, for best practices use environment variables and don't hardcode your API key\n",
        "API_KEY = os.environ[\"CONTEXTUAL_API_KEY\"]\n",
        "client = ContextualAI(api_key = API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84FKJtqnQt3x"
      },
      "source": [
        "Load up the files you will need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUk8u_lSOArL",
        "outputId": "a79c80a1-5772-4078-e32b-b58119fd32dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching data/eval_short.csv\n",
            "Fetching data/fin_train.jsonl\n"
          ]
        }
      ],
      "source": [
        "def fetch_file(filepath):\n",
        "    if not os.path.exists(os.path.dirname(filepath)):  # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)  # Create if not exists\n",
        "\n",
        "    print(f\"Fetching {filepath}\")\n",
        "    response = requests.get(f\"https://raw.githubusercontent.com/ContextualAI/examples/main/01-getting-started/{filepath}\")\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "fetch_file('data/eval_short.csv')\n",
        "fetch_file('data/fin_train.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFq4oMe9gMuz"
      },
      "source": [
        "## 1: Queries and Retrievals\n",
        "\n",
        "A first step to understanding our agent is passing it queries. Contextual AI will return a response.\n",
        "\n",
        "Besides the model response, it's also possible to retrieve the full text of all the attributions/citations. You can also retrieve images of the bounding boxes for attributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFhZ-h3vSi2l"
      },
      "source": [
        "Let's start with the prebuilt agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WvlwoXzSQP1w"
      },
      "outputs": [],
      "source": [
        "agent_id = 'eea5bd52-e207-4761-98ff-e2f6536eb5ee'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVa5n-_9Sop-"
      },
      "source": [
        "The query here also includes the optional parameter for including the retrieval contexts. Normally, you would set this to false for faster retrieval. However, here I wanted to show you the full text of information available to developers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bvc2MkpDQP1x"
      },
      "outputs": [],
      "source": [
        "query_result = client.agents.query.create(\n",
        "    agent_id=agent_id,\n",
        "    messages=[{\n",
        "        # Input your question here\n",
        "        \"content\": \"Tell about Apple's sales\",\n",
        "        \"role\": \"user\"\n",
        "    }],\n",
        "    include_retrieval_content_text = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I can now see the results of the query. If you are using the financial RAG agent with the Apple 10-Q, you some formated markdown with sales by product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple's total net sales for the quarter ended December 31, 2022 were $117.2 billion, representing a 5% decrease from $123.9 billion in the same quarter of 2021.[2]()[3]() This decline was primarily attributed to foreign currency weakness against the U.S. dollar.[1]()\n",
            "\n",
            "Product Segment Performance:\n",
            "- iPhone: $65.8 billion (down 8% year-over-year)[2]()[3]()\n",
            "- Mac: $7.7 billion (down 29% year-over-year)[2]()[3]()\n",
            "- iPad: $9.4 billion (up 30% year-over-year)[2]()[3]()\n",
            "- Wearables, Home and Accessories: $13.5 billion (down 8% year-over-year)[2]()[3]()\n",
            "- Services: $20.8 billion (up 6% year-over-year)[2]()[3]()\n",
            "\n",
            "The iPhone sales decrease was primarily due to lower sales of new iPhone models launched in the fourth quarter of 2022.[4]() The Mac segment decline was mainly driven by decreased MacBook Pro sales.[5]()\n"
          ]
        }
      ],
      "source": [
        "content = query_result.message.content\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNOmbhFiaIAX"
      },
      "source": [
        "Here I show the first document retrieved that was relevant to the query. If you are using the financial RAG agent with the Apple 10-Q, you should see a text chunk devoted to Apple's product sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aR-rCnf5QP1x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Section: FORM 10-Q\n",
            "Sub-Section: iPhone\n",
            "\n",
            "\n",
            "iPhone net sales decreased during the first quarter of 2023 compared to the same quarter in 2022 due primarily to lower net sales from the Company’s new iPhone models launched in the fourth quarter of 2022.\n"
          ]
        }
      ],
      "source": [
        "context_text = query_result.retrieval_contents[3].content_text\n",
        "print(context_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For getting usage data and agent feedback, check out example using the metrics API in the [quick start notebook](https://github.com/ContextualAI/examples/tree/main/01-getting-started/quick-start.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VTZGxEMQP1x"
      },
      "source": [
        "## 2: Running Evaluation Jobs\n",
        "\n",
        "Contextual AI offer two different endpoints for running evaluation jobs.\n",
        "  - Evaluation based on ground truth examples\n",
        "  - Natural language unit tests (LMUnit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Pd4p7STZSx"
      },
      "source": [
        "### 2.1 Eval Endpoints\n",
        "\n",
        "The eval endpoints allow you to evaluate your Agent using a set of prompts (questions) and reference (gold) answers. We support two metrics: equivalence and groundedness.\n",
        "\n",
        "- Equivalance evaluates if the Agent response is equivalent to the ground truth (model-driven binary classification).  \n",
        "- Groundedness decomposes the Agent response into claims and then evaluates if the claims are grounded by the retrieved documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1A25lECQP1y"
      },
      "source": [
        "An evaluation datasets should consist of prompts (questions) and reference (gold) answers.\n",
        "Lets walk through scoring an evaluation dataset. This dataset comes from the [getting started](https://github.com/ContextualAI/examples/tree/main/01-getting-started) and [hands on lab](https://github.com/ContextualAI/examples/tree/main/02-hands-on-lab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zeue_glVQP1y"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "prompt",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "reference",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "3b1d216f-422a-4f7f-a96e-a8192d575d10",
              "rows": [
                [
                  "0",
                  "What was Apple's total net sales for 2022?",
                  "Apple's total net sales for the three months ended December 31, 2022, were $117,154 million"
                ],
                [
                  "1",
                  "What was Apple's Services revenue in Q1 2023 and what was the year-over-year growth rate?",
                  "Apple's Services revenue was $20,766 million in Q1 2023, up from $19,516 million in Q1 2022, representing a 6% increase."
                ],
                [
                  "2",
                  "What was Apple's iPhone revenue in Q1 2026?",
                  "I am an AI assistant created by Contextual AI. I don't have relevant documentation about that topic, but feel free to ask me something else!"
                ],
                [
                  "3",
                  "How much did Apple spend on research and development in 2022?",
                  "Apple spent $7,709 million on research and development in Q1 2023."
                ],
                [
                  "4",
                  "What is the next amazing product from Apple?",
                  "I am an AI assistant created by Contextual AI. I don't have relevant documentation about that topic, but feel free to ask me something else!"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>reference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What was Apple's total net sales for 2022?</td>\n",
              "      <td>Apple's total net sales for the three months e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What was Apple's Services revenue in Q1 2023 a...</td>\n",
              "      <td>Apple's Services revenue was $20,766 million i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What was Apple's iPhone revenue in Q1 2026?</td>\n",
              "      <td>I am an AI assistant created by Contextual AI....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How much did Apple spend on research and devel...</td>\n",
              "      <td>Apple spent $7,709 million on research and dev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the next amazing product from Apple?</td>\n",
              "      <td>I am an AI assistant created by Contextual AI....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              prompt  \\\n",
              "0         What was Apple's total net sales for 2022?   \n",
              "1  What was Apple's Services revenue in Q1 2023 a...   \n",
              "2        What was Apple's iPhone revenue in Q1 2026?   \n",
              "3  How much did Apple spend on research and devel...   \n",
              "4       What is the next amazing product from Apple?   \n",
              "\n",
              "                                           reference  \n",
              "0  Apple's total net sales for the three months e...  \n",
              "1  Apple's Services revenue was $20,766 million i...  \n",
              "2  I am an AI assistant created by Contextual AI....  \n",
              "3  Apple spent $7,709 million on research and dev...  \n",
              "4  I am an AI assistant created by Contextual AI....  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval = pd.read_csv('data/eval_short.csv')\n",
        "eval.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xzYph7PQP1y"
      },
      "source": [
        "Start an evaluation job that measures equivalence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YlLkLBJEQP1y"
      },
      "outputs": [],
      "source": [
        "with open('data/eval_short.csv', 'rb') as f:\n",
        "    eval_result = client.agents.evaluate.create(\n",
        "        agent_id=agent_id,\n",
        "        metrics=[\"equivalence\", \"groundedness\"],\n",
        "        evalset_file=f\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNuMZBZMQP1y"
      },
      "source": [
        "Evaluation jobs can take time, especially longer ones. Here is how you can check on their status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bPGVo6DyQP1y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'83c54e9d-ef49-4143-84d7-a2027fbb38d8'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_result.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pb-H2cDcQP1y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation Progress: 100%|██████████| 12/12 [00:00<00:00, 83330.54it/s]"
          ]
        }
      ],
      "source": [
        "eval_status = client.agents.evaluate.jobs.metadata(agent_id=agent_id, job_id=eval_result.id)\n",
        "\n",
        "progress = tqdm(total=eval_status.job_metadata.num_predictions)\n",
        "progress.update(eval_status.job_metadata.num_processed_predictions)\n",
        "progress.set_description(\"Evaluation Progress\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZnYjw8mQP1y"
      },
      "source": [
        "View our evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8n-7k-gDQP1y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EvaluationJobMetadata(dataset_name='eval_short_83c54e9d-ef49-4143-84d7-a2027fbb38d8_results', job_metadata=JobMetadata(num_failed_predictions=0, num_predictions=12, num_successful_predictions=12, num_processed_predictions=12), metrics={'equivalence_score': {'score': 0.5833333333333334}, 'groundedness_score': {'score': 0.47222222222222227}}, status='completed')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAySoPqlUi8t"
      },
      "source": [
        "Let's download the reesults and show it in pandas for the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_binary_evaluation(binary_response):\n",
        "    \"\"\"\n",
        "    Process BinaryAPIResponse into a pandas DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        binary_response: BinaryAPIResponse from evaluate.retrieve\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Processed evaluation data\n",
        "    \"\"\"\n",
        "    # Read the binary content\n",
        "    content = binary_response.read()\n",
        "    \n",
        "    # Now decode the content\n",
        "    lines = content.decode('utf-8').strip().split('\\n')\n",
        "    \n",
        "    # Parse each line and flatten the results\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            \n",
        "            # Parse the results string if it exists\n",
        "            if 'results' in entry:\n",
        "                results = ast.literal_eval(entry['results'])\n",
        "                del entry['results']\n",
        "                if isinstance(results, dict):\n",
        "                    for key, value in results.items():\n",
        "                        if isinstance(value, dict):\n",
        "                            for subkey, subvalue in value.items():\n",
        "                                entry[f'{key}_{subkey}'] = subvalue\n",
        "                        else:\n",
        "                            entry[key] = value\n",
        "            \n",
        "            data.append(entry)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing line: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "agsodkxYQP1z"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "prompt",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "reference",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "response",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "guideline",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "knowledge",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "status",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "equivalence_score_score",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "equivalence_score_metadata",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "factuality_v4.5_score_score",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "factuality_v4.5_score_metadata",
                  "rawType": "object",
                  "type": "unknown"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "16e49d8e-2a77-4166-8d24-2b1aefde4513",
              "rows": [
                [
                  "0",
                  "What was Apple's total net sales for 2022?",
                  "Apple's total net sales for the three months ended December 31, 2022, were $117,154 million",
                  "I apologize, but I don't have access to Apple's financial data for 2022. For the most accurate and up-to-date information about Apple's net sales, I recommend checking their official financial reports or SEC filings.",
                  "",
                  "[]",
                  "completed",
                  "0.0",
                  "The generated response does not provide the requested information about Apple's total net sales for 2022, instead offering a suggestion to check official financial reports or SEC filings.",
                  "0.0",
                  "{'description': 'There are claims but no knowledge so response is ungrounded.'}"
                ],
                [
                  "1",
                  "What was Apple's Services revenue in Q1 2023 and what was the year-over-year growth rate?",
                  "Apple's Services revenue was $20,766 million in Q1 2023, up from $19,516 million in Q1 2022, representing a 6% increase.",
                  "I am an AI assistant created by Contextual AI. I don't have relevant documentation about that topic, but feel free to ask me something else!",
                  "",
                  "[]",
                  "completed",
                  "0.0",
                  "The generated response does not provide any information about Apple's Services revenue in Q1 2023 or the year-over-year growth rate, rendering it non-equivalent to the reference response.",
                  "0.0",
                  "{'description': 'There are claims but no knowledge so response is ungrounded.'}"
                ],
                [
                  "2",
                  "What was Apple's iPhone revenue in Q1 2026?",
                  "I am an AI assistant created by Contextual AI. I don't have relevant documentation about that topic, but feel free to ask me something else!",
                  "I am an AI assistant created by Contextual AI. I don't have relevant documentation about that topic, but feel free to ask me something else!",
                  "",
                  "[]",
                  "completed",
                  "1.0",
                  "Both responses are semantically equivalent as they both state the AI assistant's inability to provide information on the topic, despite not directly answering the user's query.",
                  "0.0",
                  "{'description': 'There are claims but no knowledge so response is ungrounded.'}"
                ],
                [
                  "3",
                  "How much did Apple spend on research and development in 2022?",
                  "Apple spent $7,709 million on research and development in Q1 2023.",
                  "For the quarter ended December 31, 2022, Apple's research and development (R&D) expenses were $7,709 million. This represented 7% of their total net sales for the quarter.Based on my analysis, I cannot provide the full-year R&D spending for 2022 as I only have quarterly data. However, I can tell you that this represents a significant increase from the same quarter in the previous year, when R&D spending was $6,306 million (5% of net sales).",
                  "",
                  "[\"Section: FORM 10-Q\\nSub-Section: Operating Expenses\\n\\n\\nOperating expenses for the three months ended December 31, 2022 and December\\u00a025, 2021 were as follows (dollars in millions):\\n|  | Three Months Ended |  |\\n|-------------------------------------|----------------------|-------------------|\\n|  | December 31, 2022 | December 25, 2021 |\\n| Research and development | $ 7,709 | $ 6,306 |\\n| Percentage of total net sales | 7\\u00a0% | 5\\u00a0% |\\n| Selling, general and administrative | $ 6,607 | $ 6,449 |\\n| Percentage of total net sales | 6\\u00a0% | 5\\u00a0% |\\n| Total operating expenses | $ 14,316 | $ 12,755 |\\n| Percentage of total net sales | 12\\u00a0% | 10\\u00a0% |\"]",
                  "completed",
                  "1.0",
                  "The generated response provides the same core information as the reference response, but with additional context and a clarification that the data is for a specific quarter, not the full year.",
                  "0.8333333333333334",
                  "{'claim_scores': [{'score': 'Supported', 'claim': \"For the quarter ended December 31, 2022, Apple's research and development (R&D) expenses were $7,709 million.\"}, {'score': 'Supported', 'claim': \"Apple's R&D expenses for the quarter ended December 31, 2022, represented 7% of their total net sales for the quarter.\"}, {'score': 'Unsupported', 'claim': 'The author cannot provide the full-year R&D spending for 2022 as they only have quarterly data.'}, {'score': 'Supported', 'claim': \"In the same quarter of the previous year, Apple's R&D spending was $6,306 million.\"}, {'score': 'Supported', 'claim': \"In the same quarter of the previous year, Apple's R&D spending represented 5% of net sales.\"}, {'score': 'Supported', 'claim': \"Apple's R&D spending for the quarter ended December 31, 2022, represents a significant increase from the same quarter in the previous year.\"}]}"
                ],
                [
                  "4",
                  "What is the next amazing product from Apple?",
                  "I am an AI assistant created by Contextual AI. I don't have relevant documentation about that topic, but feel free to ask me something else!",
                  "Apple's Recent Product Announcements:During the first quarter of 2023, Apple announced three new products:- A new iPad- A new iPad Pro powered by the Apple M2 chip- A new Apple TV 4K",
                  "",
                  "[\"Section: FORM 10-Q\\nSub-Section: Quarterly Highlights\\n\\n\\nTotal net sales decreased 5% or $6.8 billion during the first quarter of 2023 compared to the same quarter in 2022 due to the weakness in foreign currencies relative to the U.S. dollar. The weakness in foreign currencies contributed to lower net sales of iPhone and Mac, which was partially offset by higher net sales of iPad. \\u00ae During the first quarter of 2023, the Company announced a new iPad, a new iPad Pro  powered by the Apple M2 chip, and a new Apple TV 4K. The Company repurchased $19.0 billion of its common stock and paid dividends and dividend equivalents of $3.8 billion during the first quarter of 2023.\"]",
                  "completed",
                  "0.0",
                  "The generated response provides specific information about Apple's recent product announcements, whereas the reference response states a lack of knowledge on the topic, making them fundamentally different in core information.",
                  "1.0",
                  "{'claim_scores': [{'score': 'Supported', 'claim': 'Apple announced three new products during the first quarter of 2023.'}, {'score': 'Supported', 'claim': 'One of the new products announced by Apple is a new iPad.'}, {'score': 'Supported', 'claim': 'One of the new products announced by Apple is a new iPad Pro powered by the Apple M2 chip.'}, {'score': 'Supported', 'claim': 'One of the new products announced by Apple is a new Apple TV 4K.'}]}"
                ]
              ],
              "shape": {
                "columns": 10,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>reference</th>\n",
              "      <th>response</th>\n",
              "      <th>guideline</th>\n",
              "      <th>knowledge</th>\n",
              "      <th>status</th>\n",
              "      <th>equivalence_score_score</th>\n",
              "      <th>equivalence_score_metadata</th>\n",
              "      <th>factuality_v4.5_score_score</th>\n",
              "      <th>factuality_v4.5_score_metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What was Apple's total net sales for 2022?</td>\n",
              "      <td>Apple's total net sales for the three months e...</td>\n",
              "      <td>I apologize, but I don't have access to Apple'...</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>completed</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The generated response does not provide the re...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{'description': 'There are claims but no knowl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What was Apple's Services revenue in Q1 2023 a...</td>\n",
              "      <td>Apple's Services revenue was $20,766 million i...</td>\n",
              "      <td>I am an AI assistant created by Contextual AI....</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>completed</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The generated response does not provide any in...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{'description': 'There are claims but no knowl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What was Apple's iPhone revenue in Q1 2026?</td>\n",
              "      <td>I am an AI assistant created by Contextual AI....</td>\n",
              "      <td>I am an AI assistant created by Contextual AI....</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td>completed</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Both responses are semantically equivalent as ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>{'description': 'There are claims but no knowl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How much did Apple spend on research and devel...</td>\n",
              "      <td>Apple spent $7,709 million on research and dev...</td>\n",
              "      <td>For the quarter ended December 31, 2022, Apple...</td>\n",
              "      <td></td>\n",
              "      <td>[\"Section: FORM 10-Q\\nSub-Section: Operating E...</td>\n",
              "      <td>completed</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The generated response provides the same core ...</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>{'claim_scores': [{'score': 'Supported', 'clai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the next amazing product from Apple?</td>\n",
              "      <td>I am an AI assistant created by Contextual AI....</td>\n",
              "      <td>Apple's Recent Product Announcements:During th...</td>\n",
              "      <td></td>\n",
              "      <td>[\"Section: FORM 10-Q\\nSub-Section: Quarterly H...</td>\n",
              "      <td>completed</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The generated response provides specific infor...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>{'claim_scores': [{'score': 'Supported', 'clai...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              prompt  \\\n",
              "0         What was Apple's total net sales for 2022?   \n",
              "1  What was Apple's Services revenue in Q1 2023 a...   \n",
              "2        What was Apple's iPhone revenue in Q1 2026?   \n",
              "3  How much did Apple spend on research and devel...   \n",
              "4       What is the next amazing product from Apple?   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Apple's total net sales for the three months e...   \n",
              "1  Apple's Services revenue was $20,766 million i...   \n",
              "2  I am an AI assistant created by Contextual AI....   \n",
              "3  Apple spent $7,709 million on research and dev...   \n",
              "4  I am an AI assistant created by Contextual AI....   \n",
              "\n",
              "                                            response guideline  \\\n",
              "0  I apologize, but I don't have access to Apple'...             \n",
              "1  I am an AI assistant created by Contextual AI....             \n",
              "2  I am an AI assistant created by Contextual AI....             \n",
              "3  For the quarter ended December 31, 2022, Apple...             \n",
              "4  Apple's Recent Product Announcements:During th...             \n",
              "\n",
              "                                           knowledge     status  \\\n",
              "0                                                 []  completed   \n",
              "1                                                 []  completed   \n",
              "2                                                 []  completed   \n",
              "3  [\"Section: FORM 10-Q\\nSub-Section: Operating E...  completed   \n",
              "4  [\"Section: FORM 10-Q\\nSub-Section: Quarterly H...  completed   \n",
              "\n",
              "   equivalence_score_score                         equivalence_score_metadata  \\\n",
              "0                      0.0  The generated response does not provide the re...   \n",
              "1                      0.0  The generated response does not provide any in...   \n",
              "2                      1.0  Both responses are semantically equivalent as ...   \n",
              "3                      1.0  The generated response provides the same core ...   \n",
              "4                      0.0  The generated response provides specific infor...   \n",
              "\n",
              "   factuality_v4.5_score_score  \\\n",
              "0                     0.000000   \n",
              "1                     0.000000   \n",
              "2                     0.000000   \n",
              "3                     0.833333   \n",
              "4                     1.000000   \n",
              "\n",
              "                      factuality_v4.5_score_metadata  \n",
              "0  {'description': 'There are claims but no knowl...  \n",
              "1  {'description': 'There are claims but no knowl...  \n",
              "2  {'description': 'There are claims but no knowl...  \n",
              "3  {'claim_scores': [{'score': 'Supported', 'clai...  \n",
              "4  {'claim_scores': [{'score': 'Supported', 'clai...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_results = client.agents.datasets.evaluate.retrieve(dataset_name=eval_status.dataset_name, agent_id = agent_id)\n",
        "df = process_binary_evaluation(eval_results)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's filter this down and look at the rows where equivalance was less than 1. We should prioritize these for our error analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_df = df[df['equivalence_score_score'] != 1]\n",
        "\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save out the results\n",
        "df.to_csv('eval_results_python.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59LCCn_WQP1z"
      },
      "source": [
        "### 2.2 LMUnit\n",
        "\n",
        "The `lmunit` endpoint supports natural language unit tests. To learn more, check out the [blog post](https://contextual.ai/blog/lmunit/) or check out a [notebook using LMUnit](https://github.com/ContextualAI/examples/tree/main/03-lmunit).\n",
        "Here is a simple example of natural langauge unit test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CnC-RTJUVMRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.072\n"
          ]
        }
      ],
      "source": [
        "response = client.lmunit.create(\n",
        "                    query=\"What material is used in N95 masks?\",\n",
        "                    response=\"N95 masks are made primarily of polypropylene. This synthetic material is created through a melt-blowing process that creates multiple layers of microfibers. The material was chosen because it can be electrostatically charged to attract particles. Particles are the constituents of the universe\",\n",
        "                    unit_test=\"Does the response avoid unnecessary information?\"\n",
        "                )\n",
        "print(response.score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output there will be a numerical score on a scale of 1 to 5.\n",
        "The low score here, `2.065`, makes sense given the long response filled with excess information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWWt707mQP1z"
      },
      "source": [
        "## 3: Modifying the System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY5i_pFDVZIj"
      },
      "source": [
        "After initial testing, you may want to revise the system prompt. Here I have an updated prompt with additional information in the critical guidelines section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xi_cliANQP1z"
      },
      "outputs": [],
      "source": [
        "system_prompt2 = '''\n",
        "You are an AI assistant specialized in financial analysis and reporting. Your responses should be precise, accurate, and sourced exclusively from official financial documentation provided to you. Please follow these guidelines:\n",
        "\n",
        "Data Analysis & Response Quality:\n",
        "* Only use information explicitly stated in provided documentation (e.g., earnings releases, financial statements, investor presentations)\n",
        "* Present comparative analyses using structured formats with tables and bullet points where appropriate\n",
        "* Include specific period-over-period comparisons (quarter-over-quarter, year-over-year) when relevant\n",
        "* Maintain consistency in numerical presentations (e.g., consistent units, decimal places)\n",
        "* Flag any one-time items or special charges that impact comparability\n",
        "\n",
        "\n",
        "For any analysis, provide comprehensive insights using all relevant available information while maintaining strict adherence to these guidelines and focusing on delivering clear, actionable information.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARpHSA3dViKG"
      },
      "source": [
        "Let's now update the agent and verify the changes by checking the agent metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ASRr5EPsQP1z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are an AI assistant specialized in financial analysis and reporting. Your responses should be precise, accurate, and sourced exclusively from official financial documentation provided to you. Please follow these guidelines:\n",
            "\n",
            "Data Analysis & Response Quality:\n",
            "* Only use information explicitly stated in provided documentation (e.g., earnings releases, financial statements, investor presentations)\n",
            "* Present comparative analyses using structured formats with tables and bullet points where appropriate\n",
            "* Include specific period-over-period comparisons (quarter-over-quarter, year-over-year) when relevant\n",
            "* Maintain consistency in numerical presentations (e.g., consistent units, decimal places)\n",
            "* Flag any one-time items or special charges that impact comparability\n",
            "\n",
            "\n",
            "For any analysis, provide comprehensive insights using all relevant available information while maintaining strict adherence to these guidelines and focusing on delivering clear, actionable information.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "client.agents.update(agent_id=agent_id, system_prompt=system_prompt2)\n",
        "\n",
        "agent_config = client.agents.metadata(agent_id=agent_id)\n",
        "print (agent_config.system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yai7YC3KVmDD"
      },
      "source": [
        "Modifying the system prompt is useful when trying to improve the response generation. For example, by making it more concise, more professional, or including specific terms that should be part of the response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uLIXp51QP1z"
      },
      "source": [
        "## 4: Retrieval Settings\n",
        "\n",
        "There are a number of settings available for modifying retrieval settings. These are advanced parameters, for most users you will get good results from leaving these as platform defaults.  For more detail on the advanced settings, please refer to the [documentation](https://docs.contextual.ai/).\n",
        "\n",
        "At the global level:\n",
        "- enable_rerank: Enable/disable the use of the reranker model\n",
        "- enable_filter: The filter is a capability in the platform to remove irrelevant or low-quality information before it's used to generate responses.\n",
        "- enable_multi_turn: This feature is experimental and will be improved.\n",
        "\n",
        "Retriever settings\n",
        "- top_k_retrieved_chunks: The number of chunks retrieved at the retriever stage\n",
        "- lexical_alpha: This parameter controls how much weight is given to exact keyword matches when searching through documents.\n",
        "- semantic_alpha: This parameter controls the weight given to semantic search. The total of lexical and semantic should sum to 1.\n",
        "\n",
        "Reranker settings\n",
        "- top_k_reranked_chunks: The number of chunks returned at the reranker stage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rUpThueoQP1z"
      },
      "outputs": [],
      "source": [
        "# Simple update focusing on retrieval parameters\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"retrieval_config\": {\n",
        "                \"top_k_retrieved_chunks\": 10,\n",
        "                \"lexical_alpha\": 0.5,\n",
        "                \"semantic_alpha\": 0.5\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1ghsGvaxQP1z"
      },
      "outputs": [],
      "source": [
        "# Update focusing on filtering and reranking\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"filter_and_rerank_config\": {\n",
        "                \"top_k_reranked_chunks\": 5\n",
        "            },\n",
        "            \"global_config\": {\n",
        "                \"enable_rerank\": True,\n",
        "                \"enable_filter\": True\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz6AH2j5X8vY"
      },
      "source": [
        "Get the agent metadata that will show the retrieval changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_info = client.agents.metadata(agent_id=agent_id)\n",
        "print(agent_info.agent_configs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zuf8DGSVYCM9"
      },
      "outputs": [],
      "source": [
        "# Complete configuration update using all available parameters -- you shouldn't need to change all of these\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"retrieval_config\": {\n",
        "                \"top_k_retrieved_chunks\": 10,\n",
        "                \"lexical_alpha\": 0.5,\n",
        "                \"semantic_alpha\": 0.5\n",
        "            },\n",
        "            \"filter_and_rerank_config\": {\n",
        "                \"top_k_reranked_chunks\": 5\n",
        "            },\n",
        "            \"global_config\": {\n",
        "                \"enable_rerank\": True,\n",
        "                \"enable_filter\": True,\n",
        "                \"enable_multi_turn\": False\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5: Filter Model / Prompt \n",
        "\n",
        "Filter prompts are used to reduce or filter the retrieved documents flowing to the Contextual AI Grounded Language Model (GLM). Specifically, it filter documents coming out of the reranker and prior to the GLM. For background, the flow of retrieved documents is: Retrievers --> Rerank --> Filter Prompt --> GLM\n",
        "\n",
        "A filter prompt is helpful for selecting relevant documents from a larger pool of documents. For example, \"if a query mentions a date, only select docs from within 6 months of that date\" or \"if a query mentions <customer-specific term>, exclude all docs without that term\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The filter prompt recently become customer facing, this snippet will be updated once it's integrated into the python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers = {\n",
        "    \"accept\": \"application/json\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    url = f\"https://api.contextual.ai/v1/agents/{agent_id}\"\n",
        "\n",
        "    payload = {\n",
        "        \"filter_prompt\": \"Always reply with no\"\n",
        "        #\"filter_prompt\": \"\"\n",
        "    }\n",
        "\n",
        "    response = requests.put(\n",
        "        url,\n",
        "        headers=headers,\n",
        "        json=payload\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            metadata_url = f\"https://api.contextual.ai/v1/agents/{agent_id}/metadata\"\n",
        "            metadata_response = requests.get(metadata_url, headers=headers)\n",
        "            print(metadata_response.json())\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Query failed: {str(e)}\")\n",
        "            raise\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Query failed: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify the filter prompt has been modified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_config = client.agents.metadata(agent_id=agent_id)\n",
        "print (agent_config.filter_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This filter prompt will refuse to answer for this example. We can verify that with a new query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_result = client.agents.query.create(\n",
        "    agent_id=agent_id,\n",
        "    messages=[{\n",
        "        \"content\": \"what was the sales for Apple in 2022\",\n",
        "        \"role\": \"user\"\n",
        "    }]\n",
        ")\n",
        "print(query_result.message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXfZf3SnQP10"
      },
      "source": [
        "## 6: Generation Settings\n",
        "\n",
        "There are a number of settings available for modifying the generation of responses. These are advanced settings, please refer to the documentation for more details.\n",
        "\n",
        "- max_new_tokens\n",
        "- temperature\n",
        "- top_p\n",
        "- frequency_penalty\n",
        "- seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9KaBgYPaQP10"
      },
      "outputs": [],
      "source": [
        "# Update focusing on generation parameters\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"generate_response_config\": {\n",
        "                \"max_new_tokens\": 500,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.95,\n",
        "                \"frequency_penalty\": 0.5,\n",
        "                \"seed\": 42\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dRLX7QlAQP10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'retrieval_config': {'top_k_retrieved_chunks': 10, 'lexical_alpha': 0.5, 'semantic_alpha': 0.5, 'num_retrieved_docs': 11}, 'generate_response_config': {'max_new_tokens': 500, 'temperature': 0.7, 'top_p': 0.95, 'frequency_penalty': 0.5}}\n"
          ]
        }
      ],
      "source": [
        "agent_info = client.agents.metadata(agent_id=agent_id)\n",
        "print(agent_info.agent_configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692gSc7ZNyve"
      },
      "source": [
        "## 7: Tune your Agent\n",
        "\n",
        "Contextual AI allows you to tune your entire agent end to end for improved performance. To run a tune job, you need to specify a training file and an optional test file. (If no test file is provided, the tuning job will hold out a portion of the training file as the test set.)\n",
        "\n",
        "A tuning job requires fine tuning models and the expectation should be it will take a couple of hours to run.\n",
        "\n",
        "After the tune job completes, the metadata associated with the tune job will include evaluation results and a model ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmRrBTy0OWzc"
      },
      "source": [
        "### 7.1 Format for the Training File\n",
        "\n",
        "The file should be in JSON array format, where each element of the array is a JSON object represents a single training example. The four required fields are guideline, prompt, reference, and knowledge.\n",
        "\n",
        "- guideline field should be guidelines for the expected response.\n",
        "\n",
        "- prompt field should be a question or statement that the model should respond to.\n",
        "\n",
        "- reference: The gold-standard answer to the prompt.\n",
        "\n",
        "- knowledge field should be an array of strings, each string representing a piece of knowledge that the model should use to generate the response.\n",
        "\n",
        "There is a minimum size of 35 rows for tuning datasets. The `fin_train.jsonl` is a toy sample to illustrate how tuning operates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Rf1DeSP-VXR4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"guideline\": \"The response should analyze key financial metrics and trends.\", \"prompt\": \"What were Apple's key revenue metrics for Q1 2023?\", \"knowledge\": [\"Total net sales were $117.2 billion, down 5% YoY.\", \"Products revenue was $96.4 billion, down from $104.4 billion.\", \"Services revenue grew to $20.8 billion from $19.5 billion.\", \"iPhone revenue decreased 8% YoY to $65.8 billion.\"], \"reference\": \"Apple's Q1 2023 showed total net sales of $117.2 billion (down 5% YoY), with Products revenue declining to $96.4 billion while Services grew to $20.8 billion. iPhone revenue decreased 8% YoY to $65.8 billion.\"}\n",
            "{\"guideline\": \"The response should highlight segment performance and geographic trends.\", \"prompt\": \"How did Apple's geographic segments perform in Q1 2023?\", \"knowledge\": [\"Americas revenue declined 4% to $49.3 billion.\", \"Europe revenue decreased 7% to $27.7 billion.\", \"Greater China revenue fell 7% to $23.9 billion.\", \"Japan revenue declined 5% to $6.8 billion.\", \"Rest of Asia Pacific down 3% to $9.5 billion.\"], \"reference\": \"Apple's Q1 2023 geographic performance showed declines across all regions: Americas (-4% to $49.3B), Europe (-7% to $27.7B), Greater China (-7% to $23.9B), Japan (-5% to $6.8B), and Rest of Asia Pacific (-3% to $9.5B).\"}\n",
            "{\"guideline\": \"The response should analyze profitability metrics and margins.\", \"prompt\": \"What were Apple's key profitability metrics in Q1 2023?\", \"knowledge\": [\"Gross margin was $50.3 billion or 43.0% of revenue.\", \"Operating income was $36.0 billion.\", \"Net income was $30.0 billion.\", \"Diluted earnings per share was $1.88.\"], \"reference\": \"Apple achieved gross margin of $50.3 billion (43.0% of revenue) in Q1 2023, with operating income of $36.0 billion and net income of $30.0 billion, resulting in diluted earnings per share of $1.88.\"}\n",
            "{\"guideline\": \"The response should focus on segment margin analysis.\", \"prompt\": \"How did Apple's Products and Services margins compare in Q1 2023?\", \"knowledge\": [\"Products gross margin was 37.0%, down from 38.4%.\", \"Services gross margin was 70.8%, down from 72.4%.\", \"Products gross margin was impacted by foreign exchange.\", \"Services costs increased while leverage improved.\"], \"reference\": \"Apple's Q1 2023 segment margins showed Products gross margin of 37.0% (down from 38.4%) and Services gross margin of 70.8% (down from 72.4%), with Products impacted by foreign exchange while Services saw higher costs offset by improved leverage.\"}\n",
            "{\"guideline\": \"The response should analyze balance sheet strength and liquidity.\", \"prompt\": \"What was Apple's cash and debt position at the end of Q1 2023?\", \"knowledge\": [\"Cash and marketable securities totaled $165.5 billion.\", \"Total term debt was $109.4 billion.\", \"Commercial paper outstanding was $1.7 billion.\", \"Generated operating cash flow of $34.0 billion in Q1.\"], \"reference\": \"Apple maintained a strong financial position with $165.5 billion in cash and marketable securities against $109.4 billion in term debt and $1.7 billion in commercial paper, while generating $34.0 billion in Q1 operating cash flow.\"}\n",
            "{\"guideline\": \"The response should focus on key business metrics and operational performance.\", \"prompt\": \"How did Apple's product categories perform in Q1 2023?\", \"knowledge\": [\"iPhone revenue declined 8% to $65.8 billion.\", \"Mac revenue fell 29% to $7.7 billion.\", \"iPad revenue grew 30% to $9.4 billion.\", \"Wearables revenue decreased 8% to $13.5 billion.\", \"Services reached an all-time high of $20.8 billion.\"], \"reference\": \"Apple's Q1 2023 product performance was mixed, with iPhone (-8% to $65.8B), Mac (-29% to $7.7B), and Wearables (-8% to $13.5B) declining, while iPad grew 30% to $9.4B and Services hit a record $20.8B.\"}\n",
            "{\"guideline\": \"The response should analyze capital return and shareholder value.\", \"prompt\": \"What actions did Apple take to return capital to shareholders in Q1 2023?\", \"knowledge\": [\"Repurchased $19.0 billion of common stock.\", \"Paid $3.8 billion in dividends and equivalents.\", \"Declared dividend of $0.23 per share.\", \"Reduced share count by approximately 133 million shares.\"], \"reference\": \"Apple returned capital to shareholders through $19.0 billion in share repurchases and $3.8 billion in dividends in Q1 2023, with a $0.23 per share dividend and net reduction of approximately 133 million shares outstanding.\"}\n",
            "{\"guideline\": \"The response should focus on expense management and operational efficiency.\", \"prompt\": \"How did Apple's operating expenses change in Q1 2023?\", \"knowledge\": [\"R&D expenses increased to $7.7 billion from $6.3 billion.\", \"SG&A expenses remained relatively flat at $6.6 billion.\", \"Total operating expenses were $14.3 billion or 12% of revenue.\", \"Headcount-related expenses drove R&D growth.\"], \"reference\": \"Apple's Q1 2023 operating expenses totaled $14.3 billion (12% of revenue), with R&D increasing to $7.7 billion due to higher headcount costs while SG&A remained stable at $6.6 billion.\"}\n",
            "{\"guideline\": \"The response should analyze working capital and operational metrics.\", \"prompt\": \"How did Apple's working capital metrics change in Q1 2023?\", \"knowledge\": [\"Accounts receivable decreased to $23.8 billion.\", \"Inventories increased to $6.8 billion.\", \"Accounts payable decreased to $57.9 billion.\", \"Vendor non-trade receivables were $30.4 billion.\"], \"reference\": \"Apple's Q1 2023 working capital showed accounts receivable of $23.8 billion, increased inventories of $6.8 billion, accounts payable of $57.9 billion, and vendor non-trade receivables of $30.4 billion.\"}\n",
            "{\"guideline\": \"The response should focus on tax strategy and effective rates.\", \"prompt\": \"What was Apple's tax position in Q1 2023?\", \"knowledge\": [\"Effective tax rate was 15.8%.\", \"Provision for income taxes was $5.6 billion.\", \"Lower rate driven by foreign earnings differences.\", \"Benefited from R&D credits and share-based compensation.\"], \"reference\": \"Apple's Q1 2023 effective tax rate was 15.8% with a tax provision of $5.6 billion, benefiting from foreign earnings differentials, R&D credits, and tax benefits from share-based compensation.\"}\n"
          ]
        }
      ],
      "source": [
        "!head data/fin_train.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1tcNqTv0gaQ"
      },
      "source": [
        "### 7.2 Starting a Tuning Model Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVxQ9i00M3cx"
      },
      "outputs": [],
      "source": [
        "# create a dataset file\n",
        "with open(\"data/fin_train.jsonl\", 'rb') as training_file:\n",
        "    response = client.agents.tune.create(\n",
        "        agent_id=agent_id,\n",
        "        training_file=training_file,\n",
        "    )\n",
        "    job_id=response.id\n",
        "    print(response.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGao-yw7yLVY"
      },
      "source": [
        " ### 7.3 Checking the Status.\n",
        "\n",
        " You can check the status of the job. For detailed information, refer to the API documentation. When the tuning job is complete, the status will turn to completed. The response payload will also contain evaluation_results ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBy8pONdyOnu"
      },
      "outputs": [],
      "source": [
        "response = client.agents.tune.jobs.metadata(\n",
        "    agent_id=agent_id,\n",
        "    job_id=job_id,\n",
        ")\n",
        "response.job_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg4YXZdYyR9X"
      },
      "source": [
        "When the tuning job is complete, the metadata would look like the following:\n",
        "```\n",
        "{'job_status': 'completed',\n",
        " 'evaluation_results': {'grounded_generation_train_test.json_equivalence': 1.0,\n",
        "  'grounded_generation_train_test.json_helpfulness': 0.814156498263641,\n",
        "  'grounded_generation_train_test.json_groundedness': 0.7781168677598632},\n",
        " 'model_id': 'registry/model-ada3c484-3ce0f31f-llm-fd6c2'}\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_77uMH5ybBh"
      },
      "source": [
        "### 7.4 Updating the agent\n",
        "Once the tuned job is complete, you can deploy the tuned model via editing the agent through API. Note that currently a single fine-tuned model deployment is allowed per tenant. Please see the API doc for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiuYZrBsybjo"
      },
      "outputs": [],
      "source": [
        "response = client.agents.tune.jobs.metadata(\n",
        "    agent_id=agent_id,\n",
        "    job_id=job_id,\n",
        ")\n",
        "model_id = response.model_id\n",
        "print(f\"model_id: {model_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKPyLDbANMJt"
      },
      "outputs": [],
      "source": [
        "response = client.agents.update(\n",
        "    llm_model_id=model_id,\n",
        ")\n",
        "print(response.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akE9XOq9NTEc"
      },
      "source": [
        "### 7.5 Query your tuned model\n",
        "After you have deployed the tuned model, you can now query it with the usual command. Make sure you pass your new tuned model_id in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT_QBn1pNSOL"
      },
      "outputs": [],
      "source": [
        "query = client.agents.query.create(\n",
        "      agent_id=agent_id,\n",
        "      llm_model_id=model_id,\n",
        "      messages=[{\n",
        "          # Input your question here\n",
        "          \"content\": \"What is the revenue of Apple?\",\n",
        "          \"role\": \"user\",\n",
        "      }]\n",
        "  )\n",
        "print(query.message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
