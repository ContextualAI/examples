{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iPJnDH0HhXB"
      },
      "source": [
        "<img src=\"https://imagedelivery.net/Dr98IMl5gQ9tPkFM5JRcng/3e5f6fbd-9bc6-4aa1-368e-e8bb1d6ca100/Ultra\" alt=\"Image description\" width=\"160\" />\n",
        "\n",
        "<br/>\n",
        "\n",
        "# Specialization Deep Dive\n",
        "\n",
        "This notebook provides a deep dive on specializing or improving your Contextual AI agents. It focuses on showing you the specific settings, but to dive deeper into the usefulness of the settings, please consult full documentation available at [docs.contextual.ai](https://docs.contextual.ai/)\n",
        "\n",
        "This notebook covers the following steps:\n",
        "- Queries / Retrievals\n",
        "- Evaluation Job\n",
        "- Modifying System Prompt\n",
        "- Retrieval Settings\n",
        "- Generation Settings\n",
        "- Tuning the Agent\n",
        "\n",
        "The notebook requires you to first build an agent going through the [getting started](https://github.com/ContextualAI/examples/tree/main/01-getting-started) or the [hands on lab](https://github.com/ContextualAI/examples/tree/main/02-hands-on-lab).\n",
        "\n",
        "To run this notebook interactively, you can open it in Google Colab.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/improve_model_performance/improvement_overview.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "4wJG66VTIQvO",
        "outputId": "711cfb05-dbef-4475-f6f3-bea3272ad464"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set up your API key here - I keep it my environment variables\n",
        "API_KEY = os.environ[\"CONTEXTUAL_API_KEY\"]\n",
        "#API_KEY = 'key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "cim9P99PQP1w",
        "outputId": "d87bf3f4-b624-4e95-fb74-dc71a1ef5b9d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Dict\n",
        "from IPython.display import display, JSON\n",
        "import pandas as pd\n",
        "from contextual import ContextualAI\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "client = ContextualAI(api_key = API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84FKJtqnQt3x"
      },
      "source": [
        "Load up the files you will need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUk8u_lSOArL",
        "outputId": "a79c80a1-5772-4078-e32b-b58119fd32dd"
      },
      "outputs": [],
      "source": [
        "def fetch_file(filepath):\n",
        "    if not os.path.exists(os.path.dirname(filepath)):  # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)  # Create if not exists\n",
        "\n",
        "    print(f\"Fetching {filepath}\")\n",
        "    response = requests.get(f\"https://raw.githubusercontent.com/ContextualAI/examples/main/01-getting-started/{filepath}\")\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "fetch_file('data/eval_short.csv')\n",
        "fetch_file('data/fin_train.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFq4oMe9gMuz"
      },
      "source": [
        "## 1: Queries and Retrievals\n",
        "\n",
        "A first step to understanding our agent is passing it queries. Contextual AI will return a response.\n",
        "\n",
        "Besides the model response, it's also possible to retrieve the full text of all the attributions/citations. You can also retrieve images of the bounding boxes for attributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFhZ-h3vSi2l"
      },
      "source": [
        "Let's start with the prebuilt agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WvlwoXzSQP1w"
      },
      "outputs": [],
      "source": [
        "agent_id = 'add_your_agent_id_here'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVa5n-_9Sop-"
      },
      "source": [
        "The query here also includes the optional parameter for including the retrieval contexts. Normally, you would set this to false for faster retrieval. However, here I wanted to show you the full text of information available to developers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Bvc2MkpDQP1x"
      },
      "outputs": [],
      "source": [
        "query_result = client.agents.query.create(\n",
        "    agent_id=agent_id,\n",
        "    messages=[{\n",
        "        # Input your question here\n",
        "        \"content\": \"Tell about bonds\",\n",
        "        \"role\": \"user\"\n",
        "    }],\n",
        "    include_retrieval_content_text = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNOmbhFiaIAX"
      },
      "source": [
        "Here I show the first document retrieved that was relevant to the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR-rCnf5QP1x"
      },
      "outputs": [],
      "source": [
        "context_text = query_result.retrieval_contents[0].content_text\n",
        "print(context_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VTZGxEMQP1x"
      },
      "source": [
        "## 2: Running Evaluation Jobs\n",
        "\n",
        "Contextual AI offer two different endpoints for running evaluation jobs.\n",
        "  - Evaluation assessing ground truth\n",
        "  - Natural language unit tests (LMUnit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Pd4p7STZSx"
      },
      "source": [
        "### 2.1 Eval Endpoints\n",
        "\n",
        "The eval endpoints allow you to evaluate your Agent using a set of prompts (questions) and reference (gold) answers. We support two metrics: equivalence and groundedness.\n",
        "\n",
        "- Equivalance evaluates if the Agent response is equivalent to the ground truth (model-driven binary classification).  \n",
        "- Groundedness decomposes the Agent response into claims and then evaluates if the claims are grounded by the retrieved documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1A25lECQP1y"
      },
      "source": [
        "An evaluation datasets should consist of prompts (questions) and reference (gold) answers.\n",
        "Lets walk through scoring an evaluation dataset. This dataset comes from the [getting started](https://github.com/ContextualAI/examples/tree/main/01-getting-started) and [hands on lab](https://github.com/ContextualAI/examples/tree/main/02-hands-on-lab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeue_glVQP1y"
      },
      "outputs": [],
      "source": [
        "eval = pd.read_csv('data/eval_short.csv')\n",
        "eval.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xzYph7PQP1y"
      },
      "source": [
        "Start an evaluation job that measures equivalence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YlLkLBJEQP1y"
      },
      "outputs": [],
      "source": [
        "with open('data/eval_short.csv', 'rb') as f:\n",
        "    eval_result = client.agents.evaluate.create(\n",
        "        agent_id=agent_id,\n",
        "        metrics=[\"equivalence\", \"groundedness\"],\n",
        "        evalset_file=f\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNuMZBZMQP1y"
      },
      "source": [
        "Evaluation jobs can take time, especially longer ones. Here is how you can check on their status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPGVo6DyQP1y"
      },
      "outputs": [],
      "source": [
        "eval_result.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb-H2cDcQP1y"
      },
      "outputs": [],
      "source": [
        "eval_status = client.agents.evaluate.jobs.metadata(agent_id=agent_id, job_id=eval_result.id)\n",
        "\n",
        "progress = tqdm(total=eval_status.job_metadata.num_predictions)\n",
        "progress.update(eval_status.job_metadata.num_processed_predictions)\n",
        "progress.set_description(\"Evaluation Progress\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZnYjw8mQP1y"
      },
      "source": [
        "View our evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n-7k-gDQP1y"
      },
      "outputs": [],
      "source": [
        "eval_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAySoPqlUi8t"
      },
      "source": [
        "Let's make it easy to show the results in a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9Z4XEUAcQP1z"
      },
      "outputs": [],
      "source": [
        "def process_binary_evaluation(binary_response):\n",
        "    \"\"\"\n",
        "    Process BinaryAPIResponse into a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        binary_response: BinaryAPIResponse from evaluate.retrieve\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed evaluation data\n",
        "    \"\"\"\n",
        "    # Read the binary content\n",
        "    content = binary_response.read()\n",
        "\n",
        "    # Now decode the content\n",
        "    lines = content.decode('utf-8').strip().split('\\n')\n",
        "\n",
        "    # Parse each line and flatten the results\n",
        "    data = []\n",
        "    for line in lines:\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "\n",
        "            # Parse the results string if it exists\n",
        "            if 'results' in entry:\n",
        "                results = ast.literal_eval(entry['results'])\n",
        "                del entry['results']\n",
        "                if isinstance(results, dict):\n",
        "                    for key, value in results.items():\n",
        "                        if isinstance(value, dict):\n",
        "                            for subkey, subvalue in value.items():\n",
        "                                entry[f'{key}_{subkey}'] = subvalue\n",
        "                        else:\n",
        "                            entry[key] = value\n",
        "\n",
        "            data.append(entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing line: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9DWDlANQP1z"
      },
      "source": [
        "Let's show this in pandas and make it friendlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agsodkxYQP1z"
      },
      "outputs": [],
      "source": [
        "eval_results = client.agents.datasets.evaluate.retrieve(dataset_name=eval_status.dataset_name, agent_id = agent_id)\n",
        "df = process_binary_evaluation(eval_results)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2WzW8-KQP1z"
      },
      "source": [
        "Here I am going to save the results of the evaluation to a csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3YcVNjPQP1z"
      },
      "outputs": [],
      "source": [
        "df.to_csv('eval_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59LCCn_WQP1z"
      },
      "source": [
        "### 2.2 LMUnit\n",
        "\n",
        "The `lmunit` endpoint supports natural language unit tests. To learn more, check out the [blog post](https://contextual.ai/blog/lmunit/) or check out a [notebook using LMUnit](https://github.com/ContextualAI/examples/tree/main/03-lmunit).\n",
        "Here is a simple example of natural langauge unit test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnC-RTJUVMRA"
      },
      "outputs": [],
      "source": [
        "response = client.lmunit.create(\n",
        "                    query=\"What material is used in N95 masks?\",\n",
        "                    response=\"N95 masks are made primarily of polypropylene. This synthetic material is created through a melt-blowing process that creates multiple layers of microfibers. The material was chosen because it can be electrostatically charged to attract particles. Particles are the constituents of the universe\",\n",
        "                    unit_test=\"Does the response avoid unnecessary information?\"\n",
        "                )\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWWt707mQP1z"
      },
      "source": [
        "## 3: Modifying the System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY5i_pFDVZIj"
      },
      "source": [
        "After initial testing, you may want to revise the system prompt. Here I have an updated prompt with additional information in the critical guidelines section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xi_cliANQP1z"
      },
      "outputs": [],
      "source": [
        "system_prompt2 = '''\n",
        "You are an AI assistant specialized in financial analysis and reporting. Your responses should be precise, accurate, and sourced exclusively from official financial documentation provided to you. Please follow these guidelines:\n",
        "\n",
        "Data Analysis & Response Quality:\n",
        "* Only use information explicitly stated in provided documentation (e.g., earnings releases, financial statements, investor presentations)\n",
        "* Present comparative analyses using structured formats with tables and bullet points where appropriate\n",
        "* Include specific period-over-period comparisons (quarter-over-quarter, year-over-year) when relevant\n",
        "* Maintain consistency in numerical presentations (e.g., consistent units, decimal places)\n",
        "* Flag any one-time items or special charges that impact comparability\n",
        "\n",
        "\n",
        "For any analysis, provide comprehensive insights using all relevant available information while maintaining strict adherence to these guidelines and focusing on delivering clear, actionable information.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARpHSA3dViKG"
      },
      "source": [
        "Let's now update the agent and verify the changes by checking the agent metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASRr5EPsQP1z"
      },
      "outputs": [],
      "source": [
        "client.agents.update(agent_id=agent_id, system_prompt=system_prompt2)\n",
        "\n",
        "agent_config = client.agents.metadata(agent_id=agent_id)\n",
        "print (agent_config.system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yai7YC3KVmDD"
      },
      "source": [
        "Modifying the system prompt is useful when trying to improve the response generation. For example, by making it more concise, more professional, or including specific terms that should be part of the response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uLIXp51QP1z"
      },
      "source": [
        "## 4: Retrieval Settings\n",
        "\n",
        "There are a number of settings available for modifying retrieval settings. These are advanced settings, please refer to the documentation for more details.\n",
        "\n",
        "At the global level:\n",
        "- enable_rerank\n",
        "- enable_filter: The filter is a capability in the platform to remove irrelevant or low-quality information before it's used to generate responses.\n",
        "- enable_multi_turn: This feature is experimental and will be improved.\n",
        "\n",
        "Retriever settings.\n",
        "- top_k_retrieved_chunks\n",
        "- lexical_alpha: This parameter controls how much weight is given to exact keyword matches when searching through documents.\n",
        "- semantic_alpha: This parameter controls the weight given to semantic search. The total of lexical and semantic should sum to 1.\n",
        "\n",
        "Reranker settings\n",
        "- top_k_reranked_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rUpThueoQP1z"
      },
      "outputs": [],
      "source": [
        "# Simple update focusing on retrieval parameters\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"retrieval_config\": {\n",
        "                \"top_k_retrieved_chunks\": 10,\n",
        "                \"lexical_alpha\": 0.5,\n",
        "                \"semantic_alpha\": 0.5\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1ghsGvaxQP1z"
      },
      "outputs": [],
      "source": [
        "# Update focusing on filtering and reranking\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"filter_and_rerank_config\": {\n",
        "                \"top_k_reranked_chunks\": 5\n",
        "            },\n",
        "            \"global_config\": {\n",
        "                \"enable_rerank\": True,\n",
        "                \"enable_filter\": True\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz6AH2j5X8vY"
      },
      "source": [
        "Get the agent metadata that will show the retrieval changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_info = client.agents.metadata(agent_id=agent_id)\n",
        "print(agent_info.agent_configs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Zuf8DGSVYCM9"
      },
      "outputs": [],
      "source": [
        "# Complete configuration update using all available parameters -- you shouldn't need to change all of these\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"retrieval_config\": {\n",
        "                \"top_k_retrieved_chunks\": 10,\n",
        "                \"lexical_alpha\": 0.5,\n",
        "                \"semantic_alpha\": 0.5\n",
        "            },\n",
        "            \"filter_and_rerank_config\": {\n",
        "                \"top_k_reranked_chunks\": 5\n",
        "            },\n",
        "            \"global_config\": {\n",
        "                \"enable_rerank\": True,\n",
        "                \"enable_filter\": True,\n",
        "                \"enable_multi_turn\": False\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXfZf3SnQP10"
      },
      "source": [
        "## 5: Generation Settings\n",
        "\n",
        "There are a number of settings available for modifying the generation of responses. These are advanced settings, please refer to the documentation for more details.\n",
        "\n",
        "- max_new_tokens\n",
        "- temperature\n",
        "- top_p\n",
        "- frequency_penalty\n",
        "- seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9KaBgYPaQP10"
      },
      "outputs": [],
      "source": [
        "# Update focusing on generation parameters\n",
        "response = client.agents.update(\n",
        "    agent_id=agent_id,\n",
        "    extra_body={\n",
        "        \"agent_configs\": {\n",
        "            \"generate_response_config\": {\n",
        "                \"max_new_tokens\": 500,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.95,\n",
        "                \"frequency_penalty\": 0.5,\n",
        "                \"seed\": 42\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRLX7QlAQP10"
      },
      "outputs": [],
      "source": [
        "agent_info = client.agents.metadata(agent_id=agent_id)\n",
        "print(agent_info.agent_configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692gSc7ZNyve"
      },
      "source": [
        "## 6: Tune your Agent\n",
        "\n",
        "Contextual AI allows you to tune your entire agent end to end for improved performance. To run a tune job, you need to specify a training file and an optional test file. (If no test file is provided, the tuning job will hold out a portion of the training file as the test set.)\n",
        "\n",
        "A tuning job requires fine tuning models and the expectation should be it will take a couple of hours to run.\n",
        "\n",
        "After the tune job completes, the metadata associated with the tune job will include evaluation results and a model ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmRrBTy0OWzc"
      },
      "source": [
        "### 6.1 Format for the Training File\n",
        "\n",
        "The file should be in JSON array format, where each element of the array is a JSON object represents a single training example. The four required fields are guideline, prompt, reference, and knowledge.\n",
        "\n",
        "- guideline field should be guidelines for the expected response.\n",
        "\n",
        "- prompt field should be a question or statement that the model should respond to.\n",
        "\n",
        "- reference: The gold-standard answer to the prompt.\n",
        "\n",
        "- knowledge field should be an array of strings, each string representing a piece of knowledge that the model should use to generate the response.\n",
        "\n",
        "There is a minimum size of 35 rows for tuning datasets. The `fin_train.jsonl` is a toy sample to illustrate how tuning operates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf1DeSP-VXR4"
      },
      "outputs": [],
      "source": [
        "!head data/fin_train.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1tcNqTv0gaQ"
      },
      "source": [
        "### 6.2 Starting a Tuning Model Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVxQ9i00M3cx"
      },
      "outputs": [],
      "source": [
        "# create a dataset file\n",
        "with open(\"data/fin_train.jsonl\", 'rb') as training_file:\n",
        "    response = client.agents.tune.create(\n",
        "        agent_id=agent_id,\n",
        "        training_file=training_file,\n",
        "    )\n",
        "    job_id=response.id\n",
        "    print(response.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGao-yw7yLVY"
      },
      "source": [
        " ### 6.3 Checking the Status.\n",
        "\n",
        " You can check the status of the job. For detailed information, refer to the API documentation. When the tuning job is complete, the status will turn to completed. The response payload will also contain evaluation_results ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBy8pONdyOnu"
      },
      "outputs": [],
      "source": [
        "response = client.agents.tune.jobs.metadata(\n",
        "    agent_id=agent_id,\n",
        "    job_id=job_id,\n",
        ")\n",
        "response.job_status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg4YXZdYyR9X"
      },
      "source": [
        "When the tuning job is complete, the metadata would look like the following:\n",
        "```\n",
        "{'job_status': 'completed',\n",
        " 'evaluation_results': {'grounded_generation_train_test.json_equivalence': 1.0,\n",
        "  'grounded_generation_train_test.json_helpfulness': 0.814156498263641,\n",
        "  'grounded_generation_train_test.json_groundedness': 0.7781168677598632},\n",
        " 'model_id': 'registry/model-ada3c484-3ce0f31f-llm-fd6c2'}\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_77uMH5ybBh"
      },
      "source": [
        "### 6.4 Updating the agent\n",
        "Once the tuned job is complete, you can deploy the tuned model via editing the agent through API. Note that currently a single fine-tuned model deployment is allowed per tenant. Please see the API doc for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiuYZrBsybjo"
      },
      "outputs": [],
      "source": [
        "response = client.agents.tune.jobs.metadata(\n",
        "    agent_id=agent_id,\n",
        "    job_id=job_id,\n",
        ")\n",
        "model_id = response.model_id\n",
        "print(f\"model_id: {model_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKPyLDbANMJt"
      },
      "outputs": [],
      "source": [
        "response = client.agents.update(\n",
        "    llm_model_id=model_id,\n",
        ")\n",
        "print(response.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akE9XOq9NTEc"
      },
      "source": [
        "### 6.5 Query your tuned model\n",
        "After you have deployed the tuned model, you can now query it with the usual command. Make sure you pass your new tuned model_id in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT_QBn1pNSOL"
      },
      "outputs": [],
      "source": [
        "query = client.agents.query.create(\n",
        "      agent_id=agent_id,\n",
        "      llm_model_id=model_id,\n",
        "      messages=[{\n",
        "          # Input your question here\n",
        "          \"content\": \"What is the revenue of Apple?\",\n",
        "          \"role\": \"user\",\n",
        "      }]\n",
        "  )\n",
        "print(query.message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
