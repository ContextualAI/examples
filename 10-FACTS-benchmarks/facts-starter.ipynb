{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACTS Benchmark: Factual Accuracy and Grounding Evaluation\n",
    "\n",
    "This notebook demonstrates how to generate and evaluate responses for the FACTS (Factual Accuracy and Grounding Benchmark) dataset with Contextual AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FACTS (Factual Accuracy and Grounding Benchmark) [dataset](https://kaggle.com/facts-leaderboard) is a novel benchmark from Google DeepMind and Google Research designed to evaluate the factual accuracy and grounding of AI models.\n",
    "\n",
    "1. **Evaluate Factual Accuracy**: Test how well AI models can provide accurate information based on given context\n",
    "2. **Assess Grounding**: Measure models' ability to stick to provided reference material without introducing external knowledge\n",
    "3. **Cross-Domain Testing**: Evaluate performance across different fields like medical, legal, and financial domains\n",
    "4. **Task Versatility**: Test various types of tasks from simple fact-finding to complex analysis\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "1. [Setup and Imports](#setup-and-imports)\n",
    "2. [Load and Explore the FACTS Dataset](#load-and-explore-the-facts-dataset)\n",
    "3. [Model Setup and Response Generation](#model-setup-and-response-generation)\n",
    "4. [Generate Responses](#generate-responses)\n",
    "5. [Save Responses](#save-responses)\n",
    "6. [Response Evaluation](#response-evaluation)\n",
    "7. [Results and Analysis](#results-and-analysis)\n",
    "\n",
    "\n",
    "Thhe generated responses can then be evaluated for grounding.\n",
    "\n",
    "This notebook is largely derived from the [Starter FACTS notebook](https://www.kaggle.com/code/andrewmingwang/facts-grounding-benchmark-starter-code) at Kaggle. \n",
    "\n",
    "It is assumed you have downloaded the examples.csv and evaluation_prompts.csv from the Kaggle site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "To run across all the providers in this notebook, you will need your own set of API keys from [OpenAI](https://platform.openai.com/api-keys), [Anthropic](https://docs.anthropic.com/en/api/getting-started), [Contextual AI](https://app.contextual.ai) to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai anthropic contextual-client google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from contextual import ContextualAI\n",
    "from anthropic import Anthropic\n",
    "from google import genai\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save your keys in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_api_key = os.getenv(\"CONTEXTUAL_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anthropic_api_key = \"sk-ant-\"\n",
    "#openai_api_key = \"sk-z\n",
    "#google_api_key = \"AI\"\n",
    "#contextual_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "anthropic_client = Anthropic(api_key=anthropic_api_key)\n",
    "google_client = genai.Client(api_key=google_api_key)\n",
    "contextual_client = ContextualAI(api_key=contextual_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the FACTS Dataset\n",
    "\n",
    "We'll be using a subset of the examples from the [FACTS Grounding 1.0 Public Examples](https://kaggle.com/datasets/deepmind/FACTS-grounding-examples/data) dataset. This dataset contains 860 public examples (out of a total 1,719 examples) for the general public to access.\n",
    "\n",
    "I have downloaded the data locally for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure\n",
    "\n",
    "The dataset contains 860 public examples (out of 1,719 total) with the following columns:\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **system_instruction**\n",
    "   - Contains model guidelines and constraints\n",
    "   - Specifies how to approach the response\n",
    "   - Often emphasizes using only provided context\n",
    "\n",
    "2. **user_request**\n",
    "   - The actual question or task to be answered\n",
    "   - Ranges from simple queries to complex analysis requests\n",
    "\n",
    "3. **context_document**\n",
    "   - Reference material for grounding responses\n",
    "   - Contains the factual information needed to answer the question\n",
    "   - Serves as the knowledge base for the response\n",
    "\n",
    "4. **full_prompt**\n",
    "   - Complete formatted prompt combining system instruction, context, and user request\n",
    "   - Ready for model input\n",
    "\n",
    "### Categorization\n",
    "\n",
    "5. **domain**\n",
    "   - Categorizes questions by field:\n",
    "     - Medical\n",
    "     - Legal\n",
    "     - Financial\n",
    "     - Retail/Product\n",
    "     - Internet/Technology\n",
    "\n",
    "6. **type**\n",
    "   - Specific task categories:\n",
    "     - Effect Analysis\n",
    "     - Fact Finding\n",
    "     - Find & Summarize\n",
    "     - Summarize\n",
    "     - Explanation/Definition\n",
    "\n",
    "7. **high_level_type**\n",
    "   - Broad task classification:\n",
    "     - Q&A (Question and Answer tasks)\n",
    "     - Text Transformation (Summarization and text processing)\n",
    "\n",
    "This structured approach allows for comprehensive evaluation of model performance across different dimensions of factual grounding and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = pd.read_csv(\"examples.csv\")\n",
    "examples.head()\n",
    "\n",
    "# Limit to the first 20 examples for testing\n",
    "#examples = examples.head(10)\n",
    "responses = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup and Response Generation\n",
    "\n",
    "Define helper functions for generating responses from each LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemini-1.5-pro\", \"gpt-4o\", \"claude-3-5-sonnet\"]\n",
    "def generate_gemini(prompt, sys):\n",
    "    response = google_client.models.generate_content(model='gemini-1.5-pro-002', contents=f'{sys} {prompt}')\n",
    "    return response.text\n",
    "\n",
    "def generate_gpt(prompt, sys):\n",
    "    if len(sys) > 0:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\",\"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[{\"role\": \"user\",\"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0].message.content     \n",
    "\n",
    "def generate_claude(prompt, sys):\n",
    "    if len(sys) > 0:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=8192,\n",
    "            system=[{\"type\": \"text\", \"text\": sys}],\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    else:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=8192,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return message.content[0].text\n",
    "\n",
    "def generate(model, prompt, sys):\n",
    "    if model == \"gemini-1.5-pro\":\n",
    "        return generate_gemini(prompt, sys)\n",
    "    elif model == \"gpt-4o\":\n",
    "        return generate_gpt(prompt, sys)\n",
    "    elif model == \"claude-3-5-sonnet\":\n",
    "        return generate_claude(prompt, sys)\n",
    "    else:\n",
    "        raise Exception(\"Invalid model selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    print(generate(model, \"count to 3\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Responses\n",
    "\n",
    "Generate responses for each example in the dataset using the selected models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate responses for Contextual to match the Kaggle benchmarking results. Doing this takes around 90 minutes (you can do this faster by using an async script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"contextual\"\n",
    "for ix, row in tqdm(examples.iterrows(), total=len(examples)):\n",
    "    response = contextual_client.generate.create(\n",
    "        model=\"v2\",\n",
    "        messages=[{\"role\": \"user\", \"content\": row[\"user_request\"]}],\n",
    "        system_prompt=row[\"full_prompt\"],\n",
    "        knowledge=[],\n",
    "        top_p=0.9,\n",
    "        avoid_commentary=True,\n",
    "        temperature=0.0,\n",
    "        max_new_tokens=2048\n",
    "    )\n",
    "    responses.loc[ix, [\"system_instruction\", \"user_request\", \"context_document\", \"full_prompt\", f'{model}-response']] = [\n",
    "        row[\"system_instruction\"], row[\"user_request\"], row[\"context_document\"], row[\"full_prompt\"], response.response\n",
    "    ]\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate responses for the other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional, this is how you can get a bit more performance out of the GLM by taking advantage of the knowledge field.\n",
    "\n",
    "```\n",
    "response = contextual_client.generate.create(\n",
    "    model=\"v2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": row[\"user_request\"]}],\n",
    "    system_prompt=row[\"system_instruction\"],\n",
    "    knowledge=knowledge,\n",
    "    top_p=1,\n",
    "    avoid_commentary=True,\n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple script for getting responses from the other models. Ideally, this would work, but I kept running into timeouts and other issues, so the script kept failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in tqdm(examples.iterrows(), total=len(examples)):\n",
    "    full_prompt = row['full_prompt']\n",
    "    for model in models:\n",
    "        response = generate(model=model, prompt=full_prompt, sys=\"\")\n",
    "        responses.loc[ix, [\"system_instruction\", \"user_request\", \"context_document\", f'{model}-response']] = [row[\"system_instruction\"], row[\"user_request\"], row[\"context_document\"], response]\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer script to deal with errors from API providers. It's more robust with checkpointing and catching errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 3\n",
    "wait_seconds = 30\n",
    "checkpoint_path = \"responses_multi_checkpoint.csv\"\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    responses = pd.read_csv(checkpoint_path)\n",
    "    print(f\"Loaded checkpoint with {len(responses)} rows.\")\n",
    "else:\n",
    "    responses = pd.DataFrame(columns=[\"system_instruction\", \"user_request\", \"context_document\", \"full_prompt\"] + [f\"{model}-response\" for model in models])\n",
    "\n",
    "for ix, row in tqdm(examples.iterrows(), total=len(examples)):\n",
    "    for model in models:\n",
    "        col = f\"{model}-response\"\n",
    "        if col in responses.columns and ix in responses.index and pd.notna(responses.loc[ix, col]):\n",
    "            continue\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = generate(model=model, prompt=row[\"full_prompt\"], sys=\"\")\n",
    "                if ix not in responses.index:\n",
    "                    responses.loc[ix, [\"system_instruction\", \"user_request\", \"context_document\", \"full_prompt\"]] = [\n",
    "                        row[\"system_instruction\"], row[\"user_request\"], row[\"context_document\"], row[\"full_prompt\"]\n",
    "                    ]\n",
    "                responses.loc[ix, col] = response\n",
    "                break  # Success\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row {ix}, model {model}, attempt {attempt+1}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(wait_seconds)\n",
    "                else:\n",
    "                    print(f\"Failed after {max_retries} attempts, skipping.\")\n",
    "        # Checkpoint every 10 rows\n",
    "        if ix % 10 == 0:\n",
    "            responses.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "# Final save\n",
    "responses.to_csv(checkpoint_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Responses\n",
    "\n",
    "Save the generated responses to disk for reproducibility. You show now have columns:\n",
    "- 'contextual-response'\n",
    "- 'gemini-1.5-pro-response'\n",
    "- 'gpt-4o-response'\n",
    "- 'claude-3-5-sonnet-response'\n",
    " \n",
    " You will need all of these for the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.to_csv(\"responses_contextual.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Evaluation\n",
    "\n",
    "Evaluate the factual accuracy and grounding of the generated responses using multiple LLM-based judges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Evaluation Prompts\n",
    "\n",
    "##### Evaluation Prompt Selection\n",
    "\n",
    "We chose grounding and evaluation prompts based on evaluation against an internal set of human rated responses. See below for which evaluation prompt were selected and the [Technical Report](https://arxiv.org/abs/2501.03200) for more details.\n",
    "\n",
    "#### Grounding\n",
    "\n",
    "1) `Gemini-1.5-pro    : json`\n",
    "\n",
    "2) `GPT-4o            : json`\n",
    "\n",
    "3) `Claude-3-5-sonnet : implicit_span_level`\n",
    "\n",
    "#### Quality\n",
    "\n",
    "1) `Gemini-1.5-pro    : ineligible_responses_filter_no_context`\n",
    "\n",
    "2) `GPT-4o            : ineligible_responses_filter_no_context`\n",
    "\n",
    "3) `Claude-3-5-sonnet : ineligible_responses_filter_no_context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompts = pd.read_csv(\"evaluation_prompts.csv\")\n",
    "evaluation_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt = evaluation_prompts.loc[evaluation_prompts[\"evaluation_method\"] == 'json', \"evaluation_prompt\"].values[0]\n",
    "implicit_span_prompt = evaluation_prompts.loc[evaluation_prompts[\"evaluation_method\"] == 'implicit_span_level', \"evaluation_prompt\"].values[0]\n",
    "ineligible_responses_filter_no_context_prompt = evaluation_prompts.loc[evaluation_prompts[\"evaluation_method\"] == 'ineligible_responses_filter_with_context', \"evaluation_prompt\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluation Helpers\n",
    "\n",
    "Here we'll create a helper method for each evaluation type. Note that, for grounding, the Gemini-1.5-pro and GPT-4o judges use the `json` evaluation prompt and that the Claude-3-5-sonnet judge uses the `implicit_span_level` evaluation prompt as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grounding json evaluation\n",
    "\n",
    "import json\n",
    "\n",
    "def parse_structured_json(ans):\n",
    "  if '```json' in ans:\n",
    "      ans = ans.split('```json')[1].split('```')[0]\n",
    "  ans = ans.strip()\n",
    "  ans = ans.replace('}\\n', '}\\n@\\n@\\n')\n",
    "  parsed_answers = []\n",
    "  for line in ans.split('\\n@\\n@\\n'):\n",
    "    try:\n",
    "      line = line.replace('\\n', ' ')\n",
    "      line = line.replace(\"\\\\'\", \"'\")\n",
    "      parsed = json.loads(line)\n",
    "      parsed_answers.append(parsed)\n",
    "    except:\n",
    "      pass\n",
    "  if len(parsed_answers) > 0:\n",
    "    bool_ans = all(d['label'] == 'supported' or d['label'] == 'no_rad' for d in parsed_answers)\n",
    "  else:\n",
    "    bool_ans = False\n",
    "  return bool_ans, parsed_answers\n",
    "\n",
    "def evaluate_grounding_json(user_request, context_document, response, model):\n",
    "    prompt = json_prompt.replace('{{user_request}}', user_request).replace('{{context_document}}', context_document).replace('{{response}}', response)\n",
    "\n",
    "    evaluation_text = generate(model=model, prompt=prompt, sys=\"\")\n",
    "    evaluation, parsed = parse_structured_json(evaluation_text)\n",
    "    return evaluation\n",
    "\n",
    "def evaluate_grounding_gemini(user_request, context_document, response):\n",
    "    return evaluate_grounding_json(user_request, context_document, response, model=\"gemini-1.5-pro\")\n",
    "\n",
    "def evaluate_grounding_gpt(user_request, context_document, response):\n",
    "    return evaluate_grounding_json(user_request, context_document, response, model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Grounding Judges\n",
    "We'll test grounding with a situation where the response is correct but not grounded. We expect to see a `False` from all judges!\n",
    "\n",
    "```\n",
    "user_request:      \"what is 2 + 2?\"\n",
    "context_document:  \"2 + 2 is 3\"\n",
    "response:          \"2 + 2 is 4\" (this is correct but not grounded in the context_document)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the evaluator\n",
    "print(evaluate_grounding_gemini(user_request=\"what is 2 + 2?\", context_document=\"2 + 2 is 3\", response=\"2 + 2 is 4\"))\n",
    "print(evaluate_grounding_gpt(user_request=\"what is 2 + 2?\", context_document=\"2 + 2 is 3\", response=\"2 + 2 is 4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grounding implicit_span_level evaluation\n",
    "\n",
    "def answer_normalization(answer):\n",
    "  answer = answer.strip().lower()\n",
    "  if 'inaccurate' in answer or 'false' in answer:\n",
    "    return False\n",
    "  elif 'accurate' in answer or 'true' in answer:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "    \n",
    "def evaluate_grounding_implicit_span(user_request, context_document, response, model):\n",
    "    prompt = implicit_span_prompt.replace('{{user_request}}', user_request).replace('{{context_document}}', context_document).replace('{{response}}', response)\n",
    "    \n",
    "    evaluation_text = generate(model=model, prompt=prompt, sys=\"\")\n",
    "    splits = evaluation_text.split('Final Answer:')\n",
    "    if (len(splits) <= 1):\n",
    "        return False\n",
    "    final_ans = splits[1]\n",
    "    return answer_normalization(final_ans)\n",
    "\n",
    "def evaluate_grounding_claude(user_request, context_document, response):\n",
    "    return evaluate_grounding_implicit_span(user_request, context_document, response, model=\"claude-3-5-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test call should now work without a TypeError\n",
    "evaluate_grounding_claude(user_request=\"what is 2 + 2?\", context_document=\"2 + 2 is 3\", response=\"2 + 2 is 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Quality Judges\n",
    "\n",
    "We'll test quality with a situation where the response does not answer the posed request. We expect to see a `False` from all judges!\n",
    "\n",
    "```\n",
    "user_request:      \"what is 2 + 2?\"\n",
    "response_a:        \"3 + 3 is 6\" (this is the response we evaluate)\n",
    "response_b:        \"2 + 2 is 4\" (this is the reference response)\n",
    "```\n",
    "\n",
    "We compare our response with a reference because it improves the accuracy of the judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality no_context evaluation\n",
    "\n",
    "QUESTIONS_TO_LABELS = {\n",
    "    'Instruction Following': ['No Issues', 'Minor Issue(s)', 'Major Issue(s)', 'Invalid'],\n",
    "}\n",
    "\n",
    "def parse_json(ans):\n",
    "    parsed = {}\n",
    "    if '```json' in ans:\n",
    "        ans = ans.split('```json')[1]\n",
    "        ans = ans.split('```')[0]\n",
    "    ans = ans.replace('\\n', ' ')\n",
    "    try:\n",
    "        parsed = json.loads(ans)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    if 'Instruction Following' not in parsed:\n",
    "        parsed['Instruction Following'] = 'Invalid'\n",
    "    elif parsed['Instruction Following'] not in ['No Issues', 'Minor Issue(s)', 'Major Issue(s)', 'Invalid']:\n",
    "        parsed['Instruction Following'] = 'Invalid'\n",
    "    return parsed\n",
    "\n",
    "# Evaluates response_a for quality using response_b as a reference.\n",
    "def evaluate_quality_no_context(user_request, response_a, response_b, model):\n",
    "    prompt = ineligible_responses_filter_no_context_prompt.replace('{{user_request}}', user_request).replace('{{response_a}}', response_a).replace('{{response_b}}', response_b)\n",
    "    \n",
    "    evaluation_text = generate(prompt=prompt, model=model, sys=\"\")\n",
    "    parsed = parse_json(evaluation_text)\n",
    "\n",
    "    return \"Major Issue(s)\" not in parsed['Instruction Following']\n",
    "\n",
    "# Use the response from the judge model itself as the reference response (response_b).\n",
    "def evaluate_quality_no_context_gemini(user_request, response, references):\n",
    "    return evaluate_quality_no_context(user_request, response, references[\"gemini-1.5-pro-response\"], model=\"gemini-1.5-pro\")\n",
    "\n",
    "def evaluate_quality_no_context_gpt(user_request, response, references):\n",
    "    return evaluate_quality_no_context(user_request, response, references[\"gpt-4o-response\"], model=\"gpt-4o\")\n",
    "\n",
    "def evaluate_quality_no_context_claude(user_request, response, references):\n",
    "    return evaluate_quality_no_context(user_request, response, references[\"claude-3-5-sonnet-response\"], model=\"claude-3-5-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example references dataframe, when we're actually running the evaluation we'll be using our generated responses from before as references.\n",
    "references = pd.DataFrame({\n",
    "    \"gemini-1.5-pro-response\": [\"2 + 2 is 4\"],\n",
    "    \"gpt-4o-response\": [\"2 + 2 is 4\"],\n",
    "    \"claude-3-5-sonnet-response\": [\"2 + 2 is 4\"],\n",
    "})\n",
    "\n",
    "print(evaluate_quality_no_context_gemini(\"What is 2 + 2?\", response=\"3 + 3 is 6\", references=references.loc[0]))\n",
    "print(evaluate_quality_no_context_gpt(\"What is 2 + 2?\", response=\"3 + 3 is 6\", references=references.loc[0]))\n",
    "print(evaluate_quality_no_context_claude(\"What is 2 + 2?\", response=\"3 + 3 is 6\", references=references.loc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Responses\n",
    "\n",
    "With our evaluation helper methods, we can now evaluate the responses we generated before.\n",
    "- We'll invalidate responses only if it fails the bar for all three quality judges.\n",
    "- Then we'll sum all the responses that were considered grounded (and that passed the quality check) across each of the three grounding judges.\n",
    "- Finally, we can average across the three LLM judges to arrive at final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"contextual\", \"gemini-1.5-pro\", \"gpt-4o\", \"claude-3-5-sonnet\"]\n",
    "models = [\"contextual\"] ##For this notebook, we'll only evaluate the contextual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added some checkpointing, so if you hit an error, you should be able to restart the below cell. This cell is doing thousands of API calls and it's not unexpected to hit some errors. This usually take several hours at least to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounding_evaluation_methods = [\n",
    "    evaluate_grounding_gemini,\n",
    "    evaluate_grounding_gpt,\n",
    "    evaluate_grounding_claude,\n",
    "]\n",
    "\n",
    "quality_evaluation_methods = [\n",
    "    evaluate_quality_no_context_gemini,\n",
    "    evaluate_quality_no_context_gpt,\n",
    "    evaluate_quality_no_context_claude,\n",
    "]\n",
    "\n",
    "def nameof(f):\n",
    "    return f.__name__.replace(\"_\", \"-\")\n",
    "\n",
    "for ix, row in tqdm(responses.iterrows(), total=len(responses)):    \n",
    "    user_request = row[\"user_request\"]\n",
    "    context_document = row[\"context_document\"]\n",
    "    \n",
    "    for evaluated_model in models:\n",
    "        response_column_key = f'{evaluated_model}-response'\n",
    "        response = row[response_column_key]\n",
    "        \n",
    "        print(f'Evaluating response at ix: {ix} for model {evaluated_model} with:')\n",
    "        for eval_method in grounding_evaluation_methods:\n",
    "            print(f'    {eval_method.__name__}')\n",
    "            key = f'{response_column_key}-{nameof(eval_method)}'\n",
    "            \n",
    "            # skip rows if we've evaluated already\n",
    "            if ix in responses.index and key in responses.columns and pd.notna(responses.loc[ix, key]):\n",
    "                continue\n",
    "\n",
    "            evaluation = eval_method(user_request, context_document, response)\n",
    "            responses.loc[ix, [key]] = [evaluation]\n",
    "            \n",
    "        for eval_method in quality_evaluation_methods:\n",
    "            print(f'    {eval_method.__name__}')\n",
    "            key = f'{response_column_key}-{nameof(eval_method)}'\n",
    "\n",
    "            # skip rows if we've evaluated already\n",
    "            if ix in responses.index and key in responses.columns and pd.notna(responses.loc[ix, key]):\n",
    "                continue\n",
    "            \n",
    "            evaluation = eval_method(user_request, response, row)\n",
    "            responses.loc[ix,[key]] = [evaluation]\n",
    "    \n",
    "    # Save every 100 rows\n",
    "    if ix % 100 == 0:\n",
    "        temp_evaluations = responses[[col for col in responses.columns if \"evaluate\" in col]]\n",
    "        temp_evaluations = temp_evaluations.reindex(sorted(temp_evaluations.columns), axis=1)\n",
    "        temp_evaluations.to_csv(f\"evaluations_checkpoint_{ix}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your evaluations:\n",
    "You should have 7 distinct columns for evaluating each model with True/False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = responses[[col for col in responses.columns if \"evaluate\" in col]]\n",
    "evaluations = evaluations.reindex(sorted(evaluations.columns), axis=1)\n",
    "evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add evaluate to responses and we should have a nice wide dataset with all the useful data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this hard work!!! I noticed jsonl is the best format for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#responses.to_csv(\"evaluations_contextual_checkpoint_all.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "responses.to_json(\"responses.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.read_json(\"responses.jsonl\", lines=True)\n",
    "responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Quality Evaluations\n",
    "\n",
    "We'll invalidate responses only if it fails the bar for all three quality judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in evaluations.iterrows():\n",
    "    for model in models:\n",
    "        passes_qc_count = 0\n",
    "        for evaluation_method in quality_evaluation_methods:\n",
    "            key = f'{model}-response-{nameof(evaluation_method)}'\n",
    "            passes_qc_count += 1 if row[key] else 0\n",
    "\n",
    "        passes_qc = passes_qc_count > 0\n",
    "\n",
    "        evaluations.loc[ix, [f'{model}-response-passes-qc']] = [passes_qc]\n",
    "\n",
    "relevant_columns = [col for col in evaluations.columns if (\"evaluate-grounding\" in col or \"passes-qc\" in col)]\n",
    "\n",
    "grounding_evaluations = evaluations[relevant_columns]\n",
    "grounding_evaluations = grounding_evaluations.reindex(sorted(grounding_evaluations.columns), axis=1)\n",
    "grounding_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results and Analysis\n",
    "\n",
    "Summarize and visualize the evaluation results for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['model', 'evaluation_method', 'score'])\n",
    "n = examples.shape[0]\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for evaluation_method in grounding_evaluation_methods:\n",
    "        grounding_key = f'{model}-response-{nameof(evaluation_method)}'\n",
    "        quality_key = f'{model}-response-passes-qc'\n",
    "\n",
    "        passed_qc_responses = grounding_evaluations[grounding_evaluations[quality_key] == True]\n",
    "        n_success = passed_qc_responses[grounding_key].astype(int).sum()\n",
    "\n",
    "        scores.loc[len(scores), ['model', 'evaluation_method', 'score']] = [model, nameof(evaluation_method), n_success / n]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My scores for contextual:\n",
    "\n",
    "\tmodel\tevaluation_method\tscore  \n",
    "0\tcontextual\tevaluate-grounding-gemini\t0.948837  \n",
    "1\tcontextual\tevaluate-grounding-gpt\t0.80814  \n",
    "2\tcontextual\tevaluate-grounding-claude\t0.92907  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.groupby(\"model\")[[\"score\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My scores for contextual:  \n",
    "\n",
    "contextual\t0.895349"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
