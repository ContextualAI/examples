{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2nmrhpCdxmZ"
      },
      "source": [
        "# Evaluating Retrieval Performance with Standard and RAG-Specific Metrics\n",
        "\n",
        "This notebook provides a comprehensive framework for evaluating the performance of a retrieval system using both traditional information retrieval (IR) metrics and modern, RAG-specific metrics. It takes a dataset of queries with ground-truth evidence and systematically assesses the quality of the retrieved results.\n",
        "\n",
        "**Key Evaluation Metrics Covered:**\n",
        "- **Standard IR Metrics:**\n",
        "    - `Recall@k`: What fraction of relevant documents are retrieved in the top-k results?\n",
        "    - `Precision@k`: What fraction of the top-k retrieved documents are relevant?\n",
        "    - `nDCG@k`: A measure of ranking quality that accounts for the position of relevant documents.\n",
        "    - `Mean Reciprocal Rank (MRR)`: How high up in the ranking is the first relevant document?\n",
        "- **RAG-Specific Metrics (using RAGAs):**\n",
        "    - `Context Recall`: Measures the extent to which the ground-truth answer is covered by the retrieved context.\n",
        "    - `Context Precision`: Measures the signal-to-noise ratio in the retrieved context.\n",
        "\n",
        "## ðŸ“Š Workflow\n",
        "\n",
        "1.  **Data Preparation**: Load the matched evidence data from the previous notebook and pivot it into a one-query-per-row format.\n",
        "2.  **Metric Implementation**: Define functions for standard IR metrics.\n",
        "3.  **Retrieval and Evaluation**: For each query, retrieve documents, and calculate all metrics.\n",
        "4.  **Analysis**: Aggregate the results and analyze the mean scores to understand overall system performance.\n",
        "\n",
        "The goal of thie notebook is a starting point for retrieval analysis. Ultimately, you will want to test out retrieval performance with different agent configurations, e.g., with and without the reranker, as part of your evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment and Configuration\n",
        "\n",
        "First, we import all necessary libraries and set up the configuration parameters for the evaluation, such as API keys and Agent IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qzZYbsQjdxmb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import asyncio\n",
        "from ragas.dataset_schema import SingleTurnSample\n",
        "from ragas.metrics import NonLLMContextRecall, NonLLMContextPrecisionWithReference\n",
        "\n",
        "from contextual import ContextualAI\n",
        "client = ContextualAI()\n",
        "#Agent in the notebook is in Custom Tenant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "agent_id= '15543690-68fd-49e7-8fc9-1f53c8e42e33'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Data Preparation\n",
        "\n",
        "The evaluation script expects a pandas DataFrame (`df`) where each row represents a single retrieval test case (query). The DataFrame should be constructed from a CSV or other data source with the following columns:\n",
        "\n",
        "| Column Name                | Description                                                                                 | Example                                                                 |\n",
        "|----------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
        "| `prompt`               | The query string to be sent to the retrieval system.                                       | `\"Which company had higher R&D expenses in FY2024, Tesla or Qualcomm?\"` |\n",
        "| `evidence_1`      | The content of the first evidence strings context (evidence) for this query.                | `\"Tesla had higher expenses\"`                               |\n",
        "| `evidence_2`      | (Optional) The content of the second evidence strings context, if applicable.               | `\"R&D expenses for electric cars in 2024 were mainly  ..\"`                               |\n",
        "| ...                        | (Optional) More `evidence_*` columns for additional evidence strings contexts.        |                                                                         |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppose your DataFrame is called matched and has columns: QA_ID, Question, Answer, Content_Id\n",
        "# Add a group counter for each QA_ID group\n",
        "matched = pd.read_csv(\"matched_retrievals\")# Make a copy to avoid the warning\n",
        "matched['evidence_num'] = matched.groupby('QA_ID').cumcount() + 1\n",
        "\n",
        "# Pivot to wide format\n",
        "wide = matched.pivot_table(\n",
        "    index=['QA_ID', 'Question'],\n",
        "    columns='evidence_num',\n",
        "    values='Content_Id',\n",
        "    aggfunc='first'\n",
        ").reset_index()\n",
        "\n",
        "# Rename columns to evidence_1, evidence_2, ...\n",
        "wide.columns = ['QA_ID', 'prompt'] + [f\"evidence_{i}\" for i in wide.columns[2:]]\n",
        "\n",
        "# Now `wide` is your desired DataFrame\n",
        "wide.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGHqmV7YvF3n"
      },
      "source": [
        "## 3. Defining the Evaluation Metrics\n",
        "\n",
        "Here, we implement the functions for our evaluation metrics. This includes standard IR metrics to assess the accuracy and ranking quality of retrieval, as well as several useful variations like Hit Rate and Reciprocal Rank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recall_at_k(retrieved_ids, ground_truth_ids, k):\n",
        "    retrieved_top_k = set(retrieved_ids[:k])\n",
        "    return len(retrieved_top_k & ground_truth_ids) / len(ground_truth_ids) if ground_truth_ids else 0.0\n",
        "\n",
        "def precision_at_k(retrieved_ids, ground_truth_ids, k):\n",
        "    retrieved_top_k = retrieved_ids[:k]\n",
        "    return sum(1 for cid in retrieved_top_k if cid in ground_truth_ids) / k if k else 0.0\n",
        "\n",
        "def ndcg_at_k(retrieved_chunks, ground_truth_ids, k=10):\n",
        "    # Use the order of retrieved_chunks as provided, do not sort by score\n",
        "    top_k = retrieved_chunks[:k]\n",
        "    relevances = [1 if chunk['content_id'] in ground_truth_ids else 0 for chunk in top_k]\n",
        "    dcg = sum((2**rel - 1) / np.log2(idx + 2) for idx, rel in enumerate(relevances))\n",
        "    ideal_relevances = sorted([1]*min(len(ground_truth_ids), k) + [0]*(k - min(len(ground_truth_ids), k)), reverse=True)\n",
        "    idcg = sum((2**rel - 1) / np.log2(idx + 2) for idx, rel in enumerate(ideal_relevances))\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def idcg(matched_chunk_positions):\n",
        "    total_relevant = len(matched_chunk_positions)\n",
        "    return sum(1 / math.log2(i + 2) for i in range(total_relevant))\n",
        "\n",
        "def precision_at_r(retrieved_ids, ground_truth_ids):\n",
        "    r = len(ground_truth_ids)\n",
        "    retrieved_top_r = retrieved_ids[:r]\n",
        "    return sum(1 for cid in retrieved_top_r if cid in ground_truth_ids) / r if r else 0.0\n",
        "\n",
        "def hit_rate_at_k(retrieved_ids, ground_truth_ids, k):\n",
        "    return int(any(cid in ground_truth_ids for cid in retrieved_ids[:k]))\n",
        "\n",
        "def reciprocal_rank_at_k(retrieved_ids, ground_truth_ids, k):\n",
        "    for idx, cid in enumerate(retrieved_ids[:k]):\n",
        "        if cid in ground_truth_ids:\n",
        "            return 1.0 / (idx + 1)\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Bj3Ktydxmd"
      },
      "source": [
        "## 4. Running the Evaluation\n",
        "\n",
        "This section contains the core logic for running the evaluation. We'll define two key functions:\n",
        "\n",
        "1.  `run_query`: A simple wrapper to call the retrieval agent for a given query and parse its response to extract the retrieved chunks and attribution data.\n",
        "2.  `evaluate_single_query`: The main evaluation function. For a single query (a row in our `wide_df`), it calls `run_query`, gets the retrieved results, and then calculates all of our defined metrics (both standard IR and RAGAs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_query(user_input):\n",
        "    \"\"\"\n",
        "    Run the retrieval agent for the given user_input.\n",
        "    Returns:\n",
        "        - retrieved_chunks: list of dicts with 'content_id' and 'score'\n",
        "        - attributions: list of attribution objects (with 'content_ids')\n",
        "    \"\"\"\n",
        "    query_result = client.agents.query.create(\n",
        "        agent_id=agent_id,\n",
        "        messages=[{\n",
        "            \"content\": user_input,\n",
        "            \"role\": \"user\"\n",
        "        }],\n",
        "        include_retrieval_content_text=True,\n",
        "        retrievals_only=False\n",
        "    )\n",
        "    # Extract retrieved_chunks (content_id and score)\n",
        "    retrieved_chunks = [\n",
        "        {\"content_id\": rc.content_id, \"score\": rc.score}\n",
        "        for rc in query_result.retrieval_contents\n",
        "    ]\n",
        "\n",
        "    unique_attributions = []\n",
        "    seen = set()\n",
        "    for attr in query_result.attributions:\n",
        "        key = tuple(attr.content_ids)\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            unique_attributions.append({\"content_ids\": attr.content_ids})\n",
        "\n",
        "    # Extract attributions (list of content_ids per attribution)\n",
        "    #attributions = query_result.attributions\n",
        "    return retrieved_chunks, unique_attributions\n",
        "\n",
        "run_query(\"Apple revenue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xt11oHsBdxmd"
      },
      "outputs": [],
      "source": [
        "def evaluate_single_query(row, run_query, k=10, use_attributions=False):\n",
        "    index_df = int(row['QA_ID'])\n",
        "    user_input = row[\"prompt\"]\n",
        "    evidence_chunk_id_cols = [col for col in row.index if col.startswith('evidence_')]\n",
        "    ground_truth_ids = set(row[col] for col in evidence_chunk_id_cols if pd.notnull(row[col]))\n",
        "    retrieved_chunks, attributions = run_query(user_input)\n",
        "    \n",
        "    retriever_ids = [chunk['content_id'] for chunk in retrieved_chunks]\n",
        "    if use_attributions==True:\n",
        "        attributions_ids = [cid for chunk in attributions for cid in chunk['content_ids']]\n",
        "        retrieved_ids = attributions_ids\n",
        "    else:\n",
        "        retrieved_ids = retriever_ids\n",
        "        attributions_ids = None\n",
        "\n",
        "    #print (retrieved_ids)\n",
        "    #print (ground_truth_ids)\n",
        "    recall = recall_at_k(retrieved_ids, ground_truth_ids, k)\n",
        "    precision = precision_at_k(retrieved_ids, ground_truth_ids, k)\n",
        "    ndcg = ndcg_at_k(retrieved_chunks, ground_truth_ids, k)\n",
        "    icdg = idcg(set(range(min(len(ground_truth_ids), k))))\n",
        "    precision_r = precision_at_r(retrieved_ids, ground_truth_ids)\n",
        "    hit_rate = hit_rate_at_k(retrieved_ids, ground_truth_ids, k)\n",
        "    mrr = reciprocal_rank_at_k(retrieved_ids, ground_truth_ids, k)\n",
        "\n",
        "    # --- RAGAS metrics ---\n",
        "    reference_contexts = list(ground_truth_ids)\n",
        "    retrieved_contexts = retrieved_ids\n",
        "\n",
        "    sample = SingleTurnSample(\n",
        "        user_input=user_input,\n",
        "        reference_contexts=reference_contexts,\n",
        "        retrieved_contexts=retrieved_contexts,\n",
        "    )\n",
        "\n",
        "    # Run RAGAS metrics (must be run in an event loop)\n",
        "    async def run_ragas_metrics(sample):\n",
        "        context_recall = NonLLMContextRecall()\n",
        "        context_precision = NonLLMContextPrecisionWithReference()\n",
        "        recall_score = await context_recall.single_turn_ascore(sample)\n",
        "        precision_score = await context_precision.single_turn_ascore(sample)\n",
        "        return recall_score, precision_score\n",
        "\n",
        "    recall_score, precision_score = asyncio.run(run_ragas_metrics(sample))\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"index\": index_df,\n",
        "        \"user_input\": user_input,\n",
        "        \"k\": k,\n",
        "        \"recall@k\": recall,\n",
        "        \"precision@k\": precision,\n",
        "        \"precision@R\": precision_r,\n",
        "        \"nDCG@k\": float(ndcg),\n",
        "        \"iDCG@k\": float(icdg),\n",
        "        \"hit_rate@k\": hit_rate,\n",
        "        \"mrr@k\": mrr,\n",
        "        \"ragas_context_recall\": recall_score,\n",
        "        \"ragas_context_precision\": precision_score,\n",
        "        \"retrieved_chunks\": retrieved_chunks,  # full info\n",
        "        \"ground_truth_ids\": list(ground_truth_ids),\n",
        "        \"retriever_ids\": retriever_ids,\n",
        "        \"attributions\": attributions_ids,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1. Unit Test: Evaluating a Single Query\n",
        "\n",
        "Before running the full evaluation, it's a good practice to test our `evaluate_single_query` function on a single row. This acts as a \"unit test\" to ensure that:\n",
        "- The `run_query` function is correctly called.\n",
        "- All metrics (including the async RAGAs metrics) are calculated without errors.\n",
        "- The output dictionary has the structure we expect.\n",
        "\n",
        "If this cell runs successfully, we can be more confident that the full evaluation loop will proceed smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick the first example (row 0) from your DataFrame\n",
        "row = wide.iloc[0]\n",
        "\n",
        "# Run the evaluation for this single example\n",
        "result = evaluate_single_query(row, run_query, k=10, use_attributions=False)\n",
        "\n",
        "# Print the results\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXtVBStldxme"
      },
      "source": [
        "### 4.2. Executing the Full Evaluation Run\n",
        "\n",
        "Now that we have validated our functions on a single example, we will run the evaluation across the entire dataset.\n",
        "\n",
        "The following cell iterates through each row in our prepared `wide_df`, calling `evaluate_single_query` for each one. To prevent data loss on long runs, the script is configured to save its progress to a partial CSV file periodically.\n",
        "\n",
        "You can configure two key options here:\n",
        "- `k`: The cutoff for rank-sensitive metrics (e.g., `precision@10`, `recall@10`).\n",
        "- `use_attributions`:\n",
        "    - `False` (default): Evaluates the direct output of the **retriever**. This measures the quality of the initial candidate set.\n",
        "    - `True`: Evaluates only the chunks that were **attributed** by the generation model. This measures the quality of the final evidence used in the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esb_emrXdxme"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for idx, row in wide.iterrows():\n",
        "    try:\n",
        "        result = evaluate_single_query(row, run_query, k=10,use_attributions=False)\n",
        "        results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error on row {idx}: {e}\")\n",
        "        # Optionally, append a placeholder or skip\n",
        "        continue\n",
        "    # Save after each row (or every N rows)\n",
        "    if (idx + 1) % 25 == 0 or (idx + 1) == len(wide):  # Save every 5 rows, and at the end\n",
        "        pd.DataFrame(results).to_csv(\"retrieval_eval_results_partial.csv\", index=False)\n",
        "        print(f\"Saved progress at row {idx + 1}\")\n",
        "\n",
        "# Final save\n",
        "final = pd.DataFrame(results)\n",
        "final.to_csv(\"eval_results_final.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyzing the Results\n",
        "\n",
        "After the evaluation loop is complete, the results are stored in a final DataFrame. We can now compute the average (mean) for each metric across all queries. This gives us a high-level overview of the retrieval system's performance.\n",
        "\n",
        "The metrics below represent the average scores for the entire test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o9yzYIQRdxme",
        "outputId": "711a69e3-7b8a-4f40-a870-166ac8863038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall@k                   0.854839\n",
            "precision@k                0.164516\n",
            "precision@R                0.500000\n",
            "nDCG@k                     0.715072\n",
            "iDCG@k                     1.590225\n",
            "ragas_context_recall       0.903226\n",
            "ragas_context_precision    0.648375\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "final = pd.read_csv(\"eval_results_final.csv\")\n",
        "\n",
        "metrics = [\n",
        "    'recall@k',\n",
        "    'precision@k',\n",
        "    'precision@R',\n",
        "    'nDCG@k',\n",
        "    'iDCG@k',\n",
        "    'ragas_context_recall',\n",
        "    'ragas_context_precision'\n",
        "]\n",
        "\n",
        "means = final[metrics].mean()\n",
        "print(means)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Deeper Analysis\n",
        "\n",
        "This notebook has shown you how to assess the performance of the retrievals provided in the RAG Agent query path. Deeper analysis will consider factors:\n",
        "- The weight betweeen lexical and semantic search\n",
        "- Retreival at various stages including retriever, reranker, and filter model\n",
        "\n",
        "You can modify this script or your agent to test retrieval under different configurations to optimize your RAG Agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Comparison Metrics (Attribution-Based)\n",
        "\n",
        "For reference, these were the results from a previous run where `use_attributions` was set to `False`. This evaluates the chunks that the generation model actually used in its answer, rather than all retrieved chunks.\n",
        "\n",
        "- recall@k                   0.854839\n",
        "- precision@k                0.164516\n",
        "- precision@R                0.500000\n",
        "- nDCG@k                     0.715072\n",
        "- iDCG@k                     1.590225\n",
        "- ragas_context_recall       0.903226\n",
        "- ragas_context_precision    0.648375\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
