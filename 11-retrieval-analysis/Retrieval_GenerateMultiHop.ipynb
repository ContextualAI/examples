{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Multi-hop Question Answering Generation\n","\n","This notebook facilitates the generation of multi-hop question-answering pairs from financial documents (PDFs) using a large language model. It extracts text from uploaded PDFs, formulates questions that require combining information from different parts of one or more documents, and exports the results for analysis.\n","\n","The process involves:\n","1. Setting up the environment and API access.\n","2. Uploading and processing PDF documents.\n","3. Generating multi-hop QA pairs using a language model.\n","4. Exporting the generated QA data to JSON and Excel formats.\n","\n","This notebook is built to run on Google colab, if you are running it in another environment, you will have to modify it.\n","\n","I have included a set of sample PDFs along with generated questions and evaluations in the data folder."],"metadata":{"id":"3elnhyMBpUkd"}},{"cell_type":"markdown","source":["## Set up Claude"],"metadata":{"id":"BrJC93BaysMd"}},{"cell_type":"code","source":["!pip install pdfplumber anthropic google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests ipywidgets"],"metadata":{"id":"PHFCG-7jr6IT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slh_r8Rfvn3a"},"outputs":[],"source":["from google.colab import auth, userdata, drive, files\n","from google.auth import default\n","from googleapiclient.discovery import build\n","import requests\n","import time\n","import json\n","import ipywidgets as widgets\n","from IPython.display import display, HTML, clear_output\n","import pandas as pd\n","import os\n","import anthropic\n","import pdfplumber\n","from typing import List, Dict\n","import csv\n","from datetime import datetime\n","import tempfile\n","from io import StringIO, BytesIO\n","from openpyxl import Workbook, load_workbook\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Authenticate for Google APIs\n","auth.authenticate_user()\n","creds, _ = default()\n","\n","# Access secrets\n","try:\n","    CLAUDE_API_KEY = userdata.get('CLAUDE_API_KEY')\n","\n","    if not CLAUDE_API_KEY:\n","        raise ValueError(\"Missing CLAUDE_API_KEY. Please add it in Tools > Settings > Secrets\")\n","\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Error with API configuration: {e}\")\n","\n","# Initialize global variables\n","generated_questions = []\n","uploaded_files = []"]},{"cell_type":"markdown","source":["## Upload PDFs and Extract Text\n","\n","This section allows you to upload the PDF documents you want to use for generating multi-hop questions. The code will process these PDFs, extract their text content, and prepare them for the question generation process. You can upload multiple files.\n","\n"],"metadata":{"id":"t9o4xAdNy1vd"}},{"cell_type":"code","source":["# PDF upload and management - this will show the upload widget immediately\n","print(\"üìÇ Upload PDF files below:\")\n","from google.colab import files\n","uploaded = files.upload()\n","\n","# Process uploaded files\n","uploaded_files = []\n","for filename, content in uploaded.items():\n","    if not filename.lower().endswith('.pdf'):\n","        print(f\"‚ö†Ô∏è Ignoring non-PDF file: {filename}\")\n","        continue\n","\n","    # Create a temporary file\n","    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')\n","    temp_file.write(content)\n","    temp_file.close()\n","\n","    # Store file information\n","    file_info = {\n","        'name': filename,\n","        'path': temp_file.name,\n","        'size': len(content)\n","    }\n","\n","    uploaded_files.append(file_info)\n","    print(f\"‚úÖ Uploaded: {filename} ({len(content)/1024:.1f} KB)\")\n","\n","if len(uploaded_files) < 2:\n","    print(\"‚ÑπÔ∏è Note: For best results, upload at least 2 PDF files for multi-hop questions.\")\n","else:\n","    print(f\"‚úÖ Total files uploaded: {len(uploaded_files)}\")\n","\n","# Helper function to list uploaded PDFs\n","def list_uploaded_pdfs():\n","    \"\"\"List all currently uploaded PDFs\"\"\"\n","    global uploaded_files\n","\n","    if not uploaded_files:\n","        print(\"No PDFs have been uploaded yet.\")\n","        return\n","\n","    print(f\"üìë Currently uploaded PDFs ({len(uploaded_files)} total):\")\n","    for i, file in enumerate(uploaded_files, 1):\n","        print(f\"  {i}. {file['name']} ({file['size']/1024:.1f} KB)\")\n","\n","list_uploaded_pdfs()"],"metadata":{"id":"a6kTaS_byxhX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multi-hop QA Generation Functions\n","\n","This section contains the core logic for generating multi-hop question-answering pairs.\n","\n","It includes functions for:\n","- Extracting text from individual PDF files.\n","- Loading and processing multiple PDFs from a specified folder.\n","- Generating the prompt template for the language model, considering document preference and previously generated questions.\n","- Interacting with the language model (Anthropic Claude) to generate a single multi-hop QA pair.\n","- Orchestrating the generation of multiple QA pairs, handling progress saving and API calls.\n","- Saving the generated QA pairs to a JSON file for later use.\n","- Exporting the generated QA pairs to a CSV file.\n","\n","The main function `main_generate_qa` ties these steps together."],"metadata":{"id":"CbxYsLSQy8kc"}},{"cell_type":"code","source":["# Simplified Multi-hop QA Generator\n","# Focus on generating JSON data, export to spreadsheet at the end\n","\n","import json\n","import anthropic\n","import pdfplumber\n","import tempfile\n","import os\n","import glob\n","from datetime import datetime\n","from typing import List, Dict\n","import time\n","\n","# Global variables\n","generated_questions = []\n","uploaded_files = []\n","\n","# Initialize Anthropic client (you'll need to set your API key)\n","# CLAUDE_API_KEY = \"your_api_key_here\"\n","# client = anthropic.Client(api_key=CLAUDE_API_KEY)\n","\n","def extract_text_from_pdf(pdf_path: str) -> str:\n","    \"\"\"Extract text content from a PDF file using pdfplumber.\"\"\"\n","    text = ''\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            for page_num, page in enumerate(pdf.pages, start=1):\n","                page_text = page.extract_text()\n","                if page_text:\n","                    text += f\"Page {page_num}: {page_text}\\n\"\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Error extracting text from PDF: {e}\")\n","        raise\n","\n","    if len(text) < 100:\n","        print(\"‚ö†Ô∏è Extracted text is too short. Please check the PDF file.\")\n","        raise ValueError(\"Extracted text is too short\")\n","\n","    return text\n","\n","def load_pdfs_from_folder(folder_path: str, max_files: int = None) -> List[Dict]:\n","    \"\"\"Load all PDFs from a folder and extract their text.\"\"\"\n","    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n","\n","    if max_files:\n","        pdf_files = pdf_files[:max_files]\n","\n","    print(f\"Found {len(pdf_files)} PDF files in {folder_path}\")\n","\n","    documents = []\n","    for pdf_path in pdf_files:\n","        filename = os.path.basename(pdf_path)\n","        file_size = os.path.getsize(pdf_path)\n","\n","        print(f\"Processing {filename} ({file_size/1024:.1f} KB)...\")\n","\n","        try:\n","            text = extract_text_from_pdf(pdf_path)\n","            documents.append({\n","                'name': filename,\n","                'path': pdf_path,\n","                'size': file_size,\n","                'text': text\n","            })\n","            print(f\"‚úÖ Successfully processed {filename}\")\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Error processing {filename}: {e}\")\n","            continue\n","\n","    return documents\n","\n","def get_prompt_template(doc_preference: float, previous_questions: List[str] = None) -> str:\n","    \"\"\"Generate prompt template based on document preference.\"\"\"\n","\n","    if previous_questions is None:\n","        previous_questions = []\n","\n","    base_instruction = \"\"\"\n","    Generate 1 practical financial analysis question that requires two distinct information retrievals in sequence.\n","    Focus on questions that a financial analyst or executive would realistically ask when analyzing company performance.\n","    The first retrieval MUST provide specific information needed to know what to look for in the second retrieval.\n","\n","    REQUIREMENTS:\n","    - Questions should explicitly mention the company name and time period\n","    - Questions should follow natural business logic and analysis patterns\n","    - Use clear, objective metrics rather than vague references\n","    - Focus on meaningful business insights that require multi-step reasoning\n","\n","    EXCELLENT EXAMPLES of realistic two-step retrievals:\n","    1. \"For [Company]'s Q3 FY2024, what was the year-over-year revenue change in the segment that management identified as the biggest driver of growth, and what risks did management highlight for this segment?\"\n","\n","    2. \"In [Company]'s Q3 FY2024, for the product line with the highest profit margin, what were the key investments made this quarter and associated risk factors?\"\n","\n","    3. \"Among [Company]'s regions with double-digit revenue growth in Q3 2024, which one had the highest customer acquisition cost, and what operational risks were identified?\"\n","\n","    TOPIC DIVERSITY:\n","    - Each question should explore a different aspect of the company's financial story\n","    - Avoid repeating topics from these previously asked questions: {previous_questions}\n","    - Consider various aspects like segments, regions, strategic initiatives, risk factors, capital allocation, etc.\n","    \"\"\"\n","\n","    if doc_preference < 0.3:\n","        doc_guidance = \"\"\"\n","        Focus on finding high-quality multi-hop questions within individual documents.\n","        \"\"\"\n","    elif doc_preference < 0.7:\n","        doc_guidance = \"\"\"\n","        Look for high-quality multi-hop questions either within individual documents or across documents.\n","        \"\"\"\n","    else:\n","        doc_guidance = \"\"\"\n","        Prioritize questions that connect information across multiple documents.\n","        \"\"\"\n","\n","    # Format previous questions\n","    prev_questions = \"\\n\".join([f\"- {q}\" for q in previous_questions[-5:]])\n","    formatted_base = base_instruction.format(\n","        previous_questions=prev_questions if previous_questions else \"None yet\"\n","    )\n","\n","    return f\"\"\"{formatted_base}\\n\\n{doc_guidance}\\n\\n\n","    Return ONLY a JSON object in this exact format:\n","    {{\n","        \"question\": \"question text\",\n","        \"answer\": \"detailed answer that naturally references source documents and includes relevant numerical data and risk factors\",\n","        \"steps\": [\n","            {{\n","                \"step\": 1,\n","                \"description\": \"what needs to be found first\",\n","                \"document\": \"filename of the document used\",\n","                \"evidence\": \"EXACT 10-15 word snippet from the document that contains the key information - copy the text verbatim\"\n","            }},\n","            {{\n","                \"step\": 2,\n","                \"description\": \"what needs to be found second\",\n","                \"depends_on\": \"specific output from step 1 that was needed\",\n","                \"document\": \"filename of the document used\",\n","                \"evidence\": \"EXACT 10-15 word snippet from the document that contains the key information - copy the text verbatim\"\n","            }}\n","        ],\n","        \"multi_hop_reasoning\": \"explanation of why finding the information in step 1 was necessary to know what to look for in step 2\"\n","    }}\n","\n","    CRITICAL: For the \"evidence\" field in each step, you MUST copy an exact 10-15 word snippet directly from the source document text. This snippet should contain the specific data point or information mentioned in that step. Do not paraphrase or summarize - copy the exact words as they appear in the document.\"\"\"\n","\n","def generate_qa_pair(documents: List[Dict], doc_preference: float = 0.7,\n","                    previous_questions: List[str] = None, client=None) -> Dict:\n","    \"\"\"Generate a single QA pair from the documents.\"\"\"\n","\n","    if client is None:\n","        raise ValueError(\"Anthropic client not provided\")\n","\n","    # Prepare document content for the prompt (limit size for API)\n","    docs_content = \"\\n\\n\".join([\n","        f\"Document: {doc['name']}\\n{doc['text'][:50000]}\"\n","        for doc in documents\n","    ])\n","\n","    # Get prompt template\n","    prompt_template = get_prompt_template(doc_preference, previous_questions)\n","\n","    try:\n","        response = client.messages.create(\n","            model=\"claude-sonnet-4-0\",\n","            max_tokens=4096,\n","            messages=[{\n","                \"role\": \"user\",\n","                \"content\": f\"Available documents:\\n{docs_content}\\n\\n{prompt_template}\"\n","            }]\n","        )\n","\n","        response_text = response.content[0].text.strip()\n","\n","        # Extract and parse JSON\n","        json_start = response_text.find('{')\n","        json_end = response_text.rfind('}') + 1\n","\n","        if json_start == -1 or json_end == 0:\n","            raise ValueError(\"No JSON object found in response\")\n","\n","        json_str = response_text[json_start:json_end]\n","        qa_pair = json.loads(json_str)\n","\n","        # Add metadata\n","        qa_pair['generated_at'] = datetime.now().isoformat()\n","        qa_pair['doc_preference'] = doc_preference\n","        qa_pair['source_documents'] = [doc['name'] for doc in documents]\n","\n","        return qa_pair\n","\n","    except Exception as e:\n","        print(f\"‚ö†Ô∏è Error generating QA pair: {e}\")\n","        raise\n","\n","def generate_multiple_qa_pairs(documents: List[Dict], num_questions: int = 5,\n","                              doc_preference: float = 0.7, client=None) -> List[Dict]:\n","    \"\"\"Generate multiple QA pairs from documents.\"\"\"\n","\n","    print(f\"üöÄ Generating {num_questions} QA pairs...\")\n","    print(f\"üìÑ Using {len(documents)} documents: {[doc['name'] for doc in documents]}\")\n","    print(f\"üîß Document preference: {doc_preference} (higher = more cross-document connections)\")\n","\n","    qa_pairs = []\n","    generated_questions = []\n","\n","    for i in range(num_questions):\n","        print(f\"\\n‚è≥ Generating question {i+1}/{num_questions}...\")\n","\n","        try:\n","            qa_pair = generate_qa_pair(\n","                documents=documents,\n","                doc_preference=doc_preference,\n","                previous_questions=generated_questions,\n","                client=client\n","            )\n","\n","            qa_pairs.append(qa_pair)\n","            generated_questions.append(qa_pair['question'])\n","\n","            print(f\"‚úÖ Generated: {qa_pair['question'][:100]}...\")\n","\n","             # Save progress after each question\n","            with open('qa_backup.json', 'w') as f:\n","                json.dump(qa_pairs, f, indent=2)\n","\n","\n","            # Brief pause between requests\n","            time.sleep(1)\n","\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Failed to generate question {i+1}: {e}\")\n","            continue\n","\n","    print(f\"\\nüéâ Successfully generated {len(qa_pairs)} QA pairs!\")\n","    return qa_pairs\n","\n","def save_qa_pairs_to_json(qa_pairs: List[Dict], output_path: str = None) -> str:\n","    \"\"\"Save QA pairs to a JSON file.\"\"\"\n","\n","    if output_path is None:\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","        output_path = f\"qa_pairs_{timestamp}.json\"\n","\n","    with open(output_path, 'w', encoding='utf-8') as f:\n","        json.dump(qa_pairs, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"üíæ Saved {len(qa_pairs)} QA pairs to {output_path}\")\n","    return output_path\n","\n","def export_to_csv(qa_pairs: List[Dict], output_path: str = None) -> str:\n","    \"\"\"Export QA pairs to CSV format for easy spreadsheet import.\"\"\"\n","    import csv\n","\n","    if output_path is None:\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","        output_path = f\"qa_pairs_{timestamp}.csv\"\n","\n","    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n","        writer = csv.writer(f)\n","\n","        # Write header\n","        writer.writerow([\n","            'Question', 'Answer', 'Multi-hop Reasoning', 'Evidence Steps (JSON)',\n","            'Generated At', 'Source Documents'\n","        ])\n","\n","        # Write data\n","        for qa in qa_pairs:\n","            writer.writerow([\n","                qa.get('question', ''),\n","                qa.get('answer', ''),\n","                qa.get('multi_hop_reasoning', ''),\n","                json.dumps(qa.get('steps', [])),\n","                qa.get('generated_at', ''),\n","                ', '.join(qa.get('source_documents', []))\n","            ])\n","\n","    print(f\"üìä Exported {len(qa_pairs)} QA pairs to {output_path}\")\n","    return output_path\n","\n","def main_generate_qa(folder_path: str, num_questions: int = 5,\n","                    doc_preference: float = 0.7, max_files: int = None,\n","                    claude_api_key: str = None):\n","    \"\"\"Main function to generate QA pairs from a folder of PDFs.\"\"\"\n","\n","    # Initialize client\n","    if claude_api_key is None:\n","        raise ValueError(\"Claude API key is required\")\n","    client = anthropic.Client(api_key=claude_api_key)\n","\n","    # Load documents\n","    print(\"üìÇ Loading PDFs...\")\n","    documents = load_pdfs_from_folder(folder_path, max_files=max_files)\n","\n","    if not documents:\n","        print(\"‚ùå No documents loaded successfully\")\n","        return None\n","\n","    # Generate QA pairs\n","    qa_pairs = generate_multiple_qa_pairs(\n","        documents=documents,\n","        num_questions=num_questions,\n","        doc_preference=doc_preference,\n","        client=client\n","    )\n","\n","    if not qa_pairs:\n","        print(\"‚ùå No QA pairs generated successfully\")\n","        return None\n","\n","    # Save results\n","    json_path = save_qa_pairs_to_json(qa_pairs)\n","    csv_path = export_to_csv(qa_pairs)\n","\n","    print(f\"\\n‚úÖ Complete! Generated {len(qa_pairs)} QA pairs\")\n","    print(f\"üìÑ JSON output: {json_path}\")\n","    print(f\"üìä CSV output: {csv_path}\")\n","\n","    return qa_pairs, json_path, csv_path"],"metadata":{"id":"Njd-2m4qRal-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run QA Generation\n","\n","Execute this cell to start the multi-hop question generation process using the PDFs uploaded earlier and the specified parameters (number of questions, document preference, maximum files). This process may take some time depending on the number and size of the documents and the number of questions requested.\n"],"metadata":{"id":"dMFyq-QXqnZn"}},{"cell_type":"code","source":["\n","CLAUDE_API_KEY = userdata.get('CLAUDE_API_KEY')\n","qa_pairs, json_file, csv_file = main_generate_qa(\n","    folder_path=\"/content/\",\n","    num_questions=100,\n","    doc_preference=0.5,  # 0.0 = single doc focus, 1.0 = cross-doc focus\n","    max_files=7,         # Limit number of PDFs to process\n","    claude_api_key=CLAUDE_API_KEY\n",")"],"metadata":{"id":"t2Cuuild0hau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aQvz8Ffg1lnd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export the JSON to a Spreadsheet"],"metadata":{"id":"nalzF1AvrJCg"}},{"cell_type":"code","source":["# JSON to Spreadsheet Exporter\n","# Handles exporting the generated QA pairs to Excel\n","\n","import json\n","import pandas as pd\n","from google.oauth2.credentials import Credentials\n","from google.auth import default\n","from googleapiclient.discovery import build\n","import openpyxl\n","from datetime import datetime\n","\n","def load_qa_pairs_from_json(json_path: str) -> list:\n","    \"\"\"Load QA pairs from JSON file.\"\"\"\n","    with open(json_path, 'r', encoding='utf-8') as f:\n","        qa_pairs = json.load(f)\n","    print(f\"üìñ Loaded {len(qa_pairs)} QA pairs from {json_path}\")\n","    return qa_pairs\n","\n","def qa_pairs_to_dataframe(qa_pairs: list, multi_evidence_rows: bool = False) -> pd.DataFrame:\n","    \"\"\"Convert QA pairs to a pandas DataFrame.\"\"\"\n","\n","    if multi_evidence_rows:\n","        # Each evidence step gets its own row\n","        rows = []\n","        for qa_id, qa in enumerate(qa_pairs, 1):\n","            steps = qa.get('steps', [])\n","            for step_idx, step in enumerate(steps):\n","                row = {\n","                    'QA_ID': qa_id,\n","                    'Question': qa['question'] if step_idx == 0 else '',  # Only on first row\n","                    'Answer': qa['answer'] if step_idx == 0 else '',\n","                    'Multi_hop_Reasoning': qa.get('multi_hop_reasoning', '') if step_idx == 0 else '',\n","                    'Step_Number': step.get('step', step_idx + 1),\n","                    'Step_Description': step.get('description', ''),\n","                    'Source_Document': step.get('document', ''),\n","                    'Evidence': step.get('evidence', ''),\n","                    'Depends_On': step.get('depends_on', ''),\n","                    'Generated_At': qa.get('generated_at', ''),\n","                    'Source_Documents': ', '.join(qa.get('source_documents', []))\n","                }\n","                rows.append(row)\n","\n","    else:\n","        # Single row per QA pair (evidence as JSON)\n","        rows = []\n","        for qa_id, qa in enumerate(qa_pairs, 1):\n","            row = {\n","                'QA_ID': qa_id,\n","                'Question': qa['question'],\n","                'Answer': qa['answer'],\n","                'Multi_hop_Reasoning': qa.get('multi_hop_reasoning', ''),\n","                'Evidence_Steps': json.dumps(qa.get('steps', []), indent=2),\n","                'Generated_At': qa.get('generated_at', ''),\n","                'Source_Documents': ', '.join(qa.get('source_documents', []))\n","            }\n","            rows.append(row)\n","\n","    return pd.DataFrame(rows)\n","\n","def export_to_excel(qa_pairs: list, output_path: str = None,\n","                   multi_evidence_rows: bool = False) -> str:\n","    \"\"\"Export QA pairs to Excel file.\"\"\"\n","\n","    if output_path is None:\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","        layout = \"multi_row\" if multi_evidence_rows else \"single_row\"\n","        output_path = f\"qa_pairs_{layout}_{timestamp}.xlsx\"\n","\n","    # Convert to DataFrame\n","    df = qa_pairs_to_dataframe(qa_pairs, multi_evidence_rows)\n","\n","    # Write to Excel with formatting\n","    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n","        df.to_excel(writer, sheet_name='QA_Pairs', index=False)\n","\n","        # Get the workbook and worksheet\n","        workbook = writer.book\n","        worksheet = writer.sheets['QA_Pairs']\n","\n","        # Auto-adjust column widths\n","        for column in worksheet.columns:\n","            max_length = 0\n","            column_letter = column[0].column_letter\n","\n","            for cell in column:\n","                try:\n","                    if len(str(cell.value)) > max_length:\n","                        max_length = len(str(cell.value))\n","                except:\n","                    pass\n","\n","            adjusted_width = min(max_length + 2, 50)  # Cap at 50 characters\n","            worksheet.column_dimensions[column_letter].width = adjusted_width\n","\n","        # Style the header row\n","        header_font = openpyxl.styles.Font(bold=True)\n","        header_fill = openpyxl.styles.PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n","\n","        for cell in worksheet[1]:\n","            cell.font = header_font\n","            cell.fill = header_fill\n","\n","    print(f\"üìä Exported to Excel: {output_path}\")\n","    return output_path"],"metadata":{"id":"iz6LS2Qnx-ra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%ls -al"],"metadata":{"id":"1wtMHdUf40SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the generated QA pairs from the JSON file.\n","# Replace \"qa_pairs_20250528_211321.json\" with the actual filename if it differs.\n","qa_pairs = load_qa_pairs_from_json(\"qa_pairs_20250528_211321.json\")\n","\n","# Export the loaded QA pairs to an Excel file using the multi-evidence row format.\n","export_to_excel(qa_pairs, multi_evidence_rows=True)"],"metadata":{"id":"yyfm5zuToLC1"},"execution_count":null,"outputs":[]}]}