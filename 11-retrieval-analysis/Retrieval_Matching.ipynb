{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2nmrhpCdxmZ"
   },
   "source": [
    "# Evaluating Retrieval Quality: Evidence-to-Chunk Matching for RAG Systems\n",
    "\n",
    "## üéØ What This Notebook Does\n",
    "\n",
    "This notebook solves a critical challenge in RAG evaluation: **automatically determining whether retrieved chunks contain the ground-truth evidence needed to answer questions correctly.**\n",
    "\n",
    "Instead of manually checking hundreds of retrieved chunks, this system:\n",
    "- ‚úÖ Automatically matches evidence strings to retrieved chunks using fuzzy matching + numeric overlap\n",
    "- ‚úÖ Filters multi-hop questions to keep only those with complete evidence coverage\n",
    "- ‚úÖ Creates a high-quality evaluation dataset for measuring retrieval performance\n",
    "- ‚úÖ Outputs ready-to-use data for computing precision, recall, and other retrieval metrics\n",
    "\n",
    "## üèóÔ∏è The Problem This Solves\n",
    "\n",
    "When evaluating RAG systems, you need to know:\n",
    "1. **Did the retrieval system find the right information?** (Recall)\n",
    "2. **How much irrelevant information was retrieved?** (Precision)\n",
    "3. **Can the system handle complex, multi-step reasoning?** (Multi-hop evaluation)\n",
    "\n",
    "Manual evaluation doesn't scale. This notebook automates the process.\n",
    "\n",
    "## üìä Input & Output\n",
    "\n",
    "**Input:**\n",
    "- QA dataset with annotated evidence strings\n",
    "\n",
    "**Output:**\n",
    "- Matched evidence pairs (evidence string ‚Üî chunk ID)\n",
    "\n",
    "## üîß Prerequisites\n",
    "- Contextual AI API access (or adapt the retrieval functions for your system)\n",
    "- QA dataset (like the one from the generation notebook)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/11-retrieval-analysis/Retrieval_Matching.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cteal1Ndxmb"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install and import the necessary libraries. This notebook relies on `pandas` for data manipulation, `thefuzz` for fuzzy string matching, and the `contextual` client for interacting with the datastore and retrieval APIs. We also import standard libraries for handling data and making API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuNPF3po3Y0t"
   },
   "outputs": [],
   "source": [
    "# ! pip install thefuzz[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzZYbsQjdxmb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextRecall, NonLLMContextPrecisionWithReference\n",
    "\n",
    "\n",
    "import requests\n",
    "from thefuzz import fuzz                    # ‚Üê matches install above\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Any, Optional\n",
    "from contextual import ContextualAI\n",
    "client = ContextualAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7NYzxar3Y0t"
   },
   "source": [
    "## 2. Prepare Datastore with Metadata for Filtering\n",
    "\n",
    "To ensure that we only search for matches within the correct source document, it's crucial to have reliable metadata. Here, we'll add a `Filename` field to the custom metadata of each document in our datastore.\n",
    "\n",
    "This allows us to use a `documents_filters` parameter in our retrieval requests, which dramatically narrows down the search space and prevents incorrect matches from other documents. This is a best practice for building robust retrieval evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmFlbl5x3Y0u"
   },
   "outputs": [],
   "source": [
    "datastore_id = 'ecd873f1-134a-4c43-8d69-bcd32533fd67'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TptvvKiY3Y0u"
   },
   "outputs": [],
   "source": [
    "docs = client.datastores.documents.list(datastore_id=datastore_id)\n",
    "doc_pairs = [(doc.id, doc.name) for doc in docs.documents]\n",
    "print(\"Document ID and Name pairs:\")\n",
    "for doc_id, name in doc_pairs:\n",
    "    print(f\"ID: {doc_id}, Name: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxOqFzqQ3Y0u"
   },
   "outputs": [],
   "source": [
    "# Loop through doc_pairs and set the Filename metadata for each document\n",
    "for doc_id, name in doc_pairs:\n",
    "    result = client.datastores.documents.set_metadata(\n",
    "        datastore_id=datastore_id,\n",
    "        document_id=doc_id,\n",
    "        custom_metadata={\"Filename\": name}\n",
    "    )\n",
    "    print(f\"Set metadata for {name} (ID: {doc_id}): {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVPZyJeF3Y0v"
   },
   "outputs": [],
   "source": [
    "#Verify\n",
    "document_id = docs.documents[0].id\n",
    "metadata = client.datastores.documents.metadata(datastore_id = datastore_id,\n",
    "                        document_id = document_id)\n",
    "print(\"Document metadata:\", metadata.custom_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wePzfZEc3Y0v"
   },
   "source": [
    "## 3. Load and Preprocess Annotated Data\n",
    "\n",
    "Next, we load the annotated dataset, which contains the ground-truth information for our evaluation. This dataset is structured as an Excel file where each row corresponds to a piece of evidence required to answer a multi-hop question.\n",
    "\n",
    "The key columns for our purposes are:\n",
    "- `Question`: The user query.\n",
    "- `Source_Document`: The filename of the document containing the evidence.\n",
    "- `Evidence`: The ground-truth text string that must be found in a retrieved chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt4DU4Ph3Y0v"
   },
   "outputs": [],
   "source": [
    "XLSX_PATH = \"qa_pairs_multi_row_20250616_174936.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHDYjX-B3Y0v"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(XLSX_PATH)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Buy80iYr3Y0w"
   },
   "source": [
    "### Data Cleaning: Forward-Filling Questions\n",
    "\n",
    "Our dataset is designed for multi-hop questions, where a single complex question requires multiple steps (and evidence strings) to answer. In the raw data, the `Question` and `Answer` are only present on the first row for a given `QA_ID`.\n",
    "\n",
    "To create a clean, flat structure where every row has a question, we'll use `ffill()` (forward-fill) to propagate the question and answer to all subsequent rows belonging to the same `QA_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeuE3pL33Y0w"
   },
   "outputs": [],
   "source": [
    "df['Question'] = df['Question'].fillna(method='ffill')\n",
    "df['Answer'] = df['Answer'].fillna(method='ffill')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtnAaR_B3Y0w"
   },
   "source": [
    "## 4. Evidence-to-Chunk Matching Algorithm\n",
    "\n",
    "This section contains the core logic for matching evidence strings to retrieved chunks. A reliable matching function is essential for automated retrieval evaluation, as it allows us to programmatically verify if the retrieved content contains the necessary information.\n",
    "\n",
    "Our approach uses a hybrid scoring method:\n",
    "\n",
    "1.  **Fuzzy Matching**: Uses `thefuzz.token_set_ratio` to compare the textual similarity between the evidence and a chunk, ignoring word order.\n",
    "2.  **Numeric Overlap**: Extracts and compares numeric tokens (e.g., \"$1.2M\", \"50%\") separately. This is crucial for financial or data-heavy documents where numbers are key identifiers.\n",
    "\n",
    "The final score is a weighted average of the fuzzy and numeric scores. A chunk is considered a match if its score exceeds a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7S8qR7e3Y0w"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "API_KEY = os.environ[\"CONTEXTUAL_API_KEY\"]\n",
    "\n",
    "## Agent Configs\n",
    "AGENT_ID = '15543690-68fd-49e7-8fc9-1f53c8e42e33'\n",
    "\n",
    "## configs for retrieval\n",
    "TOP_RETRIEVED_DOCS = 150\n",
    "RETRIEVAL_ALPHA = 0.9\n",
    "\n",
    "# Configs for evidence string Matching\n",
    "# recommended to enable only when working with numerical heavy datasets like financial tables.\n",
    "ENABLE_PREPROCESSING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_LmRIAk3Y0w"
   },
   "source": [
    "### Custom Retrieval Logic\n",
    "\n",
    "To maximize our chances of finding a match, we need to configure the retrieval system to return a broad set of candidate chunks. During the initial matching phase, it's often beneficial to cast a wider net than you would in a production application.\n",
    "\n",
    "This retrieval function is configured to:\n",
    "\n",
    "- Increase the number of retrieved documents (`top_k`).\n",
    "- Bypass the reranker and filter models, which might otherwise remove relevant but lower-scored chunks.\n",
    "- Adjust the balance between lexical and semantic search to favor more direct term matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIlm0yAk3Y0w"
   },
   "outputs": [],
   "source": [
    "## Test Query to make sure the retrieval is working\n",
    "query = \"What is the total revenue for Tesla in 2024?\"\n",
    "source_document = \"Tesla_2024_Annual_Report.pdf\"\n",
    "\n",
    "url = f\"https://api.app.contextual.ai/v1/applications/{AGENT_ID}/query?retrievals_only=true\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "payload: dict[str, Any] = {\n",
    "    \"stream\": False,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    \"documents_filters\": {\n",
    "        \"operator\": \"AND\",\n",
    "        \"filters\": [\n",
    "            {\"field\": \"Filename\", \"operator\": \"equals\", \"value\": source_document}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# network I/O ‚Äì may raise Timeout / HTTPError ‚Üí caught by tenacity\n",
    "resp = requests.post(url, headers=headers,\n",
    "                        data=json.dumps(payload))\n",
    "resp.raise_for_status()\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDeT6WOT3Y0w"
   },
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))\n",
    "def retrieve_documents(\n",
    "    query: str,\n",
    "    source_document: str,\n",
    "    *,\n",
    "    alpha: float = RETRIEVAL_ALPHA,  # Uses the value from your config cell\n",
    "    top_k: int = TOP_RETRIEVED_DOCS,    # Uses the value from your config cell\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A single, simplified function to retrieve documents that combines all necessary logic.\n",
    "    \"\"\"\n",
    "    # Step 1: Build the specific override config object. This is not working right now\n",
    "    override_cfg = {\n",
    "        \"filter_retrievals\": False,\n",
    "        \"rerank_retrievals\": False,\n",
    "        \"lexical_alpha\": 1.0 - alpha,\n",
    "        \"semantic_alpha\": alpha,\n",
    "        \"top_k\": top_k,\n",
    "    }\n",
    "\n",
    "    # Step 2: Build the full payload for the API request.\n",
    "    url = f\"https://api.app.contextual.ai/v1/applications/{AGENT_ID}/query?retrievals_only=true\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload: Dict[str, Any] = {\n",
    "        \"stream\": False,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "        \"documents_filters\": {\n",
    "            \"operator\": \"AND\",\n",
    "            \"filters\": [\n",
    "                {\"field\": \"Filename\", \"operator\": \"equals\", \"value\": source_document}\n",
    "            ]\n",
    "        },\n",
    "       # \"override_retrieval_config\": override_cfg\n",
    "    }\n",
    "    # Step 3: Make the request and return the desired content.\n",
    "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    # The API returns a list of dicts in the \"retrieval_contents\" key\n",
    "    return resp.json().get(\"retrieval_contents\", [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhGyNIDJ3Y0w"
   },
   "outputs": [],
   "source": [
    "def _preprocess_text(text: str) -> str:\n",
    "    \"\"\"Lightweight text normalization to improve fuzzy matching robustness.\n",
    "\n",
    "    Steps:\n",
    "    1. Lower‚Äêcase everything.\n",
    "    2. Strip table/markdown characters (``|``), repeated dashes and newlines.\n",
    "    3. Remove formatting punctuation such as ``$``, ``%``, ``(``, ``)`` and ``+``.\n",
    "    4. Remove commas that only serve as thousands separators inside numbers (e.g. ``2,000`` ‚Üí ``2000``).\n",
    "    5. Replace any remaining non‚Äêalphanumeric characters with a single space and collapse runs of whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"|\", \" \")\n",
    "    text = re.sub(r\"-+\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"[\\(\\)\\$\\%\\+]\", \"\", text)\n",
    "    # Remove commas between digits (e.g. 1,234 -> 1234)\n",
    "    text = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", text)\n",
    "    # Remove any other non alphanum / period characters\n",
    "    text = re.sub(r\"[^a-z0-9\\. ]\", \" \", text)\n",
    "    # Collapse redundant whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Regex for capturing numbers that appear in evidence / chunks.\n",
    "_NUMBER_PATTERN = (\n",
    "    r\"[-+]?\\$?\\s*(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?%?|\"  # standard numbers / currency / percentages\n",
    "    r\"\\([^)]*\\d+[^)]*\\)\"  # numbers enclosed in parentheses\n",
    ")\n",
    "\n",
    "\n",
    "def _extract_numbers(text: str) -> List[str]:\n",
    "    \"\"\"Return a list of the numeric substrings found in *text*.\"\"\"\n",
    "    return [m.strip() for m in re.findall(_NUMBER_PATTERN, text)]\n",
    "\n",
    "\n",
    "def matcher_with_metadata(\n",
    "    evidence_str: str,\n",
    "    retrieved_chunks: List[Dict[str, Any]],\n",
    "    threshold: float = 0.85,\n",
    "    *,\n",
    "    alpha: float = 1.0,\n",
    "    preprocess_text: bool = ENABLE_PREPROCESSING,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Return the retrieved chunk (with metadata) that best matches evidence_str.\n",
    "\n",
    "    This function finds the best matching chunk from a list of chunks containing\n",
    "    both content text and metadata. The scoring mirrors the non-LLM portion of\n",
    "    ``EvidenceMatchFuzzyLLM``:\n",
    "\n",
    "    ‚Ä¢ The *fuzzy* component uses `token_set_ratio` between preprocessed strings\n",
    "    ‚Ä¢ The *numeric* component measures overlap of extracted numeric tokens\n",
    "    ‚Ä¢ The final score is ``alpha * fuzzy + (1-alpha) * numeric`` when evidence\n",
    "      contains numbers, otherwise just the fuzzy score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evidence_str : str\n",
    "        The target evidence string to match against.\n",
    "    retrieved_chunks : List[Dict[str, Any]]\n",
    "        List of chunk dictionaries, each containing a \"content_text\" key with\n",
    "        the text content to match against, plus any additional metadata.\n",
    "    threshold : float, default 0.85\n",
    "        Minimum score required to return a match. Chunks scoring below this\n",
    "        threshold will result in None being returned.\n",
    "    alpha : float, default 1.0\n",
    "        Weight assigned to the fuzzy score when both fuzzy and numeric scores\n",
    "        are available. For chunks heavy with financial tables, alpha = 0.8\n",
    "        is recommended to give more weight to numeric matching.\n",
    "    preprocess_text : bool, default ENABLE_PREPROCESSING\n",
    "        Whether to run light normalization before matching. Preprocessing is\n",
    "        helpful for chunks where numbers matter significantly, such as\n",
    "        financial tables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optional[Dict[str, Any]]\n",
    "        The complete chunk dictionary from retrieved_chunks with the highest\n",
    "        score, or None if no chunk meets the threshold or if input is empty.\n",
    "        The returned dictionary includes both the matched content and any\n",
    "        associated metadata.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> chunks = [\n",
    "    ...     {\"content_text\": \"Revenue was $1.2M in Q1\", \"source\": \"report.pdf\"},\n",
    "    ...     {\"content_text\": \"Expenses totaled $800K\", \"source\": \"budget.xlsx\"}\n",
    "    ... ]\n",
    "    >>> result = matcher_with_metadata(\"Revenue $1.2M\", chunks)\n",
    "    >>> print(result[\"source\"])  # \"report.pdf\"\n",
    "    \"\"\"\n",
    "    if not evidence_str or not retrieved_chunks:\n",
    "        return None\n",
    "\n",
    "    norm_evidence = _preprocess_text(evidence_str) if preprocess_text else evidence_str\n",
    "    evidence_nums = _extract_numbers(norm_evidence)\n",
    "\n",
    "    best_chunk = None\n",
    "    best_score = -1.0\n",
    "\n",
    "    for chunk in retrieved_chunks:\n",
    "        chunk_text = chunk[\"content_text\"]\n",
    "        norm_chunk = _preprocess_text(chunk_text) if preprocess_text else chunk_text\n",
    "\n",
    "        # Short-circuit if the entire evidence string is a substring\n",
    "        if norm_evidence in norm_chunk:\n",
    "            return chunk  # Return the whole dict\n",
    "\n",
    "        fuzzy_score = fuzz.token_set_ratio(norm_evidence, norm_chunk) / 100.0\n",
    "        chunk_nums = _extract_numbers(norm_chunk)\n",
    "        numeric_matches = sum(1 for n in evidence_nums if n in chunk_nums)\n",
    "        numeric_score = numeric_matches / len(evidence_nums) if evidence_nums else 0.0\n",
    "\n",
    "        final_score = alpha * fuzzy_score + (1 - alpha) * numeric_score if evidence_nums else fuzzy_score\n",
    "\n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            best_chunk = chunk\n",
    "\n",
    "    return best_chunk if best_score >= threshold else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTaY0BI83Y0x"
   },
   "source": [
    "### Unit Test: Matching a Single Row\n",
    "\n",
    "Before processing the entire dataset, it's good practice to test our `retrieve_documents` and `matcher_with_metadata` functions on a single example. This helps verify that the API calls are working correctly, the data is being processed as expected, and the matching logic is sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUcDWy2N3Y0x"
   },
   "outputs": [],
   "source": [
    "# Get the first row (make sure it's not a NaN row)\n",
    "row = df.iloc[1]\n",
    "\n",
    "# Extract the query and evidence string\n",
    "query = row['Question']\n",
    "evidence_str = row['Evidence']\n",
    "source_document = row['Source_Document']\n",
    "\n",
    "print(query)\n",
    "print(evidence_str)\n",
    "\n",
    "# Retrieve full response and get the list of dicts\n",
    "retrieved_chunks= retrieve_documents(query=query,source_document=source_document)\n",
    "\n",
    "# Run the matcher\n",
    "match = matcher_with_metadata(evidence_str, retrieved_chunks)\n",
    "\n",
    "# Add results to DataFrame\n",
    "df.at[row.name, 'Match'] = bool(match)\n",
    "df.at[row.name, 'Content_Id'] = match['content_id'] if match else None\n",
    "\n",
    "# Print the result\n",
    "if match:\n",
    "    print(\"Match found!\")\n",
    "    #print(\"Matched content_id:\", match[\"content_id\"])\n",
    "   # print(\"Matched chunk text:\\n\", match[\"content_text\"])\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYvXLaSC3Y0x"
   },
   "source": [
    "## 5. Batch Processing: Matching the Entire Dataset\n",
    "\n",
    "Now that we've validated the process on a single row, we'll apply it to every row in the DataFrame. We iterate through the dataset, retrieve candidate chunks for each evidence string, and run the matcher.\n",
    "\n",
    "The results‚Äîa boolean `Match` status and the `Content_Id` of the matched chunk‚Äîare stored in new columns in the DataFrame. This process can take some time, as it involves making an API call for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZlAPDDZ3Y0x"
   },
   "outputs": [],
   "source": [
    "# Initialize columns\n",
    "df['Match'] = False\n",
    "df['Content_Id'] = None\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    query = row['Question']\n",
    "    evidence_str = row['Evidence']\n",
    "    source_document = row['Source_Document']\n",
    "    retrieved_chunks= retrieve_documents(query=query,source_document=source_document)\n",
    "    match = matcher_with_metadata(evidence_str, retrieved_chunks)\n",
    "    df.at[idx, 'Match'] = bool(match)\n",
    "    df.at[idx, 'Content_Id'] = match['content_id'] if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38Xja6ca3Y0x"
   },
   "source": [
    "### Analysis: Reviewing Match Results\n",
    "\n",
    "After running the batch process, let's inspect the results. We can check the `tail` of the DataFrame and count the total number of successful matches to get a sense of the match rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6XYfaej3Y0x"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZE4p6Vr3Y0x"
   },
   "source": [
    "Not all the annotated evidence was correctly matched. For now, we will continue with the rows with matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J94YhqSv3Y0x"
   },
   "outputs": [],
   "source": [
    "true_count = df['Match'].sum()\n",
    "true_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_cPnu913Y0x"
   },
   "source": [
    "## 6. Filtering for a High-Quality Evaluation Set\n",
    "\n",
    "Since our dataset is designed for multi-hop queries, a complete evaluation example requires successful matches for *both* steps of a given question (`Step_Number` 1 and 2).\n",
    "\n",
    "In this final step, we filter the DataFrame to keep only the `QA_ID`s for which we found a valid chunk match for both evidence strings. This produces a high-quality, reliable dataset that can be used for downstream retrieval and RAG evaluation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JnEAl2G3Y0x"
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter for step 1 or 2 with Match == True\n",
    "filtered = df[df['Step_Number'].isin([1, 2]) & (df['Match'] == True)]\n",
    "\n",
    "# Step 2: Find QA_IDs with both step 1 and step 2 matched\n",
    "step_counts = filtered.groupby('QA_ID')['Step_Number'].nunique()\n",
    "qa_ids_with_both_matched = step_counts[step_counts == 2].index.tolist()\n",
    "\n",
    "# Step 3: Filter the original DataFrame for those QA_IDs and steps 1 or 2\n",
    "result_df = df[df['QA_ID'].isin(qa_ids_with_both_matched) & df['Step_Number'].isin([1, 2])]\n",
    "\n",
    "# Optional: sort for easier viewing\n",
    "result_df = result_df.sort_values(['QA_ID', 'Step_Number'])\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfCDDjXr3Y0y"
   },
   "outputs": [],
   "source": [
    "#result_df.to_csv('matched_retrievals.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
