{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2nmrhpCdxmZ"
   },
   "source": [
    "# Evaluating Retrieval Quality: Evidence-to-Chunk Matching for RAG Systems\n",
    "\n",
    "## ðŸŽ¯ What This Notebook Does\n",
    "\n",
    "This notebook solves a critical challenge in RAG evaluation: **automatically determining whether retrieved chunks contain the ground-truth evidence needed to answer questions correctly.**\n",
    "\n",
    "Instead of manually checking hundreds of retrieved chunks, this system:\n",
    "- âœ… Automatically matches evidence strings to retrieved chunks using fuzzy matching + numeric overlap\n",
    "- âœ… Filters multi-hop questions to keep only those with complete evidence coverage\n",
    "- âœ… Creates a high-quality evaluation dataset for measuring retrieval performance\n",
    "- âœ… Outputs ready-to-use data for computing precision, recall, and other retrieval metrics\n",
    "\n",
    "## ðŸ—ï¸ The Problem This Solves\n",
    "\n",
    "When evaluating RAG systems, you need to know:\n",
    "1. **Did the retrieval system find the right information?** (Recall)\n",
    "2. **How much irrelevant information was retrieved?** (Precision)\n",
    "3. **Can the system handle complex, multi-step reasoning?** (Multi-hop evaluation)\n",
    "\n",
    "Manual evaluation doesn't scale. This notebook automates the process.\n",
    "\n",
    "## ðŸ“Š Input & Output\n",
    "\n",
    "**Input:**\n",
    "- QA dataset with annotated evidence strings\n",
    "\n",
    "**Output:**\n",
    "- Matched evidence pairs (evidence string â†” chunk ID)\n",
    "\n",
    "## ðŸ”§ Prerequisites\n",
    "- Contextual AI API access (or adapt the retrieval functions for your system)\n",
    "- QA dataset (like the one from the generation notebook)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/11-retrieval-analysis/Retrieval_Matching.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cteal1Ndxmb"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install and import the necessary libraries. This notebook relies on `pandas` for data manipulation, `thefuzz` for fuzzy string matching, and the `contextual` client for interacting with the datastore and retrieval APIs. We also import standard libraries for handling data and making API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuNPF3po3Y0t"
   },
   "outputs": [],
   "source": [
    "# ! pip install thefuzz[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzZYbsQjdxmb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import NonLLMContextRecall, NonLLMContextPrecisionWithReference\n",
    "\n",
    "\n",
    "import requests\n",
    "from thefuzz import fuzz                    # â† matches install above\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Any, Optional\n",
    "from contextual import ContextualAI\n",
    "client = ContextualAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7NYzxar3Y0t"
   },
   "source": [
    "## 2. Prepare Datastore with Metadata for Filtering\n",
    "\n",
    "To ensure that we only search for matches within the correct source document, it's crucial to have reliable metadata. Here, we'll add a `Filename` field to the custom metadata of each document in our datastore.\n",
    "\n",
    "This allows us to use a `documents_filters` parameter in our retrieval requests, which dramatically narrows down the search space and prevents incorrect matches from other documents. This is a best practice for building robust retrieval evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmFlbl5x3Y0u"
   },
   "outputs": [],
   "source": [
    "datastore_id = 'ecd873f1-134a-4c43-8d69-bcd32533fd67'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TptvvKiY3Y0u"
   },
   "outputs": [],
   "source": [
    "docs = client.datastores.documents.list(datastore_id=datastore_id)\n",
    "doc_pairs = [(doc.id, doc.name) for doc in docs.documents]\n",
    "print(\"Document ID and Name pairs:\")\n",
    "for doc_id, name in doc_pairs:\n",
    "    print(f\"ID: {doc_id}, Name: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxOqFzqQ3Y0u"
   },
   "outputs": [],
   "source": [
    "# Loop through doc_pairs and set the Filename metadata for each document\n",
    "for doc_id, name in doc_pairs:\n",
    "    result = client.datastores.documents.set_metadata(\n",
    "        datastore_id=datastore_id,\n",
    "        document_id=doc_id,\n",
    "        custom_metadata={\"Filename\": name}\n",
    "    )\n",
    "    print(f\"Set metadata for {name} (ID: {doc_id}): {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVPZyJeF3Y0v"
   },
   "outputs": [],
   "source": [
    "#Verify\n",
    "document_id = docs.documents[0].id\n",
    "metadata = client.datastores.documents.metadata(datastore_id = datastore_id,\n",
    "                        document_id = document_id)\n",
    "print(\"Document metadata:\", metadata.custom_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wePzfZEc3Y0v"
   },
   "source": [
    "## 3. Load and Preprocess Annotated Data\n",
    "\n",
    "Next, we load the annotated dataset, which contains the ground-truth information for our evaluation. This dataset is structured as an Excel file where each row corresponds to a piece of evidence required to answer a multi-hop question.\n",
    "\n",
    "The key columns for our purposes are:\n",
    "- `Question`: The user query.\n",
    "- `Source_Document`: The filename of the document containing the evidence.\n",
    "- `Evidence`: The ground-truth text string that must be found in a retrieved chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt4DU4Ph3Y0v"
   },
   "outputs": [],
   "source": [
    "XLSX_PATH = \"qa_pairs_multi_row_20250616_174936.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHDYjX-B3Y0v"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(XLSX_PATH)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Buy80iYr3Y0w"
   },
   "source": [
    "### Data Cleaning: Forward-Filling Questions\n",
    "\n",
    "Our dataset is designed for multi-hop questions, where a single complex question requires multiple steps (and evidence strings) to answer. In the raw data, the `Question` and `Answer` are only present on the first row for a given `QA_ID`.\n",
    "\n",
    "To create a clean, flat structure where every row has a question, we'll use `ffill()` (forward-fill) to propagate the question and answer to all subsequent rows belonging to the same `QA_ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeuE3pL33Y0w"
   },
   "outputs": [],
   "source": [
    "df['Question'] = df['Question'].fillna(method='ffill')\n",
    "df['Answer'] = df['Answer'].fillna(method='ffill')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtnAaR_B3Y0w"
   },
   "source": [
    "## 4. Evidence-to-Chunk Matching Algorithm\n",
    "\n",
    "This section contains the core logic for matching evidence strings to retrieved chunks. A reliable matching function is essential for automated retrieval evaluation, as it allows us to programmatically verify if the retrieved content contains the necessary information.\n",
    "\n",
    "Our approach uses a hybrid scoring method:\n",
    "\n",
    "1.  **Fuzzy Matching**: Uses `thefuzz.token_set_ratio` to compare the textual similarity between the evidence and a chunk, ignoring word order.\n",
    "2.  **Numeric Overlap**: Extracts and compares numeric tokens (e.g., \"$1.2M\", \"50%\") separately. This is crucial for financial or data-heavy documents where numbers are key identifiers.\n",
    "\n",
    "The final score is a weighted average of the fuzzy and numeric scores. A chunk is considered a match if its score exceeds a predefined threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7S8qR7e3Y0w"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "API_KEY = os.environ[\"CONTEXTUAL_API_KEY\"]\n",
    "\n",
    "## Agent Configs\n",
    "AGENT_ID = '15543690-68fd-49e7-8fc9-1f53c8e42e33'\n",
    "\n",
    "## configs for retrieval\n",
    "TOP_RETRIEVED_DOCS = 150\n",
    "RETRIEVAL_ALPHA = 0.9\n",
    "\n",
    "# Configs for evidence string Matching\n",
    "# recommended to enable only when working with numerical heavy datasets like financial tables.\n",
    "ENABLE_PREPROCESSING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_LmRIAk3Y0w"
   },
   "source": [
    "### Custom Retrieval Logic\n",
    "\n",
    "To maximize our chances of finding a match, we need to configure the retrieval system to return a broad set of candidate chunks. During the initial matching phase, it's often beneficial to cast a wider net than you would in a production application.\n",
    "\n",
    "This retrieval function is configured to:\n",
    "\n",
    "- Increase the number of retrieved documents (`top_k`).\n",
    "- Bypass the reranker and filter models, which might otherwise remove relevant but lower-scored chunks.\n",
    "- Adjust the balance between lexical and semantic search to favor more direct term matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIlm0yAk3Y0w"
   },
   "outputs": [],
   "source": [
    "## Test Query to make sure the retrieval is working\n",
    "query = \"What is the total revenue for Tesla in 2024?\"\n",
    "source_document = \"Tesla_2024_Annual_Report.pdf\"\n",
    "\n",
    "url = f\"https://api.app.contextual.ai/v1/applications/{AGENT_ID}/query?retrievals_only=true\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "payload: dict[str, Any] = {\n",
    "    \"stream\": False,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    \"documents_filters\": {\n",
    "        \"operator\": \"AND\",\n",
    "        \"filters\": [\n",
    "            {\"field\": \"Filename\", \"operator\": \"equals\", \"value\": source_document}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# network I/O â€“ may raise Timeout / HTTPError â†’ caught by tenacity\n",
    "resp = requests.post(url, headers=headers,\n",
    "                        data=json.dumps(payload))\n",
    "resp.raise_for_status()\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDeT6WOT3Y0w"
   },
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))\n",
    "def retrieve_documents(\n",
    "    query: str,\n",
    "    source_document: str,\n",
    "    *,\n",
    "    alpha: float = RETRIEVAL_ALPHA,  # Uses the value from your config cell\n",
    "    top_k: int = TOP_RETRIEVED_DOCS,    # Uses the value from your config cell\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A single, simplified function to retrieve documents that combines all necessary logic.\n",
    "    \"\"\"\n",
    "    # Step 1: Build the specific override config object. This is not working right now\n",
    "    override_cfg = {\n",
    "        \"filter_retrievals\": False,\n",
    "        \"rerank_retrievals\": False,\n",
    "        \"lexical_alpha\": 1.0 - alpha,\n",
    "        \"semantic_alpha\": alpha,\n",
    "        \"top_k\": top_k,\n",
    "    }\n",
    "\n",
    "    # Step 2: Build the full payload for the API request.\n",
    "    url = f\"https://api.app.contextual.ai/v1/applications/{AGENT_ID}/query?retrievals_only=true\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload: Dict[str, Any] = {\n",
    "        \"stream\": False,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "        \"documents_filters\": {\n",
    "            \"operator\": \"AND\",\n",
    "            \"filters\": [\n",
    "                {\"field\": \"Filename\", \"operator\": \"equals\", \"value\": source_document}\n",
    "            ]\n",
    "        },\n",
    "       # \"override_retrieval_config\": override_cfg\n",
    "    }\n",
    "    # Step 3: Make the request and return the desired content.\n",
    "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    # The API returns a list of dicts in the \"retrieval_contents\" key\n",
    "    return resp.json().get(\"retrieval_contents\", [])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhGyNIDJ3Y0w"
   },
   "outputs": [],
   "source": [
    "def _preprocess_text(text: str) -> str:\n",
    "    \"\"\"Lightweight text normalization to improve fuzzy matching robustness.\n",
    "\n",
    "    Steps:\n",
    "    1. Lowerâ€case everything.\n",
    "    2. Strip table/markdown characters (``|``), repeated dashes and newlines.\n",
    "    3. Remove formatting punctuation such as ``$``, ``%``, ``(``, ``)`` and ``+``.\n",
    "    4. Remove commas that only serve as thousands separators inside numbers (e.g. ``2,000`` â†’ ``2000``).\n",
    "    5. Replace any remaining nonâ€alphanumeric characters with a single space and collapse runs of whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"|\", \" \")\n",
    "    text = re.sub(r\"-+\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = re.sub(r\"[\\(\\)\\$\\%\\+]\", \"\", text)\n",
    "    # Remove commas between digits (e.g. 1,234 -> 1234)\n",
    "    text = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", text)\n",
    "    # Remove any other non alphanum / period characters\n",
    "    text = re.sub(r\"[^a-z0-9\\. ]\", \" \", text)\n",
    "    # Collapse redundant whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Regex for capturing numbers that appear in evidence / chunks.\n",
    "_NUMBER_PATTERN = (\n",
    "    r\"[-+]?\\$?\\s*(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?%?|\"  # standard numbers / currency / percentages\n",
    "    r\"\\([^)]*\\d+[^)]*\\)\"  # numbers enclosed in parentheses\n",
    ")\n",
    "\n",
    "\n",
    "def _extract_numbers(text: str) -> List[str]:\n",
    "    \"\"\"Return a list of the numeric substrings found in *text*.\"\"\"\n",
    "    return [m.strip() for m in re.findall(_NUMBER_PATTERN, text)]\n",
    "\n",
    "\n",
    "def matcher_with_metadata(\n",
    "    evidence_str: str,\n",
    "    retrieved_chunks: List[Dict[str, Any]],\n",
    "    threshold: float = 0.85,\n",
    "    *,\n",
    "    alpha: float = 1.0,\n",
    "    preprocess_text: bool = ENABLE_PREPROCESSING,\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Return the retrieved chunk (with metadata) that best matches evidence_str.\n",
    "\n",
    "    This function finds the best matching chunk from a list of chunks containing\n",
    "    both content text and metadata. The scoring mirrors the non-LLM portion of\n",
    "    ``EvidenceMatchFuzzyLLM``:\n",
    "\n",
    "    â€¢ The *fuzzy* component uses `token_set_ratio` between preprocessed strings\n",
    "    â€¢ The *numeric* component measures overlap of extracted numeric tokens\n",
    "    â€¢ The final score is ``alpha * fuzzy + (1-alpha) * numeric`` when evidence\n",
    "      contains numbers, otherwise just the fuzzy score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evidence_str : str\n",
    "        The target evidence string to match against.\n",
    "    retrieved_chunks : List[Dict[str, Any]]\n",
    "        List of chunk dictionaries, each containing a \"content_text\" key with\n",
    "        the text content to match against, plus any additional metadata.\n",
    "    threshold : float, default 0.85\n",
    "        Minimum score required to return a match. Chunks scoring below this\n",
    "        threshold will result in None being returned.\n",
    "    alpha : float, default 1.0\n",
    "        Weight assigned to the fuzzy score when both fuzzy and numeric scores\n",
    "        are available. For chunks heavy with financial tables, alpha = 0.8\n",
    "        is recommended to give more weight to numeric matching.\n",
    "    preprocess_text : bool, default ENABLE_PREPROCESSING\n",
    "        Whether to run light normalization before matching. Preprocessing is\n",
    "        helpful for chunks where numbers matter significantly, such as\n",
    "        financial tables.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optional[Dict[str, Any]]\n",
    "        The complete chunk dictionary from retrieved_chunks with the highest\n",
    "        score, or None if no chunk meets the threshold or if input is empty.\n",
    "        The returned dictionary includes both the matched content and any\n",
    "        associated metadata.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> chunks = [\n",
    "    ...     {\"content_text\": \"Revenue was $1.2M in Q1\", \"source\": \"report.pdf\"},\n",
    "    ...     {\"content_text\": \"Expenses totaled $800K\", \"source\": \"budget.xlsx\"}\n",
    "    ... ]\n",
    "    >>> result = matcher_with_metadata(\"Revenue $1.2M\", chunks)\n",
    "    >>> print(result[\"source\"])  # \"report.pdf\"\n",
    "    \"\"\"\n",
    "    if not evidence_str or not retrieved_chunks:\n",
    "        return None\n",
    "\n",
    "    norm_evidence = _preprocess_text(evidence_str) if preprocess_text else evidence_str\n",
    "    evidence_nums = _extract_numbers(norm_evidence)\n",
    "\n",
    "    best_chunk = None\n",
    "    best_score = -1.0\n",
    "\n",
    "    for chunk in retrieved_chunks:\n",
    "        chunk_text = chunk[\"content_text\"]\n",
    "        norm_chunk = _preprocess_text(chunk_text) if preprocess_text else chunk_text\n",
    "\n",
    "        # Short-circuit if the entire evidence string is a substring\n",
    "        if norm_evidence in norm_chunk:\n",
    "            return chunk  # Return the whole dict\n",
    "\n",
    "        fuzzy_score = fuzz.token_set_ratio(norm_evidence, norm_chunk) / 100.0\n",
    "        chunk_nums = _extract_numbers(norm_chunk)\n",
    "        numeric_matches = sum(1 for n in evidence_nums if n in chunk_nums)\n",
    "        numeric_score = numeric_matches / len(evidence_nums) if evidence_nums else 0.0\n",
    "\n",
    "        final_score = alpha * fuzzy_score + (1 - alpha) * numeric_score if evidence_nums else fuzzy_score\n",
    "\n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            best_chunk = chunk\n",
    "\n",
    "    return best_chunk if best_score >= threshold else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTaY0BI83Y0x"
   },
   "source": [
    "### Unit Test: Matching a Single Row\n",
    "\n",
    "Before processing the entire dataset, it's good practice to test our `retrieve_documents` and `matcher_with_metadata` functions on a single example. This helps verify that the API calls are working correctly, the data is being processed as expected, and the matching logic is sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUcDWy2N3Y0x"
   },
   "outputs": [],
   "source": [
    "# Get the first row (make sure it's not a NaN row)\n",
    "row = df.iloc[1]\n",
    "\n",
    "# Extract the query and evidence string\n",
    "query = row['Question']\n",
    "evidence_str = row['Evidence']\n",
    "source_document = row['Source_Document']\n",
    "\n",
    "print(query)\n",
    "print(evidence_str)\n",
    "\n",
    "# Retrieve full response and get the list of dicts\n",
    "retrieved_chunks= retrieve_documents(query=query,source_document=source_document)\n",
    "\n",
    "# Run the matcher\n",
    "match = matcher_with_metadata(evidence_str, retrieved_chunks)\n",
    "\n",
    "# Add results to DataFrame\n",
    "df.at[row.name, 'Match'] = bool(match)\n",
    "df.at[row.name, 'Content_Id'] = match['content_id'] if match else None\n",
    "\n",
    "# Print the result\n",
    "if match:\n",
    "    print(\"Match found!\")\n",
    "    #print(\"Matched content_id:\", match[\"content_id\"])\n",
    "   # print(\"Matched chunk text:\\n\", match[\"content_text\"])\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYvXLaSC3Y0x"
   },
   "source": [
    "## 5. Batch Processing: Matching the Entire Dataset\n",
    "\n",
    "Now that we've validated the process on a single row, we'll apply it to every row in the DataFrame. We iterate through the dataset, retrieve candidate chunks for each evidence string, and run the matcher.\n",
    "\n",
    "The resultsâ€”a boolean `Match` status and the `Content_Id` of the matched chunkâ€”are stored in new columns in the DataFrame. This process can take some time, as it involves making an API call for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZlAPDDZ3Y0x"
   },
   "outputs": [],
   "source": [
    "# Initialize columns\n",
    "df['Match'] = False\n",
    "df['Content_Id'] = None\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    query = row['Question']\n",
    "    evidence_str = row['Evidence']\n",
    "    source_document = row['Source_Document']\n",
    "    retrieved_chunks= retrieve_documents(query=query,source_document=source_document)\n",
    "    match = matcher_with_metadata(evidence_str, retrieved_chunks)\n",
    "    df.at[idx, 'Match'] = bool(match)\n",
    "    df.at[idx, 'Content_Id'] = match['content_id'] if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38Xja6ca3Y0x"
   },
   "source": [
    "### Analysis: Reviewing Match Results\n",
    "\n",
    "After running the batch process, let's inspect the results. We can check the `tail` of the DataFrame and count the total number of successful matches to get a sense of the match rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6XYfaej3Y0x"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZE4p6Vr3Y0x"
   },
   "source": [
    "Not all the annotated evidence was correctly matched. For now, we will continue with the rows with matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J94YhqSv3Y0x"
   },
   "outputs": [],
   "source": [
    "true_count = df['Match'].sum()\n",
    "true_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_cPnu913Y0x"
   },
   "source": [
    "## 6. Filtering for a High-Quality Evaluation Set\n",
    "\n",
    "Since our dataset is designed for multi-hop queries, a complete evaluation example requires successful matches for *both* steps of a given question (`Step_Number` 1 and 2).\n",
    "\n",
    "In this final step, we filter the DataFrame to keep only the `QA_ID`s for which we found a valid chunk match for both evidence strings. This produces a high-quality, reliable dataset that can be used for downstream retrieval and RAG evaluation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JnEAl2G3Y0x"
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter for step 1 or 2 with Match == True\n",
    "filtered = df[df['Step_Number'].isin([1, 2]) & (df['Match'] == True)]\n",
    "\n",
    "# Step 2: Find QA_IDs with both step 1 and step 2 matched\n",
    "step_counts = filtered.groupby('QA_ID')['Step_Number'].nunique()\n",
    "qa_ids_with_both_matched = step_counts[step_counts == 2].index.tolist()\n",
    "\n",
    "# Step 3: Filter the original DataFrame for those QA_IDs and steps 1 or 2\n",
    "result_df = df[df['QA_ID'].isin(qa_ids_with_both_matched) & df['Step_Number'].isin([1, 2])]\n",
    "\n",
    "# Optional: sort for easier viewing\n",
    "result_df = result_df.sort_values(['QA_ID', 'Step_Number'])\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfCDDjXr3Y0y"
   },
   "outputs": [],
   "source": [
    "#result_df.to_csv('matched_retrievals.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
