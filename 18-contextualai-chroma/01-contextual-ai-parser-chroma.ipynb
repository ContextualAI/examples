{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJMV1PUvXA4e"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/18-contextualai-chroma/01-contextual-ai-parser-chroma.ipynb)\n",
        "\n",
        "# Build Multi-Modal RAG with Chroma and Contextual AI Parser\n",
        "\n",
        "**Last updated:** November 2025\n",
        "\n",
        "**Versions used:**\n",
        "- Chroma version `1.3.4`\n",
        "- Contextual AI client `0.9.0`\n",
        "- OpenAI API (for embeddings and generation)\n",
        "\n",
        "This is a code recipe that uses [Chroma](https://docs.trychroma.com/) to perform multi-modal RAG over documents parsed by [Contextual AI Parser](https://docs.contextual.ai/api-reference/parse/parse-file).\n",
        "\n",
        "In this notebook, we accomplish the following:\n",
        "* Parse two distinct document types using Contextual AI Parser: research papers and table-rich documents\n",
        "* Extract structured markdown with document hierarchy preservation and advanced table extraction\n",
        "* Generate text embeddings with OpenAI\n",
        "* Perform multi-modal RAG using [Chroma](https://docs.trychroma.com/)\n",
        "\n",
        "To run this notebook, you'll need:\n",
        "* A [Contextual AI API key](https://docs.contextual.ai/user-guides/beginner-guide) - for document parsing and content extraction.\n",
        "Visit [app.contextual.ai](https://app.contextual.ai/?utm_campaign=chroma&utm_source=contextualai&utm_medium=github&utm_content=notebook) and click the **\"Start Free\"** button to sign up and receive free credits\n",
        "* An [OpenAI API key](https://platform.openai.com/docs/quickstart) - for text embeddings and generative responses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2vgMYz0XA4i"
      },
      "source": [
        "### Install Contextual AI client and Chroma\n",
        "\n",
        "Note: If Colab prompts you to restart the session after running the cell below, click \"restart\" and proceed with running the rest of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mWtiuzumXA4i"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install --upgrade chromadb contextual-client openai requests rich nest-asyncio\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "# Suppress Chroma client logs\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i51fr8iWXA4j"
      },
      "source": [
        "## ðŸ” Part 1: Contextual AI Parser\n",
        "\n",
        "Contextual AI Parser is a cloud-based document parsing service that excels at extracting structured information from PDFs, DOC/DOCX, and PPT/PPTX files. It provides high-quality markdown extraction with document hierarchy preservation, making it ideal for RAG applications. See our [blog post on document parsing for RAG](https://contextual.ai/blog/document-parser-for-rag) for more details on parse quality and capabilities.\n",
        "\n",
        "The parser handles complex documents with images, tables, and hierarchical structures, providing multiple output formats including:\n",
        "- `markdown-document`: Single concatenated markdown output\n",
        "- `markdown-per-page`: Page-by-page markdown output\n",
        "- `blocks-per-page`: Structured JSON with document hierarchy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AvMUhmNaXA4k"
      },
      "outputs": [],
      "source": [
        "# Documents to parse with Contextual AI\n",
        "documents = [\n",
        "    {\n",
        "        \"url\": \"https://arxiv.org/pdf/1706.03762\",\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"type\": \"research_paper\",\n",
        "        \"description\": \"Seminal transformer architecture paper that introduced self-attention mechanisms\"\n",
        "    },\n",
        "    {\n",
        "        \"url\": \"https://raw.githubusercontent.com/ContextualAI/examples/refs/heads/main/03-standalone-api/04-parse/data/omnidocbench-text.pdf\",\n",
        "        \"title\": \"OmniDocBench Dataset Documentation\",\n",
        "        \"type\": \"table_rich_document\",\n",
        "        \"description\": \"Dataset documentation with large tables demonstrating table extraction capabilities\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fPuzumXA4k"
      },
      "source": [
        "### API Keys Setup ðŸ”‘\n",
        "\n",
        "We'll be using the Contextual AI API for parsing documents and OpenAI API for both generating text embeddings and for the generative model in our RAG pipeline. The code below dynamically fetches your API keys based on whether you're running this notebook in Google Colab or as a regular Jupyter notebook.\n",
        "\n",
        "If you're running this notebook in Google Colab, make sure you [add](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75) your API keys as secrets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m1GBGmQXA4l",
        "outputId": "33e681d1-8593-4221-95b2-e96fb38ac826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API keys configured successfully!\n"
          ]
        }
      ],
      "source": [
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"  # Replace with the name of your secret/env var\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"  # Replace with the name of your secret/env var\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "\n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    import os\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "\n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "print(\"API keys configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaGbsh8iXA4l"
      },
      "source": [
        "### Download and parse PDFs using Contextual AI Parser\n",
        "\n",
        "Here we use Contextual AI's Python SDK to parse a batch of PDFs. The result is structured markdown content with document hierarchy that we can use for text extraction and chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SKfMSaGXA4l",
        "outputId": "421557ea-9f3e-44a9-e295-c11244a28fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and submitting parse job for: Attention Is All You Need\n",
            "Type: research_paper - Seminal transformer architecture paper that introduced self-attention mechanisms\n",
            "Submitted job 8154bf67-0f20-4247-8c4a-26f2be9de88d for Attention Is All You Need\n",
            "Downloading and submitting parse job for: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document - Dataset documentation with large tables demonstrating table extraction capabilities\n",
            "Submitted job 8f0b9555-062b-4040-ada1-2aae9733e477 for OmniDocBench Dataset Documentation\n",
            "\n",
            "Submitted 2 parse jobs\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from contextual import ContextualAI\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Setup Contextual AI client\n",
        "client = ContextualAI(api_key=contextual_api_key)\n",
        "\n",
        "# Create directory for downloaded PDFs\n",
        "os.makedirs(\"pdfs\", exist_ok=True)\n",
        "\n",
        "# Download PDFs and submit parse jobs\n",
        "job_data = []\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Downloading and submitting parse job for: {doc['title']}\")\n",
        "    print(f\"Type: {doc['type']} - {doc['description']}\")\n",
        "\n",
        "    # Download PDF\n",
        "    file_path = f\"pdfs/{doc['type']}_{i}.pdf\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(requests.get(doc['url']).content)\n",
        "\n",
        "    # Configure parsing parameters based on document type\n",
        "    if doc['type'] == \"research_paper\":\n",
        "        # For research papers, focus on hierarchy and figures\n",
        "        parse_config = {\n",
        "            \"parse_mode\": \"standard\",\n",
        "            \"figure_caption_mode\": \"concise\",\n",
        "            \"enable_document_hierarchy\": True,\n",
        "            \"page_range\": \"0-5\"  # Parse first 6 pages\n",
        "        }\n",
        "    else:\n",
        "        # For table-rich documents, enable table splitting\n",
        "        parse_config = {\n",
        "            \"parse_mode\": \"standard\",\n",
        "            \"enable_split_tables\": True,\n",
        "            \"max_split_table_cells\": 100,\n",
        "        }\n",
        "\n",
        "    # Submit parse job\n",
        "    with open(file_path, \"rb\") as fp:\n",
        "        response = client.parse.create(\n",
        "            raw_file=fp,\n",
        "            **parse_config\n",
        "        )\n",
        "\n",
        "    job_data.append({\n",
        "        \"job_id\": response.job_id,\n",
        "        \"file_path\": file_path,\n",
        "        \"document\": doc\n",
        "    })\n",
        "    print(f\"Submitted job {response.job_id} for {doc['title']}\")\n",
        "\n",
        "print(f\"\\nSubmitted {len(job_data)} parse jobs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCJc8g0sXA4m"
      },
      "source": [
        "### Monitor parse job status and retrieve results\n",
        "\n",
        "We'll monitor all parse jobs and retrieve the results once they're completed. Contextual AI provides structured markdown with document hierarchy information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFPaAl17XA4m",
        "outputId": "01997775-68dc-43b1-9cb8-e4c86c6885b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job 1/2 (Attention Is All You Need - research_paper): processing\n",
            "Job 2/2 (OmniDocBench Dataset Documentation - table_rich_document): processing\n",
            "\n",
            "Waiting for remaining jobs to complete...\n",
            "Job 1/2 (Attention Is All You Need - research_paper): processing\n",
            "Job 2/2 (OmniDocBench Dataset Documentation - table_rich_document): completed\n",
            "\n",
            "Waiting for remaining jobs to complete...\n",
            "Job 1/2 (Attention Is All You Need - research_paper): completed\n",
            "\n",
            "All parse jobs completed!\n"
          ]
        }
      ],
      "source": [
        "# Monitor all parse jobs using asyncio\n",
        "async def wait_for_jobs_async(job_data, max_attempts=20, interval=30.0):\n",
        "    \"\"\"Asynchronously poll until all jobs are ready, exiting early if possible.\"\"\"\n",
        "    completed_jobs = set()\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        if len(completed_jobs) >= len(job_data):\n",
        "            return completed_jobs\n",
        "\n",
        "        for i, job_info in enumerate(job_data):\n",
        "            job_id = job_info[\"job_id\"]\n",
        "            if job_id not in completed_jobs:\n",
        "                # Run blocking call in thread pool\n",
        "                status = await asyncio.to_thread(client.parse.job_status, job_id)\n",
        "                doc_title = job_info[\"document\"][\"title\"]\n",
        "                doc_type = job_info[\"document\"][\"type\"]\n",
        "                print(f\"Job {i+1}/{len(job_data)} ({doc_title} - {doc_type}): {status.status}\")\n",
        "\n",
        "                if status.status == \"completed\":\n",
        "                    completed_jobs.add(job_id)\n",
        "                elif status.status == \"failed\":\n",
        "                    print(f\"Job failed for {doc_title}\")\n",
        "                    completed_jobs.add(job_id)  # Add to completed to avoid infinite loop\n",
        "\n",
        "        if len(completed_jobs) < len(job_data):\n",
        "            print(\"\\nWaiting for remaining jobs to complete...\")\n",
        "            await asyncio.sleep(interval)\n",
        "\n",
        "    return completed_jobs  # return the set of completed jobs\n",
        "\n",
        "# Run the async monitoring function\n",
        "# Apply nest_asyncio to allow nested event loops (needed for Jupyter notebooks)\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "completed_jobs = asyncio.run(wait_for_jobs_async(job_data))\n",
        "\n",
        "print(\"\\nAll parse jobs completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ9L0JIDXA4m"
      },
      "source": [
        "## ðŸ’š Part 2: Chroma\n",
        "### Create and configure a Chroma collection\n",
        "\n",
        "[Chroma](https://docs.trychroma.com/) is an open-source embedding database that makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs. It provides efficient vector storage and similarity search capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhO3rNO3XA4m",
        "outputId": "6f991d75-0353-452d-e3b7-0b4d50443048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created collection 'contextual_ai_rag_collection' with OpenAI embeddings\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Initialize Chroma client\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Use OpenAI embeddings\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=openai_api_key,\n",
        "    model_name=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "collection_name = \"contextual_ai_rag_collection\"\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=collection_name,\n",
        "    embedding_function=openai_ef\n",
        ")\n",
        "\n",
        "print(f\"Created collection '{collection_name}' with OpenAI embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcwW29hRXA4n"
      },
      "source": [
        "### Retrieve and process parsed content\n",
        "\n",
        "We'll retrieve the parsed results and process them into chunks suitable for vector search. Contextual AI provides excellent document structure preservation, which we'll leverage for better RAG performance.\n",
        "\n",
        "**Key Feature**: Contextual AI preserves document hierarchy through `parent_ids`, allowing us to maintain section relationships and provide richer context to our RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k2pdpthXA4n",
        "outputId": "9d7b72b9-7467-45b6-e3ad-ff9bec5780c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Attention Is All You Need (research_paper)\n",
            "  - 6 pages parsed\n",
            "Processing OmniDocBench Dataset Documentation (table_rich_document)\n",
            "  - 1 pages parsed\n",
            "\n",
            "Processed 71 chunks from 2 documents\n",
            "Document types: table_rich_document, research_paper\n"
          ]
        }
      ],
      "source": [
        "# Retrieve results and process into chunks\n",
        "texts, titles, sources, doc_types, block_types, hierarchy_levels, confidence_levels = [], [], [], [], [], [], []\n",
        "hierarchy_data = []\n",
        "\n",
        "for job_info in job_data:\n",
        "    job_id = job_info[\"job_id\"]\n",
        "    document = job_info[\"document\"]\n",
        "\n",
        "    if job_id in completed_jobs:\n",
        "        try:\n",
        "            print(f\"Processing {document['title']} ({document['type']})\")\n",
        "\n",
        "            # Get results with blocks-per-page for hierarchical information\n",
        "            results = client.parse.job_results(\n",
        "                job_id,\n",
        "                output_types=['blocks-per-page']\n",
        "            )\n",
        "\n",
        "            # Store hierarchy if available\n",
        "            if hasattr(results, 'document_metadata') and results.document_metadata and hasattr(results.document_metadata, 'hierarchy'):\n",
        "                hierarchy_data.append({\n",
        "                    'title': document['title'],\n",
        "                    'hierarchy': results.document_metadata.hierarchy\n",
        "                })\n",
        "\n",
        "            print(f\"  - {len(results.pages)} pages parsed\")\n",
        "\n",
        "            # Create hash table for parent content lookup\n",
        "            hash_table = {}\n",
        "            for page in results.pages:\n",
        "                for block in page.blocks:\n",
        "                    hash_table[block.id] = block.markdown\n",
        "\n",
        "            # Process blocks with hierarchy context\n",
        "            for page in results.pages:\n",
        "                for block in page.blocks:\n",
        "                    # Filter blocks based on document type and content quality\n",
        "                    if (block.type in ['text', 'heading', 'table'] and\n",
        "                        len(block.markdown.strip()) > 30):\n",
        "\n",
        "                        # Add hierarchy context if available\n",
        "                        context_text = block.markdown\n",
        "\n",
        "                        if hasattr(block, 'parent_ids') and block.parent_ids:\n",
        "                            parent_content = \"\\n\".join([\n",
        "                                hash_table.get(parent_id, \"\")\n",
        "                                for parent_id in block.parent_ids\n",
        "                            ])\n",
        "                            if parent_content.strip():\n",
        "                                context_text = f\"{parent_content}\\n\\n{block.markdown}\"\n",
        "\n",
        "                        # Add document metadata as context\n",
        "                        full_text = f\"Document: {document['title']}\\nType: {document['type']}\\n\\n{context_text}\"\n",
        "\n",
        "                        texts.append(full_text)\n",
        "                        titles.append(document['title'])\n",
        "                        sources.append(f\"Page {page.index + 1}\")\n",
        "                        doc_types.append(document['type'])\n",
        "                        block_types.append(block.type)\n",
        "                        # Extract hierarchy_level if available (for documents with hierarchy enabled)\n",
        "                        hierarchy_levels.append(getattr(block, 'hierarchy_level', None))\n",
        "                        # Extract confidence_level if available (for table blocks)\n",
        "                        confidence_levels.append(getattr(block, 'confidence_level', None))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {document['title']}: {e}\")\n",
        "\n",
        "print(f\"\\nProcessed {len(texts)} chunks from {len(set(titles))} documents\")\n",
        "print(f\"Document types: {', '.join(set(doc_types))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Lq6m_vSdYtgn",
        "outputId": "f28f66ef-1768-4bc7-d75c-1fee97594dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Document Hierarchy: Attention Is All You Need\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "# Document Hierarchy\n",
              "\n",
              "- Attention Is All You Need [(Page 0)](#attention-is-all-you-need)\n",
              "  - Abstract [(Page 0)](#abstract)\n",
              "  - 1 Introduction [(Page 1)](#1-introduction)\n",
              "  - 2 Background [(Page 1)](#2-background)\n",
              "  - 3 Model Architecture [(Page 1)](#3-model-architecture)\n",
              "    - 3.1 Encoder and Decoder Stacks [(Page 2)](#31-encoder-and-decoder-stacks)\n",
              "    - 3.2 Attention [(Page 2)](#32-attention)\n",
              "      - 3.2.1 Scaled Dot-Product Attention [(Page 3)](#321-scaled-dot-product-attention)\n",
              "      - 3.2.2 Multi-Head Attention [(Page 3)](#322-multi-head-attention)\n",
              "      - 3.2.3 Applications of Attention in our Model [(Page 4)](#323-applications-of-attention-in-our-model)\n",
              "    - 3.3 Position-wise Feed-Forward Networks [(Page 4)](#33-position-wise-feed-forward-networks)\n",
              "    - 3.4 Embeddings and Softmax [(Page 4)](#34-embeddings-and-softmax)\n",
              "    - 3.5 Positional Encoding [(Page 5)](#35-positional-encoding)\n",
              "  - 4 Why Self-Attention [(Page 5)](#4-why-self-attention)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display extracted document hierarchy\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "if hierarchy_data:\n",
        "    for item in hierarchy_data:\n",
        "        # Only display if table_of_contents exists and has meaningful content (more than just a header)\n",
        "        if (item['hierarchy'] and\n",
        "            hasattr(item['hierarchy'], 'table_of_contents') and\n",
        "            item['hierarchy'].table_of_contents and\n",
        "            len(item['hierarchy'].table_of_contents.strip()) > 50):  # Check for meaningful content\n",
        "            print(f\"\\n### Document Hierarchy: {item['title']}\")\n",
        "            display(Markdown(item['hierarchy'].table_of_contents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUmNuMZRXA4n"
      },
      "source": [
        "### Wrangle data into an acceptable format for Chroma\n",
        "\n",
        "Transform our data from lists to a list of dictionaries for insertion into our Chroma collection.\n",
        "\n",
        "**Note**: Contextual AI Parser provides additional metadata (e.g., `block_type`, `hierarchy_level`, `confidence_level`, `file_name`) that can be added to Chroma if you have capacity for more metadata fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tivq2MpAXA4n",
        "outputId": "43762968-ab04-4e57-d3e0-1210f888d71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 71 chunks for insertion into Chroma\n",
            "Chunks by document type:\n",
            "  - table_rich_document: 9 chunks\n",
            "  - research_paper: 62 chunks\n"
          ]
        }
      ],
      "source": [
        "# Initialize the data object\n",
        "data = []\n",
        "\n",
        "# Create a dictionary for each row by iterating through the corresponding lists\n",
        "for text, title, source, doc_type, block_type, hierarchy_level, confidence_level in zip(\n",
        "    texts, titles, sources, doc_types, block_types, hierarchy_levels, confidence_levels\n",
        "):\n",
        "    data_point = {\n",
        "        \"text\": text,\n",
        "        \"title\": title,\n",
        "        \"source\": source,\n",
        "        \"document_type\": doc_type,\n",
        "        \"block_type\": block_type,\n",
        "    }\n",
        "    # Add optional metadata fields if available\n",
        "    if hierarchy_level is not None:\n",
        "        data_point[\"hierarchy_level\"] = hierarchy_level\n",
        "    if confidence_level is not None:\n",
        "        data_point[\"confidence_level\"] = confidence_level\n",
        "\n",
        "    data.append(data_point)\n",
        "\n",
        "print(f\"Prepared {len(data)} chunks for insertion into Chroma\")\n",
        "print(f\"Chunks by document type:\")\n",
        "for doc_type in set(doc_types):\n",
        "    count = doc_types.count(doc_type)\n",
        "    print(f\"  - {doc_type}: {count} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7AmoDlXA4n"
      },
      "source": [
        "### Insert data into Chroma and generate embeddings\n",
        "\n",
        "Embeddings will be generated upon insertion to our Chroma collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR24qjphXA4o",
        "outputId": "37f4e1a1-a6d0-4d8f-bfa1-76dc8a1037dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Insert complete.\n"
          ]
        }
      ],
      "source": [
        "# Insert text chunks and metadata into Chroma collection\n",
        "# Build metadata dictionaries, including optional fields when available\n",
        "metadata_list = []\n",
        "for item in data:\n",
        "    metadata = {\n",
        "        \"title\": item[\"title\"],\n",
        "        \"source\": item[\"source\"],\n",
        "        \"document_type\": item[\"document_type\"],\n",
        "        \"block_type\": item[\"block_type\"]\n",
        "    }\n",
        "    # Add optional metadata fields if present\n",
        "    if \"hierarchy_level\" in item:\n",
        "        metadata[\"hierarchy_level\"] = item[\"hierarchy_level\"]\n",
        "    if \"confidence_level\" in item:\n",
        "        metadata[\"confidence_level\"] = item[\"confidence_level\"]\n",
        "    metadata_list.append(metadata)\n",
        "\n",
        "collection.add(\n",
        "    documents=[item[\"text\"] for item in data],\n",
        "    metadatas=metadata_list,\n",
        "    ids=[f\"chunk_{i}\" for i in range(len(data))]\n",
        ")\n",
        "\n",
        "print(\"Insert complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qILAMuHXA4o"
      },
      "source": [
        "### Query the data\n",
        "\n",
        "Here, we perform a simple similarity search to return the most similar embedded chunks to our search query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuyfM-npXA4o",
        "outputId": "1c1810ea-26aa-42c2-e552-ddc0f03e7d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Searching for Transformer Architecture ===\n",
            "\n",
            "--- Result 1 ---\n",
            "Title: Attention Is All You Need\n",
            "Type: research_paper\n",
            "Source: Page 3\n",
            "Similarity: 0.278\n",
            "Text preview: Document: Attention Is All You Need\n",
            "Type: research_paper\n",
            "\n",
            "# Attention Is All You Need\n",
            "## 3 Model Architecture\n",
            "\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-...\n",
            "\n",
            "--- Result 2 ---\n",
            "Title: Attention Is All You Need\n",
            "Type: research_paper\n",
            "Source: Page 3\n",
            "Similarity: 0.271\n",
            "Text preview: Document: Attention Is All You Need\n",
            "Type: research_paper\n",
            "\n",
            "# Attention Is All You Need\n",
            "## 3 Model Architecture\n",
            "### 3.2 Attention\n",
            "\n",
            "Figure 1: The Transformer - model architecture....\n",
            "\n",
            "--- Result 3 ---\n",
            "Title: Attention Is All You Need\n",
            "Type: research_paper\n",
            "Source: Page 5\n",
            "Similarity: 0.250\n",
            "Text preview: Document: Attention Is All You Need\n",
            "Type: research_paper\n",
            "\n",
            "# Attention Is All You Need\n",
            "## 3 Model Architecture\n",
            "### 3.2 Attention\n",
            "#### 3.2.3 Applications of Attention in our Model\n",
            "\n",
            "The Transformer uses ...\n",
            "\n",
            "==================================================\n",
            "\n",
            "=== Searching for Table/Data Content ===\n",
            "\n",
            "--- Result 1 ---\n",
            "Title: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document\n",
            "Source: Page 1\n",
            "Similarity: -0.020\n",
            "Text preview: Document: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document\n",
            "\n",
            "| Traits | Environment | Mean |  |  | S.D. |  |  | Minimum |  |  | Maximum |  |  |\n",
            "|--------------------------------------------...\n",
            "\n",
            "--- Result 2 ---\n",
            "Title: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document\n",
            "Source: Page 1\n",
            "Similarity: -0.028\n",
            "Text preview: Document: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document\n",
            "\n",
            "| Traits | Environment | Mean |  |  | S.D. |  |  | Minimum |  |  | Maximum |  |  |\n",
            "|----------------------------|---------------...\n",
            "\n",
            "--- Result 3 ---\n",
            "Title: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document\n",
            "Source: Page 1\n",
            "Similarity: -0.052\n",
            "Text preview: Document: OmniDocBench Dataset Documentation\n",
            "Type: table_rich_document\n",
            "\n",
            "| Traits | Environment | Mean |  |  | S.D. |  |  | Minimum |  |  | Maximum |  |  |\n",
            "|-------------------|---------------|--------...\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Search for transformer-related content\n",
        "print(\"=== Searching for Transformer Architecture ===\")\n",
        "results = collection.query(\n",
        "    query_texts=[\"transformer architecture attention mechanism\"],\n",
        "    n_results=3,\n",
        "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
        ")\n",
        "\n",
        "for i, (doc, metadata, distance) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Title: {metadata['title']}\")\n",
        "    print(f\"Type: {metadata['document_type']}\")\n",
        "    print(f\"Source: {metadata['source']}\")\n",
        "    print(f\"Similarity: {1 - distance:.3f}\")\n",
        "    print(f\"Text preview: {doc[:200]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Example 2: Search for table-related content\n",
        "print(\"\\n=== Searching for Table/Data Content ===\")\n",
        "results = collection.query(\n",
        "    query_texts=[\"dataset table benchmark performance metrics\"],\n",
        "    n_results=3,\n",
        "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
        ")\n",
        "\n",
        "for i, (doc, metadata, distance) in enumerate(zip(results['documents'][0], results['metadatas'][0], results['distances'][0])):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Title: {metadata['title']}\")\n",
        "    print(f\"Type: {metadata['document_type']}\")\n",
        "    print(f\"Source: {metadata['source']}\")\n",
        "    print(f\"Similarity: {1 - distance:.3f}\")\n",
        "    print(f\"Text preview: {doc[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWr6qUa0XA4o"
      },
      "source": [
        "### Perform RAG on parsed articles\n",
        "\n",
        "We'll use OpenAI's GPT model to generate responses based on the retrieved context from Chroma.\n",
        "\n",
        "#### Example 1: RAG on Transformer Architecture\n",
        "\n",
        "This example demonstrates a complete RAG pipeline: we query Chroma for relevant chunks, combine them as context, and generate a response using OpenAI. The `/parse` API contributed structured markdown with preserved document hierarchy, enabling better semantic search and context retrieval for technical content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "171ADiKFXA4o",
        "outputId": "c3adb5f9-f0b6-4961-dd47-00c0a1575d8a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span> Explain how transformer attention mechanism works, using only the retrieved context.                            <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;31mâ•­â”€\u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31m Prompt \u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31mâ”€â•®\u001b[0m\n",
              "\u001b[1;31mâ”‚\u001b[0m Explain how transformer attention mechanism works, using only the retrieved context.                            \u001b[1;31mâ”‚\u001b[0m\n",
              "\u001b[1;31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generated Content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> The Transformer replaces recurrence and convolution with an attention mechanism that directly models global     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> dependencies between input and output. Instead of processing sequences step-by-step, it uses self-attention to  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> compute representations of the input and of the output by letting each position in a sequence attend to (i.e.,  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> draw information from) other positions. Because these attention operations are not inherently sequential, the   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> architecture permits much greater parallelization and can be trained far more quickly. The model also uses      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> multi-head attention â€” running several attention mechanisms in parallel â€” and applies this multi-head attention <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> in multiple places within the overall architecture (the paper notes three different uses).                      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32mâ•­â”€\u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32m Generated Content \u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32mâ”€â•®\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m The Transformer replaces recurrence and convolution with an attention mechanism that directly models global     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m dependencies between input and output. Instead of processing sequences step-by-step, it uses self-attention to  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m compute representations of the input and of the output by letting each position in a sequence attend to (i.e.,  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m draw information from) other positions. Because these attention operations are not inherently sequential, the   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m architecture permits much greater parallelization and can be trained far more quickly. The model also uses      \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m multi-head attention â€” running several attention mechanisms in parallel â€” and applies this multi-head attention \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m in multiple places within the overall architecture (the paper notes three different uses).                      \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "query = \"transformer attention mechanism\"\n",
        "prompt = f\"Explain how {query} works, using only the retrieved context.\"\n",
        "\n",
        "# Retrieve relevant documents\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=4,\n",
        "    include=[\"documents\", \"metadatas\"]\n",
        ")\n",
        "\n",
        "# Prepare context\n",
        "context = \"\\n\\n\".join(results['documents'][0])\n",
        "\n",
        "# Generate response\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. Use only the information from the context.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {prompt}\"}\n",
        "    ],\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "console.print(Panel(prompt, title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.choices[0].message.content, title=\"Generated Content\", border_style=\"bold green\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3TCVDyLXA4o"
      },
      "source": [
        "#### Example 2: RAG on Dataset/Benchmark Information\n",
        "\n",
        "This example queries Chroma for dataset and benchmark information, then generates a response. The `/parse` API's advanced table extraction capabilities ensure that structured data from table-rich documents is properly extracted and searchable, improving retrieval quality for data-focused queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "yAx9Ii9JXA4o",
        "outputId": "4dbe1c42-1373-4c40-d42d-80fe756baafb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span> What information does the retrieved context provide about dataset benchmark performance evaluation?             <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;31mâ•­â”€\u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31m Prompt \u001b[0m\u001b[1;31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;31mâ”€â•®\u001b[0m\n",
              "\u001b[1;31mâ”‚\u001b[0m What information does the retrieved context provide about dataset benchmark performance evaluation?             \u001b[1;31mâ”‚\u001b[0m\n",
              "\u001b[1;31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generated Content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> The retrieved context does not contain any model or algorithm performance numbers. Instead it provides the      <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> dataset summary statistics and experimental metadata that would be used for benchmarking:                       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> - It is Table 2 from the OmniDocBench documentation: \"Statistical estimations of the quantitative               <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> agromorphological and grain quality traits for the subsp. durum, turgidum and dicoccon at each environment (N   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> north, C centre and S south).\"                                                                                  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> - For multiple traits (e.g., Thousand kernel weight, Test weight, Yellow Index, Days to heading, Plant height)  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> it reports, by subspecies (durum, turgidum, dicoccon) and environment (N, C, S or S08):                         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>   - Mean                                                                                                        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>   - Standard deviation (S.D.)                                                                                   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>   - Minimum                                                                                                     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>   - Maximum                                                                                                     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> - Experimental note: all experiments were in 2007 except the south in 2008 (S08).                               <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> In other words, the context supplies descriptive statistics and experiment timing/locations for the dataset     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> (useful for evaluating and comparing methods), but contains no benchmark performance evaluations (no accuracy,  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span> error, AUC, etc.).                                                                                              <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;32mâ•­â”€\u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32m Generated Content \u001b[0m\u001b[1;32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[1;32mâ”€â•®\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m The retrieved context does not contain any model or algorithm performance numbers. Instead it provides the      \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m dataset summary statistics and experimental metadata that would be used for benchmarking:                       \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m - It is Table 2 from the OmniDocBench documentation: \"Statistical estimations of the quantitative               \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m agromorphological and grain quality traits for the subsp. durum, turgidum and dicoccon at each environment (N   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m north, C centre and S south).\"                                                                                  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m - For multiple traits (e.g., Thousand kernel weight, Test weight, Yellow Index, Days to heading, Plant height)  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m it reports, by subspecies (durum, turgidum, dicoccon) and environment (N, C, S or S08):                         \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m   - Mean                                                                                                        \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m   - Standard deviation (S.D.)                                                                                   \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m   - Minimum                                                                                                     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m   - Maximum                                                                                                     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m - Experimental note: all experiments were in 2007 except the south in 2008 (S08).                               \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m                                                                                                                 \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m In other words, the context supplies descriptive statistics and experiment timing/locations for the dataset     \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m (useful for evaluating and comparing methods), but contains no benchmark performance evaluations (no accuracy,  \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ”‚\u001b[0m error, AUC, etc.).                                                                                              \u001b[1;32mâ”‚\u001b[0m\n",
              "\u001b[1;32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"dataset benchmark performance evaluation\"\n",
        "prompt = f\"What information does the retrieved context provide about {query}?\"\n",
        "\n",
        "# Retrieve relevant documents\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=4,\n",
        "    include=[\"documents\", \"metadatas\"]\n",
        ")\n",
        "\n",
        "# Prepare context\n",
        "context = \"\\n\\n\".join(results['documents'][0])\n",
        "\n",
        "# Generate response\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. Use only the information from the context.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {prompt}\"}\n",
        "    ],\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "console.print(Panel(prompt, title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.choices[0].message.content, title=\"Generated Content\", border_style=\"bold green\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnPjmcnZXA4o"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a unique RAG pipeline using Contextual AI Parser and Chroma with two distinct document types:\n",
        "\n",
        "### What We Demonstrated:\n",
        "1. **Research Paper Parsing**: \"Attention is All You Need\" with document hierarchy preservation\n",
        "2. **Table-Rich Document Parsing**: OmniDocBench dataset with advanced table extraction\n",
        "3. **Multi-modal RAG**: Semantic search across different document types\n",
        "4. **Contextual Intelligence**: Leveraging document structure for better retrieval\n",
        "\n",
        "### Contextual AI Parser Advantages:\n",
        "- **Cloud-based processing**: No local GPU/compute requirements\n",
        "- **Document hierarchy preservation**: Maintains section relationships and structure\n",
        "- **Advanced table handling**: Smart table splitting with header propagation\n",
        "- **Multiple output formats**: Blocks, markdown, and structured JSON\n",
        "- **Production-ready**: Scalable cloud service with enterprise features\n",
        "\n",
        "### Key Differentiators from Other Parsers:\n",
        "- **Hierarchical context**: Parent-child relationships preserved in chunks\n",
        "- **Table intelligence**: Large tables automatically split with context preservation\n",
        "- **Document type awareness**: Different parsing strategies for different content types\n",
        "- **Rich metadata**: Document structure information enhances RAG quality\n",
        "\n",
        "### Chroma Integration Benefits:\n",
        "- **Multi-modal search**: Query across different document types simultaneously\n",
        "- **Metadata filtering**: Filter by document type, source, and other attributes\n",
        "- **Efficient storage**: Optimized vector database for embeddings\n",
        "- **Scalability**: From local development to cloud production\n",
        "\n",
        "### Next Steps for Enhancement:\n",
        "* Implement document-level metadata for better source attribution\n",
        "* Add hybrid search combining keyword and semantic search\n",
        "* Experiment with different chunking strategies for each document type\n",
        "* End-to-end RAG agents via [Contextual AI](https://docs.contextual.ai/user-guides/beginner-guide)\n",
        "* Get more information about integrating [Chroma](https://docs.trychroma.com/docs/overview/introduction)\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to get started?** This notebook provides a complete, production-ready example of integrating Contextual AI Parser with Chroma for sophisticated RAG applications. The combination of Contextual AI's advanced parsing capabilities and Chroma's powerful vector search features creates a robust foundation for document-based AI applications.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
