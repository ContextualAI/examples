{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/18-contextualai-chroma/02-contextual-ai-reranker-chroma.ipynb)\n",
        "\n",
        "# Using Contextual AI Reranker with Chroma\n",
        "\n",
        "**Last updated:** October 2025\n",
        "\n",
        "Contextual AI's reranker is the first with instruction-following capabilities to handle conflicts in retrieval. It is the most accurate reranker in the world per industry-leading benchmarks like BEIR. This notebook demonstrates how to integrate Contextual AI's reranker with Chroma for enhanced RAG pipelines.\n",
        "\n",
        "**Key Features:**\n",
        "- **Instruction-following reranking**: Handle complex retrieval scenarios with custom instructions\n",
        "- **BEIR benchmark-leading accuracy**: State-of-the-art reranking performance\n",
        "- **Multi-lingual support**: Handle documents in multiple languages\n",
        "- **Chroma integration**: Seamless vector database integration for retrieval + reranking\n",
        "\n",
        "The current reranker models include: \n",
        "- ctxl-rerank-v2-instruct-multilingual \n",
        "- ctxl-rerank-v2-instruct-multilingual-mini\n",
        "- ctxl-rerank-v1-instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install --upgrade chromadb contextual-client openai requests rich\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "# Suppress Chroma client logs\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Keys Setup üîë\n",
        "\n",
        "We'll be using the Contextual AI API for reranking and OpenAI API for embeddings. The code below dynamically fetches your API keys based on whether you're running this notebook in Google Colab or as a regular Jupyter notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"  # Replace with the name of your secret/env var\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"  # Replace with the name of your secret/env var\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    import os\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "print(\"API keys configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup Chroma with Sample Data\n",
        "\n",
        "Let's create a Chroma collection with sample enterprise documents to demonstrate the reranking capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from contextual import ContextualAI\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "\n",
        "# Initialize clients\n",
        "contextual_client = ContextualAI(api_key=contextual_api_key)\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Use OpenAI embeddings\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=openai_api_key,\n",
        "    model_name=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "collection_name = \"enterprise_documents\"\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    embedding_function=openai_ef\n",
        ")\n",
        "\n",
        "print(f\"Created collection '{collection_name}' with OpenAI embeddings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample enterprise documents with different types and dates\n",
        "sample_documents = [\n",
        "    {\n",
        "        \"content\": \"Following detailed cost analysis and market research, we have implemented the following changes: AI training clusters will see a 15% uplift in raw compute performance, enterprise support packages are being restructured, and bulk procurement programs (100+ units) for the RTX 5090 Enterprise series will operate on a $2,899 baseline.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Enterprise GPU Pricing Update\",\n",
        "            \"date\": \"2025-01-15\",\n",
        "            \"source\": \"NVIDIA Enterprise Sales Portal\",\n",
        "            \"classification\": \"Internal Use Only\",\n",
        "            \"department\": \"Sales\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Enterprise pricing for the RTX 5090 GPU bulk orders (100+ units) is currently set at $3,100-$3,300 per unit. This pricing for RTX 5090 enterprise bulk orders has been confirmed across all major distribution channels.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Market Analysis Report\",\n",
        "            \"date\": \"2023-11-30\",\n",
        "            \"source\": \"TechAnalytics Research Group\",\n",
        "            \"classification\": \"Public\",\n",
        "            \"department\": \"Research\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"RTX 5090 Enterprise GPU requires 450W TDP and 20% cooling overhead. Power consumption analysis shows optimal performance at 85% utilization with enterprise-grade cooling solutions.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Technical Specifications\",\n",
        "            \"date\": \"2025-01-25\",\n",
        "            \"source\": \"NVIDIA Enterprise Sales Portal\",\n",
        "            \"classification\": \"Internal Use Only\",\n",
        "            \"department\": \"Engineering\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Our enterprise customers have reported significant performance improvements with the RTX 5090 in AI workloads. Training times reduced by 40% compared to previous generation GPUs.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Customer Performance Report\",\n",
        "            \"date\": \"2025-01-10\",\n",
        "            \"source\": \"Customer Success Team\",\n",
        "            \"classification\": \"Confidential\",\n",
        "            \"department\": \"Customer Success\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"The RTX 5090 represents a breakthrough in enterprise AI computing. With 128GB of HBM3e memory and 2.5x faster training performance, it's designed for the most demanding AI workloads.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Product Launch Announcement\",\n",
        "            \"date\": \"2024-12-01\",\n",
        "            \"source\": \"Marketing Department\",\n",
        "            \"classification\": \"Public\",\n",
        "            \"department\": \"Marketing\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Internal memo: RTX 5090 enterprise pricing strategy has been revised. New baseline pricing effective January 15, 2025: $2,899 for bulk orders (100+ units), $3,200 for standard enterprise orders.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Internal Pricing Memo\",\n",
        "            \"date\": \"2025-01-12\",\n",
        "            \"source\": \"Executive Team\",\n",
        "            \"classification\": \"Internal Use Only\",\n",
        "            \"department\": \"Executive\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Add documents to Chroma\n",
        "documents = [doc[\"content\"] for doc in sample_documents]\n",
        "metadatas = [doc[\"metadata\"] for doc in sample_documents]\n",
        "ids = [f\"doc_{i}\" for i in range(len(sample_documents))]\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Added {len(sample_documents)} documents to Chroma collection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Basic Retrieval vs. Reranked Retrieval\n",
        "\n",
        "Let's demonstrate the difference between basic Chroma retrieval and Contextual AI's instruction-following reranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query and instruction for reranking\n",
        "query = \"What is the current enterprise pricing for the RTX 5090 GPU for bulk orders?\"\n",
        "\n",
        "instruction = \"Prioritize internal sales documents over market analysis reports. More recent documents should be weighted higher. Enterprise portal content supersedes distributor communications.\"\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Basic Chroma retrieval\n",
        "print(\"üîç BASIC CHROMA RETRIEVAL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Retrieve more documents than we need for reranking\n",
        "chroma_results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=6,  # Get all documents for reranking\n",
        "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
        ")\n",
        "\n",
        "print(f\"Retrieved {len(chroma_results['documents'][0])} documents from Chroma\")\n",
        "print(\"\\nChroma Results (ordered by similarity):\")\n",
        "for i, (doc, metadata, distance) in enumerate(zip(\n",
        "    chroma_results['documents'][0], \n",
        "    chroma_results['metadatas'][0], \n",
        "    chroma_results['distances'][0]\n",
        ")):\n",
        "    print(f\"\\n{i+1}. {metadata['title']} (Similarity: {1-distance:.3f})\")\n",
        "    print(f\"   Source: {metadata['source']} | Date: {metadata['date']}\")\n",
        "    print(f\"   Classification: {metadata['classification']}\")\n",
        "    print(f\"   Content: {doc[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Contextual AI Reranking\n",
        "print(\"\\n\\nüéØ CONTEXTUAL AI RERANKING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Prepare documents and metadata for reranking\n",
        "documents_to_rerank = chroma_results['documents'][0]\n",
        "metadata_for_rerank = [str(meta) for meta in chroma_results['metadatas'][0]]\n",
        "\n",
        "# Apply Contextual AI reranking with instruction\n",
        "rerank_response = contextual_client.rerank.create(\n",
        "    query=query,\n",
        "    instruction=instruction,\n",
        "    documents=documents_to_rerank,\n",
        "    metadata=metadata_for_rerank,\n",
        "    model=\"ctxl-rerank-v2-instruct-multilingual\"\n",
        ")\n",
        "\n",
        "print(f\"Reranked {len(rerank_response.results)} documents using instruction-following reranking\")\n",
        "print(\"\\nReranked Results (ordered by relevance + instruction):\")\n",
        "for i, result in enumerate(rerank_response.results):\n",
        "    original_index = result.index\n",
        "    original_metadata = chroma_results['metadatas'][0][original_index]\n",
        "    original_doc = chroma_results['documents'][0][original_index]\n",
        "    \n",
        "    print(f\"\\n{i+1}. {original_metadata['title']} (Score: {result.relevance_score:.3f})\")\n",
        "    print(f\"   Source: {original_metadata['source']} | Date: {original_metadata['date']}\")\n",
        "    print(f\"   Classification: {original_metadata['classification']}\")\n",
        "    print(f\"   Content: {original_doc[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Complete RAG Pipeline with Reranking\n",
        "\n",
        "Now let's demonstrate a complete RAG pipeline that combines Chroma retrieval, Contextual AI reranking, and LLM generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "def complete_rag_pipeline(query, instruction, top_k=3):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: Chroma retrieval + Contextual AI reranking + LLM generation\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "    \n",
        "    # Step 1: Retrieve from Chroma\n",
        "    console.print(Panel(\"Step 1: Retrieving from Chroma\", style=\"bold blue\"))\n",
        "    chroma_results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=6,  # Get more for reranking\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "    \n",
        "    # Step 2: Rerank with Contextual AI\n",
        "    console.print(Panel(\"Step 2: Reranking with Contextual AI\", style=\"bold green\"))\n",
        "    documents_to_rerank = chroma_results['documents'][0]\n",
        "    metadata_for_rerank = [str(meta) for meta in chroma_results['metadatas'][0]]\n",
        "    \n",
        "    rerank_response = contextual_client.rerank.create(\n",
        "        query=query,\n",
        "        instruction=instruction,\n",
        "        documents=documents_to_rerank,\n",
        "        metadata=metadata_for_rerank,\n",
        "        model=\"ctxl-rerank-v2-instruct-multilingual\"\n",
        "    )\n",
        "    \n",
        "    # Step 3: Get top-k reranked documents\n",
        "    top_docs = []\n",
        "    top_metadata = []\n",
        "    \n",
        "    for i in range(min(top_k, len(rerank_response.results))):\n",
        "        result = rerank_response.results[i]\n",
        "        original_index = result.index\n",
        "        top_docs.append(chroma_results['documents'][0][original_index])\n",
        "        top_metadata.append(chroma_results['metadatas'][0][original_index])\n",
        "    \n",
        "    # Step 4: Generate response with LLM\n",
        "    console.print(Panel(\"Step 3: Generating response with LLM\", style=\"bold yellow\"))\n",
        "    context = \"\\n\\n\".join(top_docs)\n",
        "    \n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. Use only the information from the context and cite your sources.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\"}\n",
        "        ],\n",
        "        temperature=1\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"response\": response.choices[0].message.content,\n",
        "        \"sources\": top_metadata,\n",
        "        \"rerank_scores\": [result.relevance_score for result in rerank_response.results[:top_k]]\n",
        "    }\n",
        "\n",
        "# Example 1: Enterprise pricing query\n",
        "console = Console()\n",
        "console.print(Panel(\"üöÄ COMPLETE RAG PIPELINE DEMO\", style=\"bold magenta\"))\n",
        "\n",
        "result = complete_rag_pipeline(\n",
        "    query=\"What is the current enterprise pricing for the RTX 5090 GPU for bulk orders?\",\n",
        "    instruction=\"Prioritize internal sales documents over market analysis reports. More recent documents should be weighted higher. Enterprise portal content supersedes distributor communications.\",\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "console.print(Panel(result[\"response\"], title=\"Generated Response\", border_style=\"bold green\"))\n",
        "console.print(Panel(f\"Sources used: {[meta['title'] for meta in result['sources']]}\", title=\"Sources\", border_style=\"bold blue\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Technical specifications query with different instruction\n",
        "console.print(Panel(\"üîß TECHNICAL SPECIFICATIONS QUERY\", style=\"bold cyan\"))\n",
        "\n",
        "result2 = complete_rag_pipeline(\n",
        "    query=\"What are the technical specifications and power requirements for the RTX 5090?\",\n",
        "    instruction=\"Prioritize technical documentation and engineering specifications. Internal technical documents should rank higher than marketing materials. Focus on detailed specifications and performance metrics.\",\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "console.print(Panel(result2[\"response\"], title=\"Generated Response\", border_style=\"bold green\"))\n",
        "console.print(Panel(f\"Sources used: {[meta['title'] for meta in result2['sources']]}\", title=\"Sources\", border_style=\"bold blue\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Advanced Reranking Scenarios\n",
        "\n",
        "Let's demonstrate different reranking scenarios to show the flexibility of instruction-following reranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_reranking_strategies(query, strategies):\n",
        "    \"\"\"\n",
        "    Compare different reranking strategies for the same query\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "    \n",
        "    # Get initial results from Chroma\n",
        "    chroma_results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=6,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "    \n",
        "    documents_to_rerank = chroma_results['documents'][0]\n",
        "    metadata_for_rerank = [str(meta) for meta in chroma_results['metadatas'][0]]\n",
        "    \n",
        "    for strategy_name, instruction in strategies.items():\n",
        "        console.print(Panel(f\"Strategy: {strategy_name}\", style=\"bold magenta\"))\n",
        "        console.print(f\"Instruction: {instruction}\")\n",
        "        \n",
        "        # Apply reranking\n",
        "        rerank_response = contextual_client.rerank.create(\n",
        "            query=query,\n",
        "            instruction=instruction,\n",
        "            documents=documents_to_rerank,\n",
        "            metadata=metadata_for_rerank,\n",
        "            model=\"ctxl-rerank-v2-instruct-multilingual\"\n",
        "        )\n",
        "        \n",
        "        # Show top 3 results\n",
        "        console.print(\"Top 3 Results:\")\n",
        "        for i in range(min(3, len(rerank_response.results))):\n",
        "            result = rerank_response.results[i]\n",
        "            original_index = result.index\n",
        "            original_metadata = chroma_results['metadatas'][0][original_index]\n",
        "            console.print(f\"  {i+1}. {original_metadata['title']} (Score: {result.relevance_score:.3f})\")\n",
        "        \n",
        "        console.print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Define different reranking strategies\n",
        "strategies = {\n",
        "    \"Recent Documents First\": \"Prioritize the most recent documents. Documents from 2025 should rank higher than older documents.\",\n",
        "    \"Internal Documents Priority\": \"Prioritize internal and confidential documents over public documents. Internal Use Only and Confidential documents should rank highest.\",\n",
        "    \"Department-Specific\": \"Prioritize documents from Sales and Engineering departments. Customer Success and Marketing documents should rank lower.\",\n",
        "    \"Source Authority\": \"Prioritize documents from NVIDIA Enterprise Sales Portal and Executive Team. External sources like TechAnalytics should rank lower.\"\n",
        "}\n",
        "\n",
        "# Compare strategies for the same query\n",
        "query = \"What is the current status and pricing for RTX 5090 enterprise GPUs?\"\n",
        "\n",
        "console = Console()\n",
        "console.print(Panel(\"üîÑ COMPARING RERANKING STRATEGIES\", style=\"bold magenta\"))\n",
        "console.print(f\"Query: {query}\\n\")\n",
        "\n",
        "compare_reranking_strategies(query, strategies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates the powerful combination of Chroma and Contextual AI's instruction-following reranker for enhanced RAG pipelines.\n",
        "\n",
        "### What We Demonstrated:\n",
        "\n",
        "1. **Basic Chroma Retrieval**: Standard vector similarity search\n",
        "2. **Contextual AI Reranking**: Instruction-following reranking with custom business logic\n",
        "3. **Complete RAG Pipeline**: Chroma ‚Üí Reranking ‚Üí LLM Generation\n",
        "4. **Advanced Reranking Strategies**: Multiple instruction-based ranking approaches\n",
        "\n",
        "### Key Benefits of Contextual AI Reranker:\n",
        "\n",
        "- **Instruction-Following**: Handle complex business logic through natural language instructions\n",
        "- **BEIR Benchmark Leading**: State-of-the-art accuracy on industry benchmarks\n",
        "- **Multi-lingual Support**: Handle documents in multiple languages\n",
        "- **Metadata-Aware**: Leverage document metadata for intelligent ranking\n",
        "- **Conflict Resolution**: Handle conflicting information in retrieval results\n",
        "\n",
        "### Chroma Integration Advantages:\n",
        "\n",
        "- **Seamless Integration**: Easy to add reranking to existing Chroma workflows\n",
        "- **Metadata Preservation**: Maintain document metadata through the reranking process\n",
        "- **Flexible Retrieval**: Retrieve more documents than needed for optimal reranking\n",
        "- **Production Ready**: Scalable solution for enterprise applications\n",
        "\n",
        "### Use Cases Demonstrated:\n",
        "\n",
        "1. **Enterprise Document Search**: Prioritize internal documents over external sources\n",
        "2. **Technical Documentation**: Focus on engineering specifications over marketing materials\n",
        "3. **Temporal Relevance**: Weight recent documents higher than older ones\n",
        "4. **Authority-Based Ranking**: Prioritize authoritative sources and departments\n",
        "\n",
        "### Next Steps for Enhancement:\n",
        "\n",
        "- **Hybrid Search**: Combine keyword and semantic search with reranking\n",
        "- **Custom Instructions**: Develop domain-specific reranking instructions\n",
        "- **Performance Optimization**: Batch processing for large document collections\n",
        "- **Evaluation Metrics**: Measure reranking effectiveness with custom metrics\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to get started?** This notebook provides a complete, production-ready example of integrating Contextual AI's instruction-following reranker with Chroma for sophisticated RAG applications. The combination enables intelligent document ranking that goes beyond simple similarity to understand business context and requirements.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
