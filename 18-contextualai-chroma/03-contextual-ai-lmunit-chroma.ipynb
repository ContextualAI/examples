{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ContextualAI/examples/blob/main/17-contextualai-chroma/03-contextual-ai-lmunit-chroma.ipynb)\n",
        "\n",
        "# Natural Language Unit Testing for RAG Systems with Chroma and Contextual AI\n",
        "\n",
        "**Last updated:** October 2025\n",
        "\n",
        "Evaluating LLM outputs in RAG systems is critical for ensuring response quality and reliability. This notebook demonstrates how to create and apply natural language unit tests using [LMUnit](https://contextual.ai/blog/lmunit/) with Chroma vector database for comprehensive RAG evaluation.\n",
        "\n",
        "**Key Features:**\n",
        "- **RAG Pipeline Evaluation**: Test complete Chroma + LLM RAG systems\n",
        "- **Natural Language Unit Testing**: Systematic evaluation of response quality\n",
        "- **Domain-Specific Testing**: Custom unit tests for different use cases\n",
        "- **Visualization & Analysis**: Comprehensive evaluation results analysis\n",
        "\n",
        "### Why Natural Language Unit Testing for RAG?\n",
        "\n",
        "Traditional RAG evaluation methods often face several challenges:\n",
        "- **Retrieval Quality**: Hard to measure if retrieved documents are relevant\n",
        "- **Generation Quality**: LLM responses may be factually incorrect or poorly structured\n",
        "- **End-to-End Evaluation**: Difficult to assess the complete RAG pipeline\n",
        "- **Domain-Specific Requirements**: Generic metrics don't capture domain nuances\n",
        "\n",
        "Natural language unit tests address these challenges by:\n",
        "- **Breaking down evaluation** into specific, testable criteria\n",
        "- **Providing granular feedback** on different quality aspects\n",
        "- **Enabling systematic improvement** of RAG systems\n",
        "- **Supporting domain-specific** quality requirements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation and Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install --upgrade chromadb contextual-client openai requests rich pandas matplotlib seaborn scikit-learn tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "# Suppress Chroma client logs\n",
        "logging.getLogger(\"chromadb\").setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Optional, Union, Tuple\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich.table import Table\n",
        "\n",
        "# Import Contextual AI and Chroma\n",
        "from contextual import ContextualAI\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from openai import OpenAI\n",
        "\n",
        "print(\"All packages imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Keys Setup ðŸ”‘\n",
        "\n",
        "We'll be using the Contextual AI API for LMUnit evaluation, OpenAI API for embeddings and generation, and Chroma for vector storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API key variable names\n",
        "contextual_api_key_var = \"CONTEXTUAL_API_KEY\"  # Replace with the name of your secret/env var\n",
        "openai_api_key_var = \"OPENAI_API_KEY\"  # Replace with the name of your secret/env var\n",
        "\n",
        "# Fetch API keys\n",
        "try:\n",
        "    # If running in Colab, fetch API keys from Secrets\n",
        "    import google.colab\n",
        "    from google.colab import userdata\n",
        "    contextual_api_key = userdata.get(contextual_api_key_var)\n",
        "    openai_api_key = userdata.get(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise ValueError(f\"Secret '{contextual_api_key_var}' not found in Colab secrets.\")\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(f\"Secret '{openai_api_key_var}' not found in Colab secrets.\")\n",
        "except ImportError:\n",
        "    # If not running in Colab, fetch API keys from environment variables\n",
        "    import os\n",
        "    contextual_api_key = os.getenv(contextual_api_key_var)\n",
        "    openai_api_key = os.getenv(openai_api_key_var)\n",
        "    \n",
        "    if not contextual_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{contextual_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "    if not openai_api_key:\n",
        "        raise EnvironmentError(\n",
        "            f\"Environment variable '{openai_api_key_var}' is not set. \"\n",
        "            \"Please define it before running this script.\"\n",
        "        )\n",
        "\n",
        "print(\"API keys configured successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup Chroma with Sample Knowledge Base\n",
        "\n",
        "Let's create a Chroma collection with sample enterprise documents to demonstrate RAG evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize clients\n",
        "contextual_client = ContextualAI(api_key=contextual_api_key)\n",
        "chroma_client = chromadb.Client()\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Use OpenAI embeddings\n",
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "    api_key=openai_api_key,\n",
        "    model_name=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "collection_name = \"enterprise_knowledge\"\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    embedding_function=openai_ef\n",
        ")\n",
        "\n",
        "print(f\"Created collection '{collection_name}' with OpenAI embeddings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample enterprise knowledge base documents\n",
        "knowledge_documents = [\n",
        "    {\n",
        "        \"content\": \"Our company's AI training infrastructure uses NVIDIA RTX 5090 GPUs with 128GB HBM3e memory. The system achieves 2.5x faster training performance compared to previous generation GPUs. Power consumption is 450W TDP with 20% cooling overhead. Enterprise pricing for bulk orders (100+ units) is $2,899 per unit.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"AI Infrastructure Specifications\",\n",
        "            \"department\": \"Engineering\",\n",
        "            \"date\": \"2025-01-15\",\n",
        "            \"classification\": \"Internal\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Customer service AI system handles 70% of inquiries without human intervention. Average response time is 30 seconds, increasing to 2 minutes during peak hours. The system excels at returns and order tracking but struggles with complex billing disputes. Success rate for basic inquiries is 85%.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Customer Service AI Performance\",\n",
        "            \"department\": \"Customer Success\",\n",
        "            \"date\": \"2025-01-10\",\n",
        "            \"classification\": \"Internal\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Financial compliance requires all AI-generated responses to include risk disclaimers. Regulatory requirements mandate specific language for investment advice. All customer-facing AI must be audited quarterly for compliance with SEC regulations. Penalties for non-compliance can reach $1M per violation.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"AI Compliance Requirements\",\n",
        "            \"department\": \"Legal\",\n",
        "            \"date\": \"2025-01-20\",\n",
        "            \"classification\": \"Confidential\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Market analysis shows AI adoption increasing 40% year-over-year in enterprise sectors. Key trends include multimodal AI, edge computing, and responsible AI practices. Investment in AI infrastructure is expected to reach $200B by 2026. Competitive advantage requires continuous innovation in AI capabilities.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"AI Market Trends Analysis\",\n",
        "            \"department\": \"Research\",\n",
        "            \"date\": \"2025-01-05\",\n",
        "            \"classification\": \"Public\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"content\": \"Data privacy regulations require all AI systems to implement data minimization principles. Personal data processing must be limited to necessary purposes. Users have the right to data portability and deletion. AI systems must provide clear explanations of automated decisions affecting individuals.\",\n",
        "        \"metadata\": {\n",
        "            \"title\": \"AI Data Privacy Guidelines\",\n",
        "            \"department\": \"Privacy\",\n",
        "            \"date\": \"2025-01-12\",\n",
        "            \"classification\": \"Confidential\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Add documents to Chroma\n",
        "documents = [doc[\"content\"] for doc in knowledge_documents]\n",
        "metadatas = [doc[\"metadata\"] for doc in knowledge_documents]\n",
        "ids = [f\"doc_{i}\" for i in range(len(knowledge_documents))]\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(f\"Added {len(knowledge_documents)} documents to Chroma collection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: RAG Pipeline and Test Queries\n",
        "\n",
        "Let's create a complete RAG pipeline and generate responses for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_pipeline(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: Chroma retrieval + LLM generation\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve from Chroma\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=top_k,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "    \n",
        "    # Step 2: Prepare context\n",
        "    context_docs = results['documents'][0]\n",
        "    context_metadata = results['metadatas'][0]\n",
        "    \n",
        "    # Combine context with metadata\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"Source: {meta['title']} ({meta['department']})\\n{doc}\"\n",
        "        for doc, meta in zip(context_docs, context_metadata)\n",
        "    ])\n",
        "    \n",
        "    # Step 3: Generate response with LLM\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context. Use only the information from the context and cite your sources.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\"}\n",
        "        ],\n",
        "        temperature=1\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": response.choices[0].message.content,\n",
        "        \"sources\": context_metadata,\n",
        "        \"context\": context\n",
        "    }\n",
        "\n",
        "# Test queries for evaluation\n",
        "test_queries = [\n",
        "    \"What are the technical specifications of our AI infrastructure?\",\n",
        "    \"How well does our customer service AI perform?\",\n",
        "    \"What compliance requirements do we need to follow for AI systems?\",\n",
        "    \"What are the current market trends in AI adoption?\",\n",
        "    \"What privacy guidelines must our AI systems follow?\"\n",
        "]\n",
        "\n",
        "print(f\"Created {len(test_queries)} test queries for RAG evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate RAG responses for all test queries\n",
        "console = Console()\n",
        "console.print(Panel(\"ðŸš€ GENERATING RAG RESPONSES\", style=\"bold magenta\"))\n",
        "\n",
        "rag_results = []\n",
        "for i, query in enumerate(test_queries):\n",
        "    console.print(f\"Processing query {i+1}/{len(test_queries)}: {query}\")\n",
        "    result = rag_pipeline(query)\n",
        "    rag_results.append(result)\n",
        "    console.print(f\"Generated response: {result['response'][:100]}...\")\n",
        "    console.print(\"\")\n",
        "\n",
        "console.print(Panel(f\"Generated {len(rag_results)} RAG responses for evaluation\", style=\"bold green\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Natural Language Unit Testing with LMUnit\n",
        "\n",
        "Now let's define unit tests and evaluate our RAG responses using Contextual AI's LMUnit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define unit tests for RAG evaluation\n",
        "unit_tests = [\n",
        "    \"Does the response accurately reflect the information from the retrieved context?\",\n",
        "    \"Is the response clear and well-structured for the target audience?\",\n",
        "    \"Does the response provide specific details and avoid vague statements?\",\n",
        "    \"Are potential risks or limitations mentioned when relevant?\",\n",
        "    \"Does the response cite or reference the source information appropriately?\",\n",
        "    \"Is the response actionable and provide clear next steps or implications?\"\n",
        "]\n",
        "\n",
        "print(\"Unit Tests for RAG Evaluation:\")\n",
        "for i, test in enumerate(unit_tests, 1):\n",
        "    print(f\"{i}. {test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_lmunit_evaluation(rag_results, unit_tests):\n",
        "    \"\"\"\n",
        "    Run LMUnit evaluation on RAG responses\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "    console.print(Panel(\"ðŸ§ª RUNNING LMUNIT EVALUATION\", style=\"bold blue\"))\n",
        "    \n",
        "    evaluation_results = []\n",
        "    \n",
        "    for i, result in enumerate(tqdm(rag_results, desc=\"Evaluating responses\")):\n",
        "        console.print(f\"\\nEvaluating response {i+1}/{len(rag_results)}\")\n",
        "        console.print(f\"Query: {result['query']}\")\n",
        "        \n",
        "        response_tests = []\n",
        "        \n",
        "        for j, test in enumerate(unit_tests):\n",
        "            try:\n",
        "                # Run LMUnit evaluation\n",
        "                lmunit_result = contextual_client.lmunit.create(\n",
        "                    query=result['query'],\n",
        "                    response=result['response'],\n",
        "                    unit_test=test\n",
        "                )\n",
        "                \n",
        "                score_value = getattr(lmunit_result, 'score', None)\n",
        "                response_tests.append({\n",
        "                    'test': test,\n",
        "                    'score': score_value,\n",
        "                    'evaluation': None\n",
        "                })\n",
        "                \n",
        "                if score_value is not None:\n",
        "                    console.print(f\"  Test {j+1}: Score {score_value:.2f}\")\n",
        "                else:\n",
        "                    console.print(f\"  Test {j+1}: No score returned\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                error_message = str(e)\n",
        "                try:\n",
        "                    import json as _json\n",
        "                    parsed = _json.loads(error_message) if isinstance(error_message, str) else None\n",
        "                    if isinstance(parsed, dict) and 'detail' in parsed and isinstance(parsed['detail'], list):\n",
        "                        msgs = \"; \".join([item.get('msg', '') for item in parsed['detail'] if isinstance(item, dict)])\n",
        "                        if msgs:\n",
        "                            error_message = msgs\n",
        "                except Exception:\n",
        "                    pass\n",
        "                console.print(f\"  Test {j+1}: Error - {error_message}\")\n",
        "                response_tests.append({\n",
        "                    'test': test,\n",
        "                    'score': None,\n",
        "                    'error': error_message\n",
        "                })\n",
        "        \n",
        "        evaluation_results.append({\n",
        "            'query': result['query'],\n",
        "            'response': result['response'],\n",
        "            'sources': result['sources'],\n",
        "            'test_results': response_tests\n",
        "        })\n",
        "    \n",
        "    return evaluation_results\n",
        "\n",
        "# Run the evaluation\n",
        "evaluation_results = run_lmunit_evaluation(rag_results, unit_tests)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Analysis and Visualization\n",
        "\n",
        "Let's analyze the evaluation results and create visualizations to understand RAG performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation summary\n",
        "def create_evaluation_summary(evaluation_results):\n",
        "    \"\"\"\n",
        "    Create a summary of evaluation results\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "    \n",
        "    # Calculate overall statistics\n",
        "    all_scores = []\n",
        "    test_names = []\n",
        "    \n",
        "    for result in evaluation_results:\n",
        "        for test_result in result['test_results']:\n",
        "            if test_result['score'] is not None:\n",
        "                all_scores.append(test_result['score'])\n",
        "                test_names.append(test_result['test'])\n",
        "    \n",
        "    if all_scores:\n",
        "        avg_score = np.mean(all_scores)\n",
        "        min_score = np.min(all_scores)\n",
        "        max_score = np.max(all_scores)\n",
        "        \n",
        "        console.print(Panel(f\"\"\"\n",
        "ðŸ“Š EVALUATION SUMMARY\n",
        "\n",
        "Total Tests: {len(all_scores)}\n",
        "Average Score: {avg_score:.2f}/5.0\n",
        "Score Range: {min_score:.2f} - {max_score:.2f}\n",
        "Responses Evaluated: {len(evaluation_results)}\n",
        "        \"\"\", title=\"LMUnit Evaluation Results\", border_style=\"bold green\"))\n",
        "    \n",
        "    return all_scores, test_names\n",
        "\n",
        "# Generate summary\n",
        "all_scores, test_names = create_evaluation_summary(evaluation_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create detailed results table\n",
        "def create_results_table(evaluation_results):\n",
        "    \"\"\"\n",
        "    Create a detailed results table\n",
        "    \"\"\"\n",
        "    console = Console()\n",
        "    \n",
        "    table = Table(title=\"Detailed Evaluation Results\")\n",
        "    table.add_column(\"Query\", style=\"cyan\", no_wrap=True)\n",
        "    table.add_column(\"Test\", style=\"magenta\")\n",
        "    table.add_column(\"Score\", style=\"green\")\n",
        "    table.add_column(\"Response Preview\", style=\"yellow\")\n",
        "    \n",
        "    for result in evaluation_results:\n",
        "        query_short = result['query'][:50] + \"...\" if len(result['query']) > 50 else result['query']\n",
        "        response_short = result['response'][:50] + \"...\" if len(result['response']) > 50 else result['response']\n",
        "        \n",
        "        for test_result in result['test_results']:\n",
        "            if test_result['score'] is not None:\n",
        "                test_short = test_result['test'][:40] + \"...\" if len(test_result['test']) > 40 else test_result['test']\n",
        "                table.add_row(\n",
        "                    query_short,\n",
        "                    test_short,\n",
        "                    f\"{test_result['score']:.2f}\",\n",
        "                    response_short\n",
        "                )\n",
        "    \n",
        "    console.print(table)\n",
        "\n",
        "# Display results table\n",
        "create_results_table(evaluation_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "def create_evaluation_visualizations(evaluation_results, unit_tests):\n",
        "    \"\"\"\n",
        "    Create visualizations for evaluation results\n",
        "    \"\"\"\n",
        "    # Set up the plotting style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('RAG System Evaluation Results', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Score distribution histogram\n",
        "    all_scores = []\n",
        "    for result in evaluation_results:\n",
        "        for test_result in result['test_results']:\n",
        "            if test_result['score'] is not None:\n",
        "                all_scores.append(test_result['score'])\n",
        "    \n",
        "    axes[0, 0].hist(all_scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0, 0].set_title('Score Distribution')\n",
        "    axes[0, 0].set_xlabel('Score (1-5)')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].axvline(np.mean(all_scores), color='red', linestyle='--', label=f'Mean: {np.mean(all_scores):.2f}')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # 2. Test performance comparison\n",
        "    test_scores = {}\n",
        "    for i, test in enumerate(unit_tests):\n",
        "        test_scores[f\"Test {i+1}\"] = []\n",
        "        for result in evaluation_results:\n",
        "            if i < len(result['test_results']) and result['test_results'][i]['score'] is not None:\n",
        "                test_scores[f\"Test {i+1}\"].append(result['test_results'][i]['score'])\n",
        "    \n",
        "    test_means = [np.mean(scores) if scores else 0 for scores in test_scores.values()]\n",
        "    test_names = list(test_scores.keys())\n",
        "    \n",
        "    bars = axes[0, 1].bar(test_names, test_means, color='lightcoral', alpha=0.7)\n",
        "    axes[0, 1].set_title('Average Score by Test')\n",
        "    axes[0, 1].set_ylabel('Average Score')\n",
        "    axes[0, 1].set_ylim(0, 5)\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, mean in zip(bars, test_means):\n",
        "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "                        f'{mean:.2f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 3. Response performance comparison\n",
        "    response_scores = []\n",
        "    response_names = []\n",
        "    for i, result in enumerate(evaluation_results):\n",
        "        scores = [test_result['score'] for test_result in result['test_results'] if test_result['score'] is not None]\n",
        "        if scores:\n",
        "            response_scores.append(np.mean(scores))\n",
        "            response_names.append(f\"Response {i+1}\")\n",
        "    \n",
        "    axes[1, 0].bar(response_names, response_scores, color='lightgreen', alpha=0.7)\n",
        "    axes[1, 0].set_title('Average Score by Response')\n",
        "    axes[1, 0].set_ylabel('Average Score')\n",
        "    axes[1, 0].set_ylim(0, 5)\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 4. Score heatmap\n",
        "    score_matrix = []\n",
        "    for result in evaluation_results:\n",
        "        row = []\n",
        "        for test_result in result['test_results']:\n",
        "            row.append(test_result['score'] if test_result['score'] is not None else 0)\n",
        "        score_matrix.append(row)\n",
        "    \n",
        "    im = axes[1, 1].imshow(score_matrix, cmap='RdYlGn', aspect='auto', vmin=1, vmax=5)\n",
        "    axes[1, 1].set_title('Score Heatmap')\n",
        "    axes[1, 1].set_xlabel('Test Number')\n",
        "    axes[1, 1].set_ylabel('Response Number')\n",
        "    axes[1, 1].set_xticks(range(len(unit_tests)))\n",
        "    axes[1, 1].set_yticks(range(len(evaluation_results)))\n",
        "    axes[1, 1].set_xticklabels([f\"T{i+1}\" for i in range(len(unit_tests))])\n",
        "    axes[1, 1].set_yticklabels([f\"R{i+1}\" for i in range(len(evaluation_results))])\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=axes[1, 1])\n",
        "    cbar.set_label('Score')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "create_evaluation_visualizations(evaluation_results, unit_tests)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates comprehensive RAG system evaluation using Chroma and Contextual AI's LMUnit for natural language unit testing.\n",
        "\n",
        "### What We Demonstrated:\n",
        "\n",
        "1. **Complete RAG Pipeline**: Chroma retrieval + LLM generation with enterprise knowledge base\n",
        "2. **Natural Language Unit Testing**: Systematic evaluation using LMUnit with 6 quality dimensions\n",
        "3. **Comprehensive Analysis**: Statistical analysis and visualization of evaluation results\n",
        "4. **Production-Ready Evaluation**: Scalable approach for enterprise RAG systems\n",
        "\n",
        "### Key Benefits of LMUnit for RAG Evaluation:\n",
        "\n",
        "- **Granular Quality Assessment**: Break down response quality into specific, measurable criteria\n",
        "- **Domain-Specific Testing**: Custom unit tests for different use cases and requirements\n",
        "- **Consistent Evaluation**: Standardized scoring across different responses and contexts\n",
        "- **Actionable Insights**: Identify specific areas for RAG system improvement\n",
        "\n",
        "### RAG Evaluation Dimensions Covered:\n",
        "\n",
        "1. **Accuracy**: Does the response reflect retrieved context accurately?\n",
        "2. **Clarity**: Is the response clear and well-structured?\n",
        "3. **Specificity**: Does the response provide specific details?\n",
        "4. **Risk Awareness**: Are limitations and risks mentioned?\n",
        "5. **Source Attribution**: Are sources properly cited?\n",
        "6. **Actionability**: Does the response provide clear next steps?\n",
        "\n",
        "### Chroma Integration Benefits:\n",
        "\n",
        "- **Rich Context**: Leverage document metadata for better retrieval\n",
        "- **Scalable Evaluation**: Test RAG systems with large knowledge bases\n",
        "- **Metadata-Aware Testing**: Evaluate responses based on source quality and relevance\n",
        "- **Production Monitoring**: Continuous evaluation of RAG system performance\n",
        "\n",
        "### Next Steps for Enhancement:\n",
        "\n",
        "- **Automated Evaluation**: Set up continuous evaluation pipelines\n",
        "- **Custom Unit Tests**: Develop domain-specific evaluation criteria\n",
        "- **Performance Monitoring**: Track RAG system performance over time\n",
        "- **A/B Testing**: Compare different RAG configurations using LMUnit scores\n",
        "\n",
        "---\n",
        "\n",
        "**Ready to get started?** This notebook provides a complete, production-ready example of evaluating RAG systems using Chroma and Contextual AI's LMUnit. The combination enables systematic quality assessment and continuous improvement of RAG applications.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
