# ğŸŒŸ Contextual API Examples

Welcome to our collection of example notebooks showcasing the power of Contextual's AI platform! ğŸš€

## ğŸ¯ Overview

This repository contains practical examples and demonstrations of how to interact with Contextual's API, helping you get started quickly and efficiently. You can run these examples in any jupyter notebook but an easy way to get started is Colab notebooks:

<a target="_blank" href="https://colab.research.google.com/github/ContextualAI/examples/blob/main/01-getting-started/end-to-end-example.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## ğŸ“š Table of Contents

  - ğŸš€ [End to End Example](01-getting-started/) - End to End example of the Contextual Platform
  - ğŸ”¬ [Hands on Lab](02-hands-on-lab/) - Lab broken into three chapters, Creating Agent & Datastores, Evaluation, and Tuning
  - ğŸ”§ [Standalone API](03-standalone-api/) - Examples of using individual API endpoints like `/generate` and `/rerank`, `/parse` and `/lmunit`.
  - ğŸ“Š [Sheets Script](04-sheets-script/) - A Google Sheets script that automates form filling using Contextual AI's API integration.
  - ğŸ“ [Policy Changes](05-policy-changes/) - An example use case for tracking changes in long policy documents.
  - ğŸ“ˆ [Improving Agent](06-improve-agent-performance/) - Settings for improving or specializing your RAG agent.
  - âš–ï¸ [Using RAGAS for Evaluation](07-evaluation-ragas/) - A walkthrough for using RAGAS on a RAG agent.
  - ğŸ¯ [LMUnit Evaluation for RewardBench](09-lmunit-rewardbench/) - Showing LMUnit for evaluating RewardBench.
  - ğŸ¯ [FACTS Benchmark](10-FACTS-benchmark/) - Benchmark for evaluating grounding for LLMs
  - ğŸ” [Retrieval Analysis](11-retrieval-analysis/) - Notebooks for an end-to-end evaluation of RAG retrieval
  - ğŸ§¾ [Structured Data Extraction](12-legal-contract-extraction/) - Showing how to perform extraction across legal documents.
  - ğŸ‘€ [Using Metrics API and Monitoring RAG](14-monitoring) - Showing how to monitor your RAG agent


## ğŸš€ Getting Started

1. ğŸ“¥ Clone this repository
    ```bash
    git clone https://github.com/ContextualAI/examples
    ```
2. ğŸ”‘ Set up your API credentials in the respective [notebook](01-getting-started/end-to-end-example.ipynb)
    ```bash
    API_TOKEN = '...'  # Replace with your actual API token
    ```
3. ğŸ“¦ Install required dependencies
    ```bash
    pip install -r requirements.txt
    ```
4. ğŸ® Run the example notebook

## ğŸ”§ Prerequisites

- Python 3.7+
- Jupyter Notebook/Lab
- Contextual API credentials
- Required Python packages (listed in `requirements.txt`)

## ğŸ’¡ Related Examples

- ğŸ§  [Contextual AI MCP Server](https://github.com/ContextualAI/contextual-mcp-server)
- ğŸ“š [Benchmarking with RAG QA Arena](https://github.com/rajshah4/LLM-Evaluation/tree/main/RAG_QA_Arena)
- ğŸ§ª [Full Stack Deep Research with Gemini, Contextual AI, and LangGraph](https://github.com/rajshah4/contextualai-gemini-research-agent)
- ğŸ§­ [Deep Research Agent using Agno, Contextual AI,  Tavily, and Langfuse](https://github.com/rajshah4/LLM-Evaluation/blob/main/ResearchAgent_Agno_LangFuse.ipynb)
- ğŸ‘ï¸ [Using Dify.AI with Contextual AI](https://www.youtube.com/watch?v=3WNUoKiwd2U)

## ğŸ¤ Contributing

We welcome contributions! Feel free to:
- ğŸ› Report bugs
- âœ¨ Request features
- ğŸ”€ Submit pull requests

## ğŸ“« Support

Need help? 
- ğŸ“§ Contact our support team [support@contextual.ai](mailto:support@contextual.ai)
- ğŸ’¬ Join our community discussions
- ğŸ“– Check out our [documentation](https://docs.contextual.ai)

## âš–ï¸ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
Made with â¤ï¸ by the Contextual team

